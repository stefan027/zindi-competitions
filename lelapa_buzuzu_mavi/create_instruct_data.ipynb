{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create instruct data from\n",
    "\n",
    "**Data sources**\n",
    "1. Inkuba-Instruct\n",
    "2. XNLI (Swahili data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = \"\"\n",
    "login(token=os.environ[\"HF_TOKEN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_mt_from_hub = True\n",
    "extract_sent_from_hub = True\n",
    "extract_xnli_from_hub = True\n",
    "additional_mt_data = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "basedir = Path(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {\n",
    "    'english_dev': 'data/english_dev-*-of-*.parquet',\n",
    "    'english_train': 'data/english_train-*-of-*.parquet',\n",
    "    'hausa_dev': 'data/hausa_dev-00000-of-00001.parquet',\n",
    "    'hausa_train': 'data/hausa_train-*-of-*.parquet',\n",
    "    'isizulu_dev': 'data/isizulu_dev-*-of-*.parquet',\n",
    "    'isizulu_train': 'data/isizulu_train-*-of-*.parquet',\n",
    "    'swahili_dev': 'data/swahili_dev-*-of-*.parquet',\n",
    "    'swahili_train': 'data/swahili_train-*-of-*.parquet',\n",
    "    'xhosa_dev': 'data/xhosa_dev-*-of-*.parquet',\n",
    "    'xhosa_train': 'data/xhosa_train-*-of-*.parquet',\n",
    "    'yoruba_dev': 'data/yoruba_dev-00000-of-00001.parquet',\n",
    "    'yoruba_train': 'data/yoruba_train-*-of-*.parquet'\n",
    "}\n",
    "base_path = \"hf://datasets/lelapa/Inkuba-instruct/\"\n",
    "# extract = ['hausa_dev', 'hausa_train', 'swahili_dev', 'swahili_train']\n",
    "# extract = ['swahili_dev', 'swahili_train']\n",
    "# extract = ['hausa_dev', 'hausa_train']\n",
    "# extract = ['english_dev', 'english_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _row2dict(row):\n",
    "    return {\n",
    "        \"task\": row[\"task\"], \"instruction_orig\": row[\"instruction\"], \"input\": row[\"inputs\"],\n",
    "        \"output\": row[\"targets\"], \"language\": row[\"lang\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MT data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if extract_mt_from_hub:\n",
    "    extract = ['english_dev', 'english_train']\n",
    "    paths = [base_path + splits[k] for k in extract]\n",
    "    df = dd.read_parquet(paths)\n",
    "    df = df[\n",
    "        (df[\"task\"] == 'mmt')\n",
    "        & (df[\"instruction\"].str.contains(\"swahili\")\n",
    "        | df[\"instruction\"].str.contains(\"hausa\"))\n",
    "    ]\n",
    "    df.to_parquet(\"inkuba_instruct_mmt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>inputs</th>\n",
       "      <th>targets</th>\n",
       "      <th>task</th>\n",
       "      <th>data_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>translate the following from english into swah...</td>\n",
       "      <td>dessel joins the sith army.[1]</td>\n",
       "      <td>saint nikitas the sinaite[21]</td>\n",
       "      <td>mmt</td>\n",
       "      <td>wmt22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>translate the following from english into swah...</td>\n",
       "      <td>\"there's some good cars here.</td>\n",
       "      <td>magari mazuri hapa</td>\n",
       "      <td>mmt</td>\n",
       "      <td>wmt22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>could you convert this english text to swahili?</td>\n",
       "      <td>you're committed to something</td>\n",
       "      <td>utakuwa una upungufu wa kitu fulani</td>\n",
       "      <td>mmt</td>\n",
       "      <td>wmt22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>please convert this english content into swahili.</td>\n",
       "      <td>hold the winds until god's people are ready to...</td>\n",
       "      <td>hadi kazi ya kupigwa muhuri kwa watu wa mungu ...</td>\n",
       "      <td>mmt</td>\n",
       "      <td>wmt22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>please convert this english content into swahili.</td>\n",
       "      <td>perth and fremantle can be easily explored on ...</td>\n",
       "      <td>perth na fremantle wanaweza kuchunuliwa kwa ra...</td>\n",
       "      <td>mmt</td>\n",
       "      <td>wmt22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         instruction  \\\n",
       "0  translate the following from english into swah...   \n",
       "1  translate the following from english into swah...   \n",
       "2    could you convert this english text to swahili?   \n",
       "3  please convert this english content into swahili.   \n",
       "4  please convert this english content into swahili.   \n",
       "\n",
       "                                              inputs  \\\n",
       "0                     dessel joins the sith army.[1]   \n",
       "1                      \"there's some good cars here.   \n",
       "2                      you're committed to something   \n",
       "3  hold the winds until god's people are ready to...   \n",
       "4  perth and fremantle can be easily explored on ...   \n",
       "\n",
       "                                             targets task data_source  \n",
       "0                      saint nikitas the sinaite[21]  mmt       wmt22  \n",
       "1                                 magari mazuri hapa  mmt       wmt22  \n",
       "2                utakuwa una upungufu wa kitu fulani  mmt       wmt22  \n",
       "3  hadi kazi ya kupigwa muhuri kwa watu wa mungu ...  mmt       wmt22  \n",
       "4  perth na fremantle wanaweza kuchunuliwa kwa ra...  mmt       wmt22  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dd.read_parquet(basedir/\"inkuba_instruct_mmt/*\")\n",
    "df[df[\"task\"] == \"mmt\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data_source\n",
       "mafand      103301\n",
       "wmt22     61604701\n",
       "Name: count, dtype: int64[pyarrow]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"task\"] == \"mmt\"][\"data_source\"].value_counts().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mmt_hausa\n",
       "False    56144270\n",
       "True      5563732\n",
       "Name: count, dtype: Int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"mmt_hausa\"] = df[\"instruction\"].str.contains(\"hausa\")\n",
    "df[df[\"task\"] == \"mmt\"][\"mmt_hausa\"].value_counts().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data_source  mmt_hausa\n",
       "mafand       False           82563\n",
       "             True            20738\n",
       "wmt22        False        56061707\n",
       "             True          5542994\n",
       "Name: task, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totals = df[df[\"task\"] == \"mmt\"].groupby([\"data_source\", \"mmt_hausa\"])[\"task\"].count().compute()\n",
    "totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mmt_wmt22_swahili_total = 50_000\n",
    "n_mmt_wmt22_hausa_total = 80_000\n",
    "n_mmt_mafand_swahili_total = 50_000\n",
    "n_mmt_mafand_hausa_total = 20_000\n",
    "\n",
    "custom_instructions = {\n",
    "    \"swahili\": \"Translate the following from English into Swahili.\",\n",
    "    \"hausa\": \"Translate the following from English into Hausa.\"\n",
    "}\n",
    "\n",
    "df_mmt_wmt22_swahili = (df[(df[\"data_source\"] == \"wmt22\") & (df[\"mmt_hausa\"] == False)]\n",
    "    .sample(frac=n_mmt_wmt22_swahili_total/totals.loc[(\"wmt22\", False)], random_state=123)).compute()\n",
    "df_mmt_wmt22_swahili[\"lang\"] = \"swahili\"\n",
    "\n",
    "df_mmt_wmt22_hausa = (df[(df[\"data_source\"] == \"wmt22\") & (df[\"mmt_hausa\"] == True)]\n",
    "    .sample(frac=n_mmt_wmt22_hausa_total/totals.loc[(\"wmt22\", True)], random_state=123)).compute()\n",
    "df_mmt_wmt22_hausa[\"lang\"] = \"hausa\"\n",
    "\n",
    "df_mmt_mafand_swahili = (df[(df[\"data_source\"] == \"mafand\") & (df[\"mmt_hausa\"] == False)]\n",
    "    .sample(frac=n_mmt_mafand_swahili_total/totals.loc[(\"mafand\", False)], random_state=123)).compute()\n",
    "df_mmt_mafand_swahili[\"lang\"] = \"swahili\"\n",
    "\n",
    "df_mmt_mafand_hausa = (df[(df[\"data_source\"] == \"mafand\") & (df[\"mmt_hausa\"] == True)]\n",
    "    .sample(frac=n_mmt_mafand_hausa_total/totals.loc[(\"mafand\", True)], random_state=123)).compute()\n",
    "df_mmt_mafand_hausa[\"lang\"] = \"hausa\"\n",
    "\n",
    "mt_examples = []\n",
    "dfs = [df_mmt_wmt22_swahili, df_mmt_wmt22_hausa, df_mmt_mafand_swahili, df_mmt_mafand_hausa]\n",
    "for df_ in dfs:\n",
    "    for _, row in df_.iterrows():\n",
    "        d = _row2dict(row)\n",
    "        d[\"instruction\"] = custom_instructions[d[\"language\"]]\n",
    "        mt_examples.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199999"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mt_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(basedir/\"inkuba_instruct_sample_mt.json\", \"w\") as f:\n",
    "    json.dump(mt_examples, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create additonal MT data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(basedir/\"inkuba_instruct_sample_mt.json\", \"r\") as f:\n",
    "    mt_examples = json.load(f)\n",
    "len(mt_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_inputs = set([d[\"input\"] for d in mt_examples])\n",
    "len(mt_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if additional_mt_data:\n",
    "    n_swa, n_hau = 500_000, 500_000\n",
    "\n",
    "    custom_instructions = {\n",
    "        \"swahili\": \"Translate the following from English into Swahili.\",\n",
    "        \"hausa\": \"Translate the following from English into Hausa.\"\n",
    "    }\n",
    "\n",
    "    add_mt_examples_swa, add_mt_examples_hau = [], []\n",
    "    last_milestone = 0\n",
    "    for _, row in df.iterrows():\n",
    "        if row[\"inputs\"] in mt_inputs:\n",
    "            continue\n",
    "        if not row[\"mmt_hausa\"] and len(add_mt_examples_swa) < n_swa:\n",
    "            row[\"lang\"] = \"swahili\"\n",
    "            d = _row2dict(row)\n",
    "            d[\"instruction\"] = custom_instructions[\"swahili\"]\n",
    "            add_mt_examples_swa.append(d)\n",
    "        elif row[\"mmt_hausa\"] and len(add_mt_examples_hau) < n_hau:\n",
    "            row[\"lang\"] = \"hausa\"\n",
    "            d = _row2dict(row)\n",
    "            d[\"instruction\"] = custom_instructions[\"hausa\"]\n",
    "            add_mt_examples_hau.append(d)\n",
    "        if len(add_mt_examples_swa) >= n_swa and len(add_mt_examples_hau) >= n_hau:\n",
    "            break\n",
    "        n = len(add_mt_examples_swa) + len(add_mt_examples_hau)\n",
    "        if n % 50_000 == 0:\n",
    "            if n > last_milestone:\n",
    "                last_milestone = n\n",
    "                print(n, \"examples processed\")\n",
    "    add_mt_examples = add_mt_examples_swa + add_mt_examples_hau\n",
    "    len(add_mt_examples), len(add_mt_examples_swa), len(add_mt_examples_hau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(basedir/\"inkuba_instruct_sample_mt_additional.json\", \"w\") as f:\n",
    "    json.dump(add_mt_examples, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if extract_sent_from_hub:\n",
    "    # extract = ['hausa_dev', 'hausa_train', 'swahili_dev', 'swahili_train']\n",
    "    # Swahili\n",
    "    extract = ['swahili_dev', 'swahili_train']\n",
    "    paths = [base_path + splits[k] for k in extract]\n",
    "    df = dd.read_parquet(paths)\n",
    "    df = df[(df[\"task\"] == 'sentiment')]\n",
    "    df.to_parquet(basedir/\"inkuba_instruct_sent_swahili\")\n",
    "\n",
    "    # Hausa\n",
    "    extract = ['hausa_dev', 'hausa_train']\n",
    "    paths = [base_path + splits[k] for k in extract]\n",
    "    df = dd.read_parquet(paths)\n",
    "    df = df[(df[\"task\"] == 'sentiment')]\n",
    "    df.to_parquet(basedir/\"inkuba_instruct_sent_hausa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>task</th>\n",
       "      <th>langs</th>\n",
       "      <th>data_source</th>\n",
       "      <th>instruction</th>\n",
       "      <th>inputs</th>\n",
       "      <th>targets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_6aba33a1_sentiment_ dev_hausa</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>hausa</td>\n",
       "      <td>afrisenti</td>\n",
       "      <td>Za ka iya tantance yanayin wannan rubutu? Bi w...</td>\n",
       "      <td>@user @user allah ya tsayyaba yar uwa 🎂 😍</td>\n",
       "      <td>Kyakkyawa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_ce64d307_sentiment_ dev_hausa</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>hausa</td>\n",
       "      <td>naijasenti</td>\n",
       "      <td>Da fatan za a gano ra'ayin da ke cikin wannan ...</td>\n",
       "      <td>@user intenet a masallachi😭😭😭 wani salo ne na ...</td>\n",
       "      <td>Tsaka-tsaki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_dfb02831_sentiment_ dev_swahili</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>swahili</td>\n",
       "      <td>swahili_tweet</td>\n",
       "      <td>Tafadhali tambua mawazo yaliyoonyeshwa kwenye ...</td>\n",
       "      <td>picha mbunge wa kilombero peter lijualikali ak...</td>\n",
       "      <td>Wastani</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_2efc9515_sentiment_ dev_hausa</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>hausa</td>\n",
       "      <td>afrisenti</td>\n",
       "      <td>Gano ra'ayin da aka bayyana a cikin wannan rub...</td>\n",
       "      <td>@user @user @user @user @user hhh amma rahama ...</td>\n",
       "      <td>Tsaka-tsaki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_ad1d9888_sentiment_ dev_swahili</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>swahili</td>\n",
       "      <td>afrisenti</td>\n",
       "      <td>Tafadhali tambua mawazo yaliyoonyeshwa kwenye ...</td>\n",
       "      <td>swali zuri sana nawatafuta wajuzi wa mambo wat...</td>\n",
       "      <td>Wastani</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   ID       task    langs    data_source  \\\n",
       "0    ID_6aba33a1_sentiment_ dev_hausa  sentiment    hausa      afrisenti   \n",
       "1    ID_ce64d307_sentiment_ dev_hausa  sentiment    hausa     naijasenti   \n",
       "2  ID_dfb02831_sentiment_ dev_swahili  sentiment  swahili  swahili_tweet   \n",
       "3    ID_2efc9515_sentiment_ dev_hausa  sentiment    hausa      afrisenti   \n",
       "4  ID_ad1d9888_sentiment_ dev_swahili  sentiment  swahili      afrisenti   \n",
       "\n",
       "                                         instruction  \\\n",
       "0  Za ka iya tantance yanayin wannan rubutu? Bi w...   \n",
       "1  Da fatan za a gano ra'ayin da ke cikin wannan ...   \n",
       "2  Tafadhali tambua mawazo yaliyoonyeshwa kwenye ...   \n",
       "3  Gano ra'ayin da aka bayyana a cikin wannan rub...   \n",
       "4  Tafadhali tambua mawazo yaliyoonyeshwa kwenye ...   \n",
       "\n",
       "                                              inputs      targets  \n",
       "0          @user @user allah ya tsayyaba yar uwa 🎂 😍    Kyakkyawa  \n",
       "1  @user intenet a masallachi😭😭😭 wani salo ne na ...  Tsaka-tsaki  \n",
       "2  picha mbunge wa kilombero peter lijualikali ak...      Wastani  \n",
       "3  @user @user @user @user @user hhh amma rahama ...  Tsaka-tsaki  \n",
       "4  swali zuri sana nawatafuta wajuzi wa mambo wat...      Wastani  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comp = pd.read_csv(basedir/\"data/SentimentTrain.csv\")\n",
    "# df_comp = pd.read_csv(\"/kaggle/input/vulavula-data/SentimentTrain.csv\")\n",
    "print(df_comp.shape)\n",
    "df_comp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swahili sentiment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16847\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>inputs</th>\n",
       "      <th>targets</th>\n",
       "      <th>task</th>\n",
       "      <th>data_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Changanua mawazo ya matini yanayofuata na uain...</td>\n",
       "      <td>habari tunaomba radhi kwa usumbufu unaojitokez...</td>\n",
       "      <td>Wastani</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>afrisenti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>mimi watani wangu hawajawahi kuvuka mipaka ila...</td>\n",
       "      <td>Hasi</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>swahili_tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tafadhali tambua mawazo yaliyoonyeshwa kwenye ...</td>\n",
       "      <td>anasema kuvuka mia kukanyaga mavi bure</td>\n",
       "      <td>Wastani</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>afrisenti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>na hivi ndivyo viwango vyetu vya kubadilishia ...</td>\n",
       "      <td>Wastani</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>afrisenti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tafadhali tambua mawazo yaliyoonyeshwa kwenye ...</td>\n",
       "      <td>sisi piahakika nyie ndio mlikua wapinzani h...</td>\n",
       "      <td>Chanya</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>swahili_tweet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         instruction  \\\n",
       "0  Changanua mawazo ya matini yanayofuata na uain...   \n",
       "1                                               <NA>   \n",
       "2  Tafadhali tambua mawazo yaliyoonyeshwa kwenye ...   \n",
       "3                                               <NA>   \n",
       "4  Tafadhali tambua mawazo yaliyoonyeshwa kwenye ...   \n",
       "\n",
       "                                              inputs  targets       task  \\\n",
       "0  habari tunaomba radhi kwa usumbufu unaojitokez...  Wastani  sentiment   \n",
       "1  mimi watani wangu hawajawahi kuvuka mipaka ila...     Hasi  sentiment   \n",
       "2             anasema kuvuka mia kukanyaga mavi bure  Wastani  sentiment   \n",
       "3  na hivi ndivyo viwango vyetu vya kubadilishia ...  Wastani  sentiment   \n",
       "4     sisi piahakika nyie ndio mlikua wapinzani h...   Chanya  sentiment   \n",
       "\n",
       "     data_source  \n",
       "0      afrisenti  \n",
       "1  swahili_tweet  \n",
       "2      afrisenti  \n",
       "3      afrisenti  \n",
       "4  swahili_tweet  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_swa = pd.read_parquet(basedir/\"inkuba_instruct_sent_swahili\").reset_index(drop=True)\n",
    "print(len(df_swa))\n",
    "df_swa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8392"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_swa[\"instruction\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_swa = df_swa.drop_duplicates(subset=[\"inputs\", \"targets\"]).copy()\n",
    "# print(len(df_swa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11084\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4055"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_swa = df_swa.drop_duplicates(subset=[\"instruction\", \"inputs\", \"targets\"]).copy()\n",
    "print(len(df_swa))\n",
    "df_swa[\"instruction\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "instruction\n",
       "<NA>                                                                                                                                                                                                                                                                                                                                           4055\n",
       "Tafadhali tambua mawazo yaliyoonyeshwa kwenye matini haya kwa kutegemea miongozo ifuatayo: Chanya: iwapo matini yanadokeza mawazo, mtazamo na hali chanya ya kihisia. Hasi: iwapo matini yanadokeza mawazo au hisia hasi. Wastani: iwapo matini hayadokezi lugha chanya au hasi kwa njia ya moja kwa moja au isiyo ya moja kwa moja.           3518\n",
       "Changanua mawazo ya matini yanayofuata na uainishe matini hayo katika mojawapo ya lebo zifuatazo. Chanya: iwapo matini yanadokeza mawazo, mtazamo na hali chanya ya kihisia. Hasi: iwapo matini yanadokeza mawazo au hisia hasi. Wastani: iwapo matini hayadokezi lugha chanya au hasi kwa njia ya moja kwa moja au isiyo ya moja kwa moja.    3511\n",
       "Name: count, dtype: Int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_swa[\"instruction\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instructions = []\n",
    "instr_options = list(set(df_swa[\"instruction\"].dropna()))\n",
    "for i, t in enumerate(df_swa[\"instruction\"]):\n",
    "    if pd.isna(t):\n",
    "        idx = i % len(instr_options)\n",
    "        instructions.append(instr_options[idx])\n",
    "    else:\n",
    "        instructions.append(t)\n",
    "df_swa[\"instruction\"] = instructions\n",
    "df_swa[\"instruction\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7637\n"
     ]
    }
   ],
   "source": [
    "df_swa = df_swa.drop_duplicates(subset=[\"instruction\", \"inputs\", \"targets\"]).copy()\n",
    "print(len(df_swa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_swa[\"in_comp_data\"] = df_swa[\"inputs\"].isin(df_comp[\"inputs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>inputs</th>\n",
       "      <th>targets</th>\n",
       "      <th>task</th>\n",
       "      <th>data_source</th>\n",
       "      <th>in_comp_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1109</th>\n",
       "      <td>Changanua mawazo ya matini yanayofuata na uain...</td>\n",
       "      <td>habari tunaomba namba yako dm kwa msaada zaidika</td>\n",
       "      <td>Chanya</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>afrisenti</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>Tafadhali tambua mawazo yaliyoonyeshwa kwenye ...</td>\n",
       "      <td>habari tunaomba namba yako dm kwa msaada zaidika</td>\n",
       "      <td>Chanya</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>afrisenti</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9578</th>\n",
       "      <td>Changanua mawazo ya matini yanayofuata na uain...</td>\n",
       "      <td>habari tunaomba namba yako dm kwa msaada zaidika</td>\n",
       "      <td>Wastani</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>swahili_tweet</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10416</th>\n",
       "      <td>Tafadhali tambua mawazo yaliyoonyeshwa kwenye ...</td>\n",
       "      <td>habari tunaomba namba yako dm kwa msaada zaidika</td>\n",
       "      <td>Wastani</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>afrisenti</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1164</th>\n",
       "      <td>Changanua mawazo ya matini yanayofuata na uain...</td>\n",
       "      <td>tunafunga sana kumi na nusu jionikaribu sana</td>\n",
       "      <td>Chanya</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>afrisenti</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2299</th>\n",
       "      <td>Tafadhali tambua mawazo yaliyoonyeshwa kwenye ...</td>\n",
       "      <td>tunafunga sana kumi na nusu jionikaribu sana</td>\n",
       "      <td>Wastani</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>afrisenti</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2444</th>\n",
       "      <td>Changanua mawazo ya matini yanayofuata na uain...</td>\n",
       "      <td>tunafunga sana kumi na nusu jionikaribu sana</td>\n",
       "      <td>Wastani</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>afrisenti</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10249</th>\n",
       "      <td>Tafadhali tambua mawazo yaliyoonyeshwa kwenye ...</td>\n",
       "      <td>tunafunga sana kumi na nusu jionikaribu sana</td>\n",
       "      <td>Chanya</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>swahili_tweet</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Tafadhali tambua mawazo yaliyoonyeshwa kwenye ...</td>\n",
       "      <td>upo eneo gani mteja tunaomba details vizuri il...</td>\n",
       "      <td>Chanya</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>swahili_tweet</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2502</th>\n",
       "      <td>Changanua mawazo ya matini yanayofuata na uain...</td>\n",
       "      <td>upo eneo gani mteja tunaomba details vizuri il...</td>\n",
       "      <td>Chanya</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>afrisenti</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5029</th>\n",
       "      <td>Tafadhali tambua mawazo yaliyoonyeshwa kwenye ...</td>\n",
       "      <td>upo eneo gani mteja tunaomba details vizuri il...</td>\n",
       "      <td>Wastani</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>swahili_tweet</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8119</th>\n",
       "      <td>Changanua mawazo ya matini yanayofuata na uain...</td>\n",
       "      <td>upo eneo gani mteja tunaomba details vizuri il...</td>\n",
       "      <td>Wastani</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>afrisenti</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             instruction  \\\n",
       "1109   Changanua mawazo ya matini yanayofuata na uain...   \n",
       "3996   Tafadhali tambua mawazo yaliyoonyeshwa kwenye ...   \n",
       "9578   Changanua mawazo ya matini yanayofuata na uain...   \n",
       "10416  Tafadhali tambua mawazo yaliyoonyeshwa kwenye ...   \n",
       "1164   Changanua mawazo ya matini yanayofuata na uain...   \n",
       "2299   Tafadhali tambua mawazo yaliyoonyeshwa kwenye ...   \n",
       "2444   Changanua mawazo ya matini yanayofuata na uain...   \n",
       "10249  Tafadhali tambua mawazo yaliyoonyeshwa kwenye ...   \n",
       "135    Tafadhali tambua mawazo yaliyoonyeshwa kwenye ...   \n",
       "2502   Changanua mawazo ya matini yanayofuata na uain...   \n",
       "5029   Tafadhali tambua mawazo yaliyoonyeshwa kwenye ...   \n",
       "8119   Changanua mawazo ya matini yanayofuata na uain...   \n",
       "\n",
       "                                                  inputs  targets       task  \\\n",
       "1109    habari tunaomba namba yako dm kwa msaada zaidika   Chanya  sentiment   \n",
       "3996    habari tunaomba namba yako dm kwa msaada zaidika   Chanya  sentiment   \n",
       "9578    habari tunaomba namba yako dm kwa msaada zaidika  Wastani  sentiment   \n",
       "10416   habari tunaomba namba yako dm kwa msaada zaidika  Wastani  sentiment   \n",
       "1164        tunafunga sana kumi na nusu jionikaribu sana   Chanya  sentiment   \n",
       "2299        tunafunga sana kumi na nusu jionikaribu sana  Wastani  sentiment   \n",
       "2444        tunafunga sana kumi na nusu jionikaribu sana  Wastani  sentiment   \n",
       "10249       tunafunga sana kumi na nusu jionikaribu sana   Chanya  sentiment   \n",
       "135    upo eneo gani mteja tunaomba details vizuri il...   Chanya  sentiment   \n",
       "2502   upo eneo gani mteja tunaomba details vizuri il...   Chanya  sentiment   \n",
       "5029   upo eneo gani mteja tunaomba details vizuri il...  Wastani  sentiment   \n",
       "8119   upo eneo gani mteja tunaomba details vizuri il...  Wastani  sentiment   \n",
       "\n",
       "         data_source  in_comp_data  \n",
       "1109       afrisenti         False  \n",
       "3996       afrisenti         False  \n",
       "9578   swahili_tweet         False  \n",
       "10416      afrisenti         False  \n",
       "1164       afrisenti          True  \n",
       "2299       afrisenti          True  \n",
       "2444       afrisenti          True  \n",
       "10249  swahili_tweet          True  \n",
       "135    swahili_tweet         False  \n",
       "2502       afrisenti         False  \n",
       "5029   swahili_tweet         False  \n",
       "8119       afrisenti         False  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dups = df_swa.groupby(\"inputs\")[[\"targets\"]].nunique()\n",
    "dups = set(dups[dups[\"targets\"] > 1].index)\n",
    "df_swa[df_swa[\"inputs\"].isin(dups)].sort_values(\"inputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7625\n"
     ]
    }
   ],
   "source": [
    "df_swa = df_swa[~df_swa[\"inputs\"].isin(dups)].copy()\n",
    "print(len(df_swa))\n",
    "assert len(df_swa) == len(df_swa[[\"instruction\", \"inputs\"]].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "in_comp_data\n",
       "False    7292\n",
       "True      333\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_swa[\"in_comp_data\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_swa[\"lang\"] = \"swahili\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hausa sentiment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141814\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>inputs</th>\n",
       "      <th>targets</th>\n",
       "      <th>task</th>\n",
       "      <th>data_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Da fatan za a gano ra'ayin da ke cikin wannan ...</td>\n",
       "      <td>gaskiyane ansha kyau sosaiiiii allah ya barmu ...</td>\n",
       "      <td>Kyakkyawa</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>naijasenti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Za ka iya tantance yanayin wannan rubutu? Bi w...</td>\n",
       "      <td>kai wai kudai wane bayahude ne ke typing in nan</td>\n",
       "      <td>Tsaka-tsaki</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>afrisenti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tantance ra’ayin wannan rubutu kuma a rarraba ...</td>\n",
       "      <td>@user ina ruwan ku da yanda yake gudana tunda ...</td>\n",
       "      <td>Korau</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>afrisenti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tantance ra’ayin wannan rubutu kuma a rarraba ...</td>\n",
       "      <td>gaskiya kinsha dalma</td>\n",
       "      <td>Korau</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>afrisenti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Da fatan za a gano ra'ayin da ke cikin wannan ...</td>\n",
       "      <td>@user @user su tsigeshi kawai dan bayada wani ...</td>\n",
       "      <td>Korau</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>afrisenti</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         instruction  \\\n",
       "0  Da fatan za a gano ra'ayin da ke cikin wannan ...   \n",
       "1  Za ka iya tantance yanayin wannan rubutu? Bi w...   \n",
       "2  Tantance ra’ayin wannan rubutu kuma a rarraba ...   \n",
       "3  Tantance ra’ayin wannan rubutu kuma a rarraba ...   \n",
       "4  Da fatan za a gano ra'ayin da ke cikin wannan ...   \n",
       "\n",
       "                                              inputs      targets       task  \\\n",
       "0  gaskiyane ansha kyau sosaiiiii allah ya barmu ...    Kyakkyawa  sentiment   \n",
       "1    kai wai kudai wane bayahude ne ke typing in nan  Tsaka-tsaki  sentiment   \n",
       "2  @user ina ruwan ku da yanda yake gudana tunda ...        Korau  sentiment   \n",
       "3                               gaskiya kinsha dalma        Korau  sentiment   \n",
       "4  @user @user su tsigeshi kawai dan bayada wani ...        Korau  sentiment   \n",
       "\n",
       "  data_source  \n",
       "0  naijasenti  \n",
       "1   afrisenti  \n",
       "2   afrisenti  \n",
       "3   afrisenti  \n",
       "4   afrisenti  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hau = pd.read_parquet(basedir/\"inkuba_instruct_sent_hausa\").reset_index(drop=True)\n",
    "print(len(df_hau))\n",
    "df_hau.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hau = df_hau.drop_duplicates(subset=[\"instruction\", \"inputs\", \"targets\"]).copy()\n",
    "print(len(df_hau))\n",
    "df_hau[\"instruction\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "instruction\n",
       "Tantance ra’ayin wannan rubutu kuma a rarraba rubutun zuwa ɗaya daga cikin waɗannan rukunoni. Kyakkyawa: idan rubutu yana nuna kyakkyawan tunani, hali, da yanayin motsin rai. Korau: idan rubutu yana nuna mummunar tunani ko motsin rai. Tsaka-tsaki: idan rubutu baya nufin magana mai kyau ko mara kyau kai tsaye ko a kaikaice.    21123\n",
       "Da fatan za a gano ra'ayin da ke cikin wannan rubutu bisa ga jagorori masu zuwa: Kyakkyawa: idan rubutu na nuna kyakkyawan tunani, hali, da yanayi. Korau: idan rubutu yana nuna mummunar tunani ko yanayi. Neutral: idan rubutu baya nuna kyakkyawar magana ko mara kyau kai tsaye ko a kaikaice.                                      21084\n",
       "Za ka iya tantance yanayin wannan rubutu? Bi waɗannan jagororin sharhi: Kyakkyawa: idan rubutu na nuna kyakkyawan tunani, hali, da yanayi. Korau: idan rubutu yana nuna mummunar tunani ko yanayi. Neutral: idan rubutu baya nuna kyakkyawar magana ko mara kyau kai tsaye ko a kaikaice.                                               21075\n",
       "Gano ra'ayin da aka bayyana a cikin wannan rubutu. Bin waɗannan jagororin, kyakkyawa yana na rubutu na nufin kyakkyawan tunani, ɗabi'a, da motsin rai. Korau na nuna rubutu na nufin mummunan tunani ko motsin rai. Tsaka-tsaki na nuna rubutu baya nufin magana mai kyau ko mara kyau kai tsaye ko a kaikaice.                         21062\n",
       "Name: count, dtype: Int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hau[\"instruction\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hau[\"instruction\"] = df_hau[\"instruction\"].str.replace(\"Neutral:\", \"Tsaka-tsaki:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84344\n"
     ]
    }
   ],
   "source": [
    "df_hau = df_hau.drop_duplicates(subset=[\"instruction\", \"inputs\", \"targets\"]).copy()\n",
    "print(len(df_hau))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hau[\"in_comp_data\"] = df_hau[\"inputs\"].isin(df_comp[\"inputs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>inputs</th>\n",
       "      <th>targets</th>\n",
       "      <th>task</th>\n",
       "      <th>data_source</th>\n",
       "      <th>in_comp_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54944</th>\n",
       "      <td>Gano ra'ayin da aka bayyana a cikin wannan rub...</td>\n",
       "      <td>@user allah ya isa domin ba za mu yafe irin ta...</td>\n",
       "      <td>Korau</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>naijasenti</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85438</th>\n",
       "      <td>Da fatan za a gano ra'ayin da ke cikin wannan ...</td>\n",
       "      <td>@user allah ya isa domin ba za mu yafe irin ta...</td>\n",
       "      <td>Kyakkyawa</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>afrisenti</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75596</th>\n",
       "      <td>Tantance ra’ayin wannan rubutu kuma a rarraba ...</td>\n",
       "      <td>@user allah ya isa domin ba za mu yafe irin ta...</td>\n",
       "      <td>Kyakkyawa</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>naijasenti</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30007</th>\n",
       "      <td>Za ka iya tantance yanayin wannan rubutu? Bi w...</td>\n",
       "      <td>@user allah ya isa domin ba za mu yafe irin ta...</td>\n",
       "      <td>Korau</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>afrisenti</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17487</th>\n",
       "      <td>Da fatan za a gano ra'ayin da ke cikin wannan ...</td>\n",
       "      <td>@user allah ya isa domin ba za mu yafe irin ta...</td>\n",
       "      <td>Korau</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>afrisenti</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50160</th>\n",
       "      <td>Gano ra'ayin da aka bayyana a cikin wannan rub...</td>\n",
       "      <td>@user 😢😢😢😢😢😢 mu yau aka mana low down. wlh ba ...</td>\n",
       "      <td>Korau</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>afrisenti</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52833</th>\n",
       "      <td>Za ka iya tantance yanayin wannan rubutu? Bi w...</td>\n",
       "      <td>@user 😢😢😢😢😢😢 mu yau aka mana low down. wlh ba ...</td>\n",
       "      <td>Kyakkyawa</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>naijasenti</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67164</th>\n",
       "      <td>Tantance ra’ayin wannan rubutu kuma a rarraba ...</td>\n",
       "      <td>@user 😢😢😢😢😢😢 mu yau aka mana low down. wlh ba ...</td>\n",
       "      <td>Korau</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>afrisenti</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12535</th>\n",
       "      <td>Za ka iya tantance yanayin wannan rubutu? Bi w...</td>\n",
       "      <td>@user 😢😢😢😢😢😢 mu yau aka mana low down. wlh ba ...</td>\n",
       "      <td>Korau</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>afrisenti</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Gano ra'ayin da aka bayyana a cikin wannan rub...</td>\n",
       "      <td>@user 😢😢😢😢😢😢 mu yau aka mana low down. wlh ba ...</td>\n",
       "      <td>Kyakkyawa</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>naijasenti</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             instruction  \\\n",
       "54944  Gano ra'ayin da aka bayyana a cikin wannan rub...   \n",
       "85438  Da fatan za a gano ra'ayin da ke cikin wannan ...   \n",
       "75596  Tantance ra’ayin wannan rubutu kuma a rarraba ...   \n",
       "30007  Za ka iya tantance yanayin wannan rubutu? Bi w...   \n",
       "17487  Da fatan za a gano ra'ayin da ke cikin wannan ...   \n",
       "...                                                  ...   \n",
       "50160  Gano ra'ayin da aka bayyana a cikin wannan rub...   \n",
       "52833  Za ka iya tantance yanayin wannan rubutu? Bi w...   \n",
       "67164  Tantance ra’ayin wannan rubutu kuma a rarraba ...   \n",
       "12535  Za ka iya tantance yanayin wannan rubutu? Bi w...   \n",
       "38     Gano ra'ayin da aka bayyana a cikin wannan rub...   \n",
       "\n",
       "                                                  inputs    targets  \\\n",
       "54944  @user allah ya isa domin ba za mu yafe irin ta...      Korau   \n",
       "85438  @user allah ya isa domin ba za mu yafe irin ta...  Kyakkyawa   \n",
       "75596  @user allah ya isa domin ba za mu yafe irin ta...  Kyakkyawa   \n",
       "30007  @user allah ya isa domin ba za mu yafe irin ta...      Korau   \n",
       "17487  @user allah ya isa domin ba za mu yafe irin ta...      Korau   \n",
       "...                                                  ...        ...   \n",
       "50160  @user 😢😢😢😢😢😢 mu yau aka mana low down. wlh ba ...      Korau   \n",
       "52833  @user 😢😢😢😢😢😢 mu yau aka mana low down. wlh ba ...  Kyakkyawa   \n",
       "67164  @user 😢😢😢😢😢😢 mu yau aka mana low down. wlh ba ...      Korau   \n",
       "12535  @user 😢😢😢😢😢😢 mu yau aka mana low down. wlh ba ...      Korau   \n",
       "38     @user 😢😢😢😢😢😢 mu yau aka mana low down. wlh ba ...  Kyakkyawa   \n",
       "\n",
       "            task data_source  in_comp_data  \n",
       "54944  sentiment  naijasenti         False  \n",
       "85438  sentiment   afrisenti         False  \n",
       "75596  sentiment  naijasenti         False  \n",
       "30007  sentiment   afrisenti         False  \n",
       "17487  sentiment   afrisenti         False  \n",
       "...          ...         ...           ...  \n",
       "50160  sentiment   afrisenti         False  \n",
       "52833  sentiment  naijasenti         False  \n",
       "67164  sentiment   afrisenti         False  \n",
       "12535  sentiment   afrisenti         False  \n",
       "38     sentiment  naijasenti         False  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dups = df_hau.groupby(\"inputs\")[[\"targets\"]].nunique()\n",
    "dups = set(dups[dups[\"targets\"] > 1].index)\n",
    "df_hau[df_hau[\"inputs\"].isin(dups)].sort_values(\"inputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84244\n"
     ]
    }
   ],
   "source": [
    "df_hau = df_hau[~df_hau[\"inputs\"].isin(dups)].copy()\n",
    "print(len(df_hau))\n",
    "assert len(df_hau) == len(df_hau[[\"instruction\", \"inputs\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "in_comp_data\n",
       "False    83471\n",
       "True       773\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hau[\"in_comp_data\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hau[\"lang\"] = \"hausa\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine sentiment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_swa = df_swa[~df_swa[\"in_comp_data\"]].copy()\n",
    "df_hau = df_hau[~df_hau[\"in_comp_data\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90763"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_swa) + len(df_hau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_custom_instructions = False\n",
    "\n",
    "custom_instructions_sent = {\n",
    "    \"swahili\": (\n",
    "        \"Changanua mawazo ya matini yanayofuata na uainishe matini hayo katika mojawapo ya lebo zifuatazo. \"\n",
    "        \"Chanya: iwapo matini yanadokeza mawazo, mtazamo na hali chanya ya kihisia. \"\n",
    "        \"Hasi: iwapo matini yanadokeza mawazo au hisia hasi. \"\n",
    "        \"Wastani: iwapo matini hayadokezi lugha chanya au hasi kwa njia ya moja kwa moja au isiyo ya moja kwa moja.\"\n",
    "    ),\n",
    "    \"hausa\": (\n",
    "        \"Tantance ra’ayin wannan rubutu kuma a rarraba rubutun zuwa ɗaya daga cikin waɗannan rukunoni. \"\n",
    "        \"Kyakkyawa: idan rubutu yana nuna kyakkyawan tunani, hali, da yanayin motsin rai. \"\n",
    "        \"Korau: idan rubutu yana nuna mummunar tunani ko motsin rai. \"\n",
    "        \"Tsaka-tsaki: idan rubutu baya nufin magana mai kyau ko mara kyau kai tsaye ko a kaikaice.\"\n",
    "    )\n",
    "}\n",
    "\n",
    "sent_examples = []\n",
    "dfs = [df_swa, df_hau]\n",
    "for df_ in dfs:\n",
    "    df_[\"instruction\"] = df_[\"instruction\"].fillna(\"\")\n",
    "    for _, row in df_.iterrows():\n",
    "        d = _row2dict(row)\n",
    "        if use_custom_instructions:\n",
    "            d[\"instruction\"] = custom_instructions_sent[d[\"language\"]]\n",
    "        else:\n",
    "            d[\"instruction\"] = row[\"instruction\"]\n",
    "        sent_examples.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(sent_examples) == len(df_swa) + len(df_hau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(basedir/\"inkuba_instruct_sample_sent.json\", \"w\") as f:\n",
    "    json.dump(sent_examples, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XNLI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if extract_xnli_from_hub:\n",
    "    splits = {'train': 'sw/train-*.parquet', 'test': 'sw/test-*.parquet', 'validation': 'sw/validation-*.parquet'}\n",
    "    ddf = dd.read_parquet(\"hf://datasets/facebook/xnli/\" + splits[\"train\"])\n",
    "    ddf.to_parquet(basedir/\"xnli_swa_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 6)\n"
     ]
    }
   ],
   "source": [
    "df_comp = pd.read_csv(basedir/\"data/XNLITrain.csv\")\n",
    "print(df_comp.shape)\n",
    "xnli_inputs = set(df_comp[\"inputs\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392702"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf = dd.read_parquet(basedir/\"xnli_swa_train\")\n",
    "len(ddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at class distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    130899\n",
       "1    130900\n",
       "2    130903\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf[\"label\"].value_counts().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the dataset\n",
    "\n",
    " - Create the Swahili dataset by selecting 10,000 examples from each class.\n",
    " - Create the Hausa dataset by translating the Swahili examples using the NLLB-200 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the `NLLB-200` model and define the translation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.11/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "2025-04-09 10:10:44.462510: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-09 10:10:44.462608: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-09 10:10:44.464109: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-09 10:10:44.471344: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-09 10:10:45.489500: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "856a340d34cc46e6b46cb28fd547d57e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 6.24GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import NllbTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_id = \"facebook/nllb-200-3.3B\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id, device_map=device, torch_dtype=torch.bfloat16)\n",
    "tokenizer = NllbTokenizer.from_pretrained(model_id)\n",
    "print(f\"Memory footprint: {model.get_memory_footprint() / 1024**3 :.2f}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(\n",
    "    article, model, tokenizer, src_lang=\"dyu_Latn\", tgt_lang=\"fra_Latn\",\n",
    "    max_length=30, num_beams=1, do_sample=False, temperature=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Translates a given text using a specified model and tokenizer.\n",
    "\n",
    "    Args:\n",
    "        article (`str`):\n",
    "            The text to be translated from the source language to the target language.\n",
    "        model (`transformers.PreTrainedModel`):\n",
    "            The pre-trained model used for generating translations.\n",
    "        tokenizer (`transformers.PreTrainedTokenizer`):\n",
    "            The tokenizer used for encoding the input text and decoding the generated output.\n",
    "        src_lang (`str`, optional):\n",
    "            The source language code for the tokenizer. Default is `\"dyu_Latn\"`.\n",
    "        tgt_lang (`str`, optional):\n",
    "            The target language code for the tokenizer. Default is `\"fra_Latn\"`.\n",
    "        max_length (`int`, optional):\n",
    "            The maximum length of the generated translation. Default is `30`.\n",
    "        num_beams (`int`, optional):\n",
    "            The number of beams for beam search. Default is `1`. If set to `1`, it uses\n",
    "            greedy decoding.\n",
    "        do_sample (`bool`, optional):\n",
    "            Whether to use sampling instead of greedy decoding. Default is `False`.\n",
    "        temperature (`float`, optional):\n",
    "            The temperature to use for sampling. Higher values (e.g., 1.0) result in more\n",
    "            diverse outputs, while lower values (e.g., 0.5) make the output more deterministic.\n",
    "            Default is `None`.\n",
    "\n",
    "    Returns:\n",
    "        list of `str`: \n",
    "            The translated text(s) as a list of strings. If the input is a single string,\n",
    "            the output will be a list with one translation.\n",
    "    \"\"\"\n",
    "    tokenizer.src_lang = src_lang\n",
    "    inputs = tokenizer(\n",
    "        article, return_tensors='pt', padding=True, truncation=True, max_length=128\n",
    "    )\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.to(\"cuda\")\n",
    "    translated_tokens = model.generate(\n",
    "        **inputs,\n",
    "        forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_lang),\n",
    "        max_length=max_length,\n",
    "        num_beams=num_beams,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate translations. This takes a while..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = (\n",
    "    \"You will be given two {lang} sentences. Your job is to  is to predict textual entailment. \"\n",
    "    \"In other words, does sentence A imply/contradict/neither sentence B. \"\n",
    "    \"Your response must be one word: entailment, contradiction, or neutral.\"\n",
    ")\n",
    "print(instruction)\n",
    "\n",
    "label_map = {0: 'entailment', 1: 'neutral', 2: 'contradiction'}\n",
    "\n",
    "max_per_class = 10_000\n",
    "counts = {0: 0, 1: 0, 2: 0}\n",
    "qa_data = []\n",
    "for _, row in ddf.iterrows():\n",
    "    tgt = row[\"label\"]\n",
    "    if counts[tgt] >= max_per_class:\n",
    "        continue\n",
    "    if row[\"hypothesis\"] not in xnli_inputs:\n",
    "        d = {\n",
    "            \"task\": \"qa\",\n",
    "            \"instruction\": instruction.format(lang=\"Swahili\"),\n",
    "            \"input\": f\"Sentence A: {row['premise']}\\nSentence B: {row['hypothesis']}\",\n",
    "            \"output\": label_map[tgt],\n",
    "            \"lang\": \"swahili\"\n",
    "        }\n",
    "        qa_data.append(d)\n",
    "        # translate the sentences to Hausa\n",
    "        hau_prem, hau_hyp = translate(\n",
    "            [row[\"premise\"], row[\"hypothesis\"]], model, tokenizer, src_lang=\"swh_Latn\", tgt_lang=\"hau_Latn\",\n",
    "            max_length=100, num_beams=4, do_sample=False, temperature=None\n",
    "        )\n",
    "        d = {\n",
    "            \"task\": \"qa\",\n",
    "            \"instruction\": instruction.format(lang=\"Hausa\"),\n",
    "            \"input\": f\"Sentence A: {hau_prem}\\nSentence B: {hau_hyp}\",\n",
    "            \"output\": label_map[tgt],\n",
    "            \"lang\": \"hausa\"\n",
    "        }\n",
    "        qa_data.append(d)\n",
    "        counts[tgt] += 1\n",
    "        if sum(counts.values()) % 1000 == 0:\n",
    "            print(f\"Processed {sum(counts.values())} of {max_per_class*len(counts)} samples\")\n",
    "        if all([c == max_per_class for c in counts.values()]):\n",
    "            break\n",
    "len(qa_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(basedir/\"inkuba_instruct_sample_qa.json\", \"w\") as f:\n",
    "    json.dump(qa_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_sent = pd.read_csv(basedir/\"data/SentimentTrain.csv\")\n",
    "comp_qa = pd.read_csv(basedir/\"data/XNLITrain.csv\")\n",
    "comp_mt = pd.read_csv(basedir/\"data/MTTrain.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_sent = set(comp_sent[\"inputs\"])\n",
    "comp_mt = set(comp_mt[\"inputs\"])\n",
    "\n",
    "comp_qa[\"inputs\"] = \"Sentence A: \" + comp_qa[\"premise\"] + \"\\nSentence B: \" + comp_qa[\"inputs\"]\n",
    "comp_qa = set(comp_qa[\"inputs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90763, 199999, 60000)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(basedir/\"inkuba_instruct_sample_sent.json\", \"r\") as f:\n",
    "    sent_examples = json.load(f)\n",
    "\n",
    "with open(basedir/\"inkuba_instruct_sample_mt.json\", \"r\") as f:\n",
    "    mt_examples = json.load(f)\n",
    "\n",
    "with open(basedir/\"inkuba_instruct_sample_qa.json\", \"r\") as f:\n",
    "    qa_examples = json.load(f)\n",
    "\n",
    "len(sent_examples), len(mt_examples), len(qa_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90763, 199987, 60000)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_examples = [d for d in sent_examples if d[\"input\"] not in comp_sent]\n",
    "mt_examples = [d for d in mt_examples if d[\"input\"] not in comp_mt]\n",
    "qa_examples = [d for d in qa_examples if d[\"input\"] not in comp_qa]\n",
    "len(sent_examples), len(mt_examples), len(qa_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350750"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sent_examples + mt_examples + qa_examples\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qa': 60000, 'sentiment': 90763, 'mmt': 199987}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks = {\"qa\": 0, \"sentiment\": 0, \"mmt\": 0}\n",
    "for d in data:\n",
    "    tasks[d[\"task\"]] += 1\n",
    "tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(basedir/\"inkuba_instruct_sample.json\", \"w\") as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
