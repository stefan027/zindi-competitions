{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7db0522b",
   "metadata": {},
   "source": [
    "# Buzuzu-Mavi Challenge - Finetuning InkubaLM (100M parameter version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cceefcea",
   "metadata": {},
   "source": [
    "**You will need a Huggingface account to be able to access the datasets and the models**\n",
    "\n",
    "Make sure you have generated an HF_TOKEN and added it to your notebook.\n",
    "\n",
    "ðŸš¨ NB: In order to access the Inkuba model, make sure you have requested and granted access to the Gated InkubaLM model on huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b17db981",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"HF_TOKEN\"] = \"\"\n",
    "login(token=os.environ[\"HF_TOKEN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e19327b-6c02-4881-ad02-9b6d3ec0b1b4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4e19327b-6c02-4881-ad02-9b6d3ec0b1b4",
    "outputId": "bcdfe2cb-d084-4920-d703-503131aabec3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.26.3\n",
      "matplotlib version: 3.7.3\n",
      "torch version: 2.6.0\n",
      "transformers version: 4.49.0\n",
      "accelerate version: 1.5.2\n",
      "sentencepiece version: 0.2.0\n",
      "protobuf version: 4.25.6\n",
      "sacrebleu version: 2.0.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "    \"numpy\",\n",
    "    \"matplotlib\",\n",
    "    \"torch\",\n",
    "    \"transformers\",\n",
    "    \"accelerate\",\n",
    "    \"sentencepiece\",\n",
    "    \"protobuf\",\n",
    "    \"sacrebleu\"\n",
    "]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4993e371",
   "metadata": {},
   "source": [
    "## Set up paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcbd868",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "basedir = Path(\"./\")\n",
    "data_path = basedir/\"inkuba_instruct_sample.json\"\n",
    "\n",
    "# if `tokenizer_path` is None, the InkubaLM tokenizer will be used without modification\n",
    "tokenizer_path = basedir/\"tokenizer\"\n",
    "\n",
    "# option to limit the number of translation examples per language\n",
    "# to ensure that the dataset is better balanced by task\n",
    "max_mt_data_per_lang = 50_000  # set to None to disable this option"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64b58f0",
   "metadata": {},
   "source": [
    "## Load the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1937b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 350815\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(data_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "print(\"Number of entries:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28e8874d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hausa', 'swahili'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langs = set([d.get(\"lang\", d.get(\"language\", \"\")) for d in data])\n",
    "langs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bd615e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qa': 60000, 'sentiment': 90828, 'mmt': 199987}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks = {\"qa\": 0, \"sentiment\": 0, \"mmt\": 0}\n",
    "for d in data:\n",
    "    tasks[d[\"task\"]] += 1\n",
    "tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418660c8",
   "metadata": {},
   "source": [
    "Shuffle the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5f7b156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350815"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(123)\n",
    "# data = data + data_sent + data_sent\n",
    "random.shuffle(data)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ecf0006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250828\n"
     ]
    }
   ],
   "source": [
    "if max_mt_data_per_lang is not None:\n",
    "    data_mt_swa = [d for d in data if d[\"task\"] == \"mmt\" and d[\"language\"] == \"swahili\"][:max_mt_data_per_lang]\n",
    "    data_mt_hau = [d for d in data if d[\"task\"] == \"mmt\" and d[\"language\"] == \"hausa\"][:max_mt_data_per_lang]\n",
    "    data = [d for d in data if d[\"task\"] != \"mmt\"]\n",
    "\n",
    "    random.seed(123)\n",
    "    data = data + data_mt_swa + data_mt_hau\n",
    "    random.shuffle(data)\n",
    "    print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7af8176-4255-4e92-8c7d-998771733eb8",
   "metadata": {
    "id": "d7af8176-4255-4e92-8c7d-998771733eb8"
   },
   "source": [
    "Look at a few data examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "-LiuBMsHkzQV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-LiuBMsHkzQV",
    "outputId": "a4ee5c2d-db53-4a80-e5ee-0bbcf6fe0450"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example entry:\n",
      " {'task': 'qa', 'instruction': 'You will be given two Hausa sentences. Your job is to  is to predict textual entailment. In other words, does sentence A imply/contradict/neither sentence B. Your response must be one word: entailment, contradiction, or neutral.', 'input': 'Sentence A: Otr ne , vermont , sabon , massachusetts , rhode , rhode , rhode , rhode , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland\\nSentence B: Massachusetts da New York da kuma daga OTR.', 'output': 'contradiction', 'lang': 'hausa'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Example entry:\\n\", data[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac3e3a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example entry:\n",
      " {'task': 'sentiment', 'instruction_orig': \"Gano ra'ayin da aka bayyana a cikin wannan rubutu. Bin waÉ—annan jagororin, kyakkyawa yana na rubutu na nufin kyakkyawan tunani, É—abi'a, da motsin rai. Korau na nuna rubutu na nufin mummunan tunani ko motsin rai. Tsaka-tsaki na nuna rubutu baya nufin magana mai kyau ko mara kyau kai tsaye ko a kaikaice.\", 'input': 'kai amma su buhari baajin kunyaryin karya', 'output': 'Korau', 'language': 'hausa', 'instruction': \"Gano ra'ayin da aka bayyana a cikin wannan rubutu. Bin waÉ—annan jagororin, kyakkyawa yana na rubutu na nufin kyakkyawan tunani, É—abi'a, da motsin rai. Korau na nuna rubutu na nufin mummunan tunani ko motsin rai. Tsaka-tsaki na nuna rubutu baya nufin magana mai kyau ko mara kyau kai tsaye ko a kaikaice.\"}\n"
     ]
    }
   ],
   "source": [
    "print(\"Example entry:\\n\", data[-7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ed4c06",
   "metadata": {},
   "source": [
    "## Prepare data for instruction tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "Jhk37nnJnkBh",
   "metadata": {
    "id": "Jhk37nnJnkBh"
   },
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "F9UQRfjzo4Js",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F9UQRfjzo4Js",
    "outputId": "7b615d35-2a5f-474d-9292-a69bc3850e16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "You will be given two Hausa sentences. Your job is to  is to predict textual entailment. In other words, does sentence A imply/contradict/neither sentence B. Your response must be one word: entailment, contradiction, or neutral.\n",
      "\n",
      "### Input:\n",
      "Sentence A: Otr ne , vermont , sabon , massachusetts , rhode , rhode , rhode , rhode , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland\n",
      "Sentence B: Massachusetts da New York da kuma daga OTR.\n",
      "\n",
      "### Response:\n",
      "contradiction\n"
     ]
    }
   ],
   "source": [
    "model_input = format_input(data[50])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
    "\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e37d0af",
   "metadata": {},
   "source": [
    "Train/validation/test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fb3e9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(123)\n",
    "\n",
    "train_data, test_data, val_data = [], [], []\n",
    "for example in data:\n",
    "    r = random.random()\n",
    "    if r < 0.85:\n",
    "        train_data.append(example)\n",
    "    elif r < 0.95:\n",
    "        test_data.append(example)\n",
    "    else:\n",
    "        val_data.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "-zf6oht6bIUQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-zf6oht6bIUQ",
    "outputId": "657ec5c6-4caa-4d1a-ba2e-23acd755ab07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 212841\n",
      "Validation set length: 12646\n",
      "Test set length: 25341\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Validation set length:\", len(val_data))\n",
    "print(\"Test set length:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaaf606-f913-4445-8301-632ae10d387d",
   "metadata": {
    "id": "fcaaf606-f913-4445-8301-632ae10d387d"
   },
   "source": [
    "### Create `Dataset` object to pre-tokenize the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adc29dc4-f1c7-4c71-937b-95119d6239bb",
   "metadata": {
    "id": "adc29dc4-f1c7-4c71-937b-95119d6239bb"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "\n",
    "        # Pre-tokenize texts\n",
    "        self.encoded_texts = []\n",
    "        for entry in data:\n",
    "            instruction_plus_input = format_input(entry)\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            self.encoded_texts.append(\n",
    "                tokenizer.encode(full_text)\n",
    "            )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_texts[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59586eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}\n",
      "2\n",
      "61788\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"lelapa/InkubaLM-0.4B\"\n",
    "if tokenizer_path is None:\n",
    "    tokenizer_path = model_name\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "print(tokenizer.special_tokens_map)\n",
    "print(tokenizer.convert_tokens_to_ids(\"</s>\"))\n",
    "print(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201423d2",
   "metadata": {},
   "source": [
    "### Define a custom collate function to create training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41ec6e2d-9eb2-4124-913e-d2af39be4cf2",
   "metadata": {
    "id": "41ec6e2d-9eb2-4124-913e-d2af39be4cf2"
   },
   "outputs": [],
   "source": [
    "def custom_collate_fn(\n",
    "    batch,\n",
    "    pad_token_id=2,\n",
    "    ignore_index=-100,\n",
    "    allowed_max_length=None,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    # Find the longest sequence in the batch\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "\n",
    "    # Pad and prepare inputs and targets\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Add an <|endoftext|> token\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to max_length\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
    "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
    "\n",
    "        # New: Replace all but the first padding tokens in targets by ignore_index\n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index\n",
    "\n",
    "        # New: Optionally truncate to maximum sequence length\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    # Convert list of inputs and targets to tensors and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c2306f",
   "metadata": {},
   "source": [
    "Check that it works as expected with a toy example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdf5eec4-9ebe-4be0-9fca-9a47bee88fdc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cdf5eec4-9ebe-4be0-9fca-9a47bee88fdc",
    "outputId": "e8f709b9-f4c5-428a-a6ac-2a4c1b9358ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3, 4],\n",
      "        [5, 6, 2, 2, 2],\n",
      "        [7, 8, 9, 2, 2]])\n",
      "tensor([[   1,    2,    3,    4, -100],\n",
      "        [   6,    2, -100, -100, -100],\n",
      "        [   8,    9,    2, -100, -100]])\n"
     ]
    }
   ],
   "source": [
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "\n",
    "batch = (\n",
    "    inputs_1,\n",
    "    inputs_2,\n",
    "    inputs_3\n",
    ")\n",
    "\n",
    "inputs, targets = custom_collate_fn(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccaf048-ec95-498c-9155-d5b3ccba6c96",
   "metadata": {
    "id": "bccaf048-ec95-498c-9155-d5b3ccba6c96"
   },
   "source": [
    "### Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "etpqqWh8phKc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "etpqqWh8phKc",
    "outputId": "b4391c33-1a89-455b-faaa-5f874b6eb409"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e47fb30-c2c6-4e6d-a64c-76cc65be4a2c",
   "metadata": {
    "id": "4e47fb30-c2c6-4e6d-a64c-76cc65be4a2c"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "customized_collate_fn = partial(\n",
    "    custom_collate_fn,\n",
    "    device=device,\n",
    "    allowed_max_length=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff42c29-8b81-45e5-ae8d-b97cd1cf447a",
   "metadata": {
    "id": "8ff42c29-8b81-45e5-ae8d-b97cd1cf447a"
   },
   "source": [
    "- Next, we instantiate the data loaders similar to previous chapters, except that we now provide our own collate function for the batching process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "BtWkgir6Hlpe",
   "metadata": {
    "id": "BtWkgir6Hlpe"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d097dc8-ad34-4f05-b435-e4147965f532",
   "metadata": {
    "id": "1d097dc8-ad34-4f05-b435-e4147965f532"
   },
   "outputs": [],
   "source": [
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f67c147-b1a2-4a95-9807-e2d0de0324c0",
   "metadata": {
    "id": "3f67c147-b1a2-4a95-9807-e2d0de0324c0"
   },
   "source": [
    "Let's see what the dimensions of the resulting input and target batches look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "GGs1AI3vHpnX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GGs1AI3vHpnX",
    "outputId": "f6a74c8b-1af3-4bc1-b48c-eda64b0200d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([8, 190]) torch.Size([8, 190])\n",
      "torch.Size([8, 173]) torch.Size([8, 173])\n",
      "torch.Size([8, 199]) torch.Size([8, 199])\n",
      "torch.Size([8, 203]) torch.Size([8, 203])\n",
      "torch.Size([8, 181]) torch.Size([8, 181])\n",
      "torch.Size([8, 177]) torch.Size([8, 177])\n",
      "torch.Size([8, 230]) torch.Size([8, 230])\n",
      "torch.Size([8, 163]) torch.Size([8, 163])\n",
      "torch.Size([8, 163]) torch.Size([8, 163])\n",
      "torch.Size([8, 642]) torch.Size([8, 642])\n",
      "torch.Size([8, 170]) torch.Size([8, 170])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for i, (inputs, targets) in enumerate(train_loader):\n",
    "    print(inputs.shape, targets.shape)\n",
    "    if i == 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21b8fd02-014f-4481-9b71-5bfee8f9dfcd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "21b8fd02-014f-4481-9b71-5bfee8f9dfcd",
    "outputId": "1b8ad342-2b5b-4f12-ad1a-3cb2a6c712ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    1, 13866,   338,   385, 15278,   393, 16612,   263,  3414, 29889,\n",
      "        14350,   263,  2933,   393,  2134, 29886, 48289,   873,  1614,  2167,\n",
      "          278,  2009, 29889,    13,    13,  2277, 29937, 50760,   428, 29901,\n",
      "           13, 44342,  1217,  1153, 29915, 39492,  1146, 32065, 32903,   263,\n",
      "          274,   638,   262, 32199, 40574, 29889, 27662, 11324, 43462, 13707,\n",
      "          432,   351,   272,   272,   262, 29892, 32501, 29895,  3459, 10011,\n",
      "        32078,  1055, 40574,  1055,   302,  1137,   262, 32501, 29895,  3459,\n",
      "         1450,   273,   260, 37230, 29875, 29892, 34014,  5365, 29915, 29874,\n",
      "        29892,  1146, 51191,   262, 35483, 29889,  8224,   336, 29884,  1055,\n",
      "        33189, 40574,  1055,   302,  1137,   262, 37406, 29885, 37230,   260,\n",
      "        37230, 29875,  5812, 51191,   262, 35483, 29889,   323, 35216, 29899,\n",
      "         1372,  9940,  1055, 33189, 40574, 33009,   302,  1137,   262, 34710,\n",
      "         5530, 32832,  5812, 32119, 32832, 32451, 37281,  5812,   263,   413,\n",
      "        35335,   625, 29889,    13,    13,  2277, 29937, 10567, 29901,    13,\n",
      "        38409, 36726, 37026,  5812,  7284,    13,    13,  2277, 29937, 13291,\n",
      "        29901,    13, 29911, 35216, 29899,  1372,  9940,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51649ab4-1a7e-4a9e-92c5-950a24fde211",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "51649ab4-1a7e-4a9e-92c5-950a24fde211",
    "outputId": "5e8c23f8-6a05-4c13-9f92-373b75b57ea6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([13866,   338,   385, 15278,   393, 16612,   263,  3414, 29889, 14350,\n",
      "          263,  2933,   393,  2134, 29886, 48289,   873,  1614,  2167,   278,\n",
      "         2009, 29889,    13,    13,  2277, 29937, 50760,   428, 29901,    13,\n",
      "        44342,  1217,  1153, 29915, 39492,  1146, 32065, 32903,   263,   274,\n",
      "          638,   262, 32199, 40574, 29889, 27662, 11324, 43462, 13707,   432,\n",
      "          351,   272,   272,   262, 29892, 32501, 29895,  3459, 10011, 32078,\n",
      "         1055, 40574,  1055,   302,  1137,   262, 32501, 29895,  3459,  1450,\n",
      "          273,   260, 37230, 29875, 29892, 34014,  5365, 29915, 29874, 29892,\n",
      "         1146, 51191,   262, 35483, 29889,  8224,   336, 29884,  1055, 33189,\n",
      "        40574,  1055,   302,  1137,   262, 37406, 29885, 37230,   260, 37230,\n",
      "        29875,  5812, 51191,   262, 35483, 29889,   323, 35216, 29899,  1372,\n",
      "         9940,  1055, 33189, 40574, 33009,   302,  1137,   262, 34710,  5530,\n",
      "        32832,  5812, 32119, 32832, 32451, 37281,  5812,   263,   413, 35335,\n",
      "          625, 29889,    13,    13,  2277, 29937, 10567, 29901,    13, 38409,\n",
      "        36726, 37026,  5812,  7284,    13,    13,  2277, 29937, 13291, 29901,\n",
      "           13, 29911, 35216, 29899,  1372,  9940,     2,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(targets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfedf93",
   "metadata": {},
   "source": [
    "(TODO: mask out tokens corresponding to the instruction.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6aad445-8f19-4238-b9bf-db80767fb91a",
   "metadata": {
    "id": "d6aad445-8f19-4238-b9bf-db80767fb91a"
   },
   "source": [
    "## Load InkubaLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a658a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.11/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "2025-04-09 16:23:10.317844: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-09 16:23:10.374617: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-09 16:23:10.374668: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-09 16:23:10.375979: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-09 16:23:10.382778: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-09 16:23:11.701624: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "VulavulaLlamaForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory (GB): 0.39\n",
      "num params:  105,493,504\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_name = \"lelapa/InkubaLM-0.4B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    return_dict=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float32,\n",
    ")\n",
    "\n",
    "# Update the model config and load the pruned model weights\n",
    "config_prn = deepcopy(model.config)\n",
    "config_prn.vocab_size = tokenizer.vocab_size\n",
    "config_prn.hidden_size = 1024\n",
    "config_prn.intermediate_size = 2816\n",
    "config_prn.max_position_embeddings = 1024\n",
    "config_prn.tie_word_embeddings = True\n",
    "model = AutoModelForCausalLM.from_config(config_prn).to(torch.float32).to(device)\n",
    "model.load_state_dict(torch.load(basedir/\"pruned_model_60k_init.pth\"))\n",
    "\n",
    "model.eval();\n",
    "print(f\"memory (GB): {model.get_memory_footprint() / 1024**3:.2f}\")\n",
    "print(f\"num params:  {model.num_parameters():,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4c1801",
   "metadata": {},
   "source": [
    "### Define a `generate` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dccb6c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond).logits\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float('-inf')).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d27b9d-a942-4cf5-b797-848c5f01e723",
   "metadata": {
    "id": "70d27b9d-a942-4cf5-b797-848c5f01e723"
   },
   "source": [
    "## Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5193f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch).logits\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00083059-aa41-4d37-8a17-1c72d1b1ca00",
   "metadata": {
    "id": "00083059-aa41-4d37-8a17-1c72d1b1ca00"
   },
   "source": [
    "Check that loss functions work as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d99fc6f8-63b2-43da-adbb-a7b6b92c8dd5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d99fc6f8-63b2-43da-adbb-a7b6b92c8dd5",
    "outputId": "a3f5e1b0-093a-4c51-e7fc-c9cac48c2ea2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 12.079226684570312\n",
      "Validation loss: 12.087370491027832\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13177744",
   "metadata": {},
   "source": [
    "### Define functions to handle the training and validation loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e7d66a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (B, T) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond).logits\n",
    "\n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_token, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest logits value\n",
    "        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    # context_size = model.pos_emb.weight.shape[0]\n",
    "    context_size = model.config.max_position_embeddings\n",
    "    # encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    encoded = tokenizer.encode(start_context, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=100, context_size=context_size\n",
    "        )\n",
    "        # decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "        decoded_text = tokenizer.decode(token_ids[0].tolist(), skip_special_tokens=True)\n",
    "        print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()\n",
    "\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer, scheduler=None):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()  # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()  # Calculate loss gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()  # Update model weights using loss gradients\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "824289a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches per epoch 26605\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of batches per epoch\", len(train_data)//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bcf83a-1fff-4540-97c1-765c4016d5e3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "78bcf83a-1fff-4540-97c1-765c4016d5e3",
    "outputId": "ecb9a3dd-97c0-492d-8a51-fbd175bb139b"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "num_epochs = 2\n",
    "lr = 0.00005\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, total_steps=len(train_data)//batch_size*num_epochs)\n",
    "\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=200, eval_iter=10,\n",
    "    start_context=format_input(val_data[0]), tokenizer=tokenizer,\n",
    "    scheduler=scheduler\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf10e95",
   "metadata": {},
   "source": [
    "Let's look at a few validation examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VQ2NZMbfucAc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VQ2NZMbfucAc",
    "outputId": "066c56ff-b52a-4ee6-eae7-1bddfc74d0c1"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "\n",
    "for entry in test_data[:3]:\n",
    "\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=tokenizer.encode(input_text, return_tensors=\"pt\").to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=1024,\n",
    "        eos_id=2\n",
    "    )\n",
    "    generated_text = tokenizer.decode(token_ids[0].tolist(), skip_special_tokens=True)\n",
    "    response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "    print(input_text)\n",
    "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
    "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
    "    print(\"-------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b2f3f6-8569-405a-9db6-d47cba65608a",
   "metadata": {
    "id": "c1b2f3f6-8569-405a-9db6-d47cba65608a"
   },
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cBU0iHmVfOI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8cBU0iHmVfOI",
    "outputId": "135849ed-9acd-43a2-f438-053d07dae9b2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_name = basedir/\"vulavula_inkuba_instruct_tokenizer60k_pruned.pth\"\n",
    "torch.save(model.state_dict(), file_name)\n",
    "print(f\"Model saved as {file_name}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
