{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7db0522b",
   "metadata": {},
   "source": [
    "# Buzuzu-Mavi Challenge - Finetuning InkubaLM (50M parameter version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cceefcea",
   "metadata": {},
   "source": [
    "**You will need a Huggingface account to be able to access the datasets and the models**\n",
    "\n",
    "Make sure you have generated an HF_TOKEN and added it to your notebook.\n",
    "\n",
    "ðŸš¨ NB: In order to access the Inkuba model, make sure you have requested and granted access to the Gated InkubaLM model on huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17db981",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"HF_TOKEN\"] = \"\"\n",
    "login(token=os.environ[\"HF_TOKEN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e19327b-6c02-4881-ad02-9b6d3ec0b1b4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4e19327b-6c02-4881-ad02-9b6d3ec0b1b4",
    "outputId": "bcdfe2cb-d084-4920-d703-503131aabec3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.26.3\n",
      "matplotlib version: 3.7.3\n",
      "torch version: 2.6.0\n",
      "transformers version: 4.49.0\n",
      "accelerate version: 1.5.2\n",
      "sentencepiece version: 0.2.0\n",
      "protobuf version: 4.25.6\n",
      "sacrebleu version: 2.0.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "    \"numpy\",\n",
    "    \"matplotlib\",\n",
    "    \"torch\",\n",
    "    \"transformers\",\n",
    "    \"accelerate\",\n",
    "    \"sentencepiece\",\n",
    "    \"protobuf\",\n",
    "    \"sacrebleu\"\n",
    "]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4993e371",
   "metadata": {},
   "source": [
    "## Set up paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcbd868",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "basedir = Path(\"/.\")\n",
    "data_path = basedir/\"inkuba_instruct_sample.json\"\n",
    "\n",
    "# if `tokenizer_path` is None, the InkubaLM tokenizer will be used without modification\n",
    "tokenizer_path = basedir/\"tokenizer\"\n",
    "\n",
    "# option to limit the number of translation examples per language\n",
    "# to ensure that the dataset is better balanced by task\n",
    "max_mt_data_per_lang = 50_000  # set to None to disable this option"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64b58f0",
   "metadata": {},
   "source": [
    "## Load the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1937b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 350815\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(data_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "print(\"Number of entries:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28e8874d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hausa', 'swahili'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langs = set([d.get(\"lang\", d.get(\"language\", \"\")) for d in data])\n",
    "langs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bd615e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qa': 60000, 'sentiment': 90828, 'mmt': 199987}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks = {\"qa\": 0, \"sentiment\": 0, \"mmt\": 0}\n",
    "for d in data:\n",
    "    tasks[d[\"task\"]] += 1\n",
    "tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418660c8",
   "metadata": {},
   "source": [
    "Shuffle the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5f7b156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350815"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(123)\n",
    "# data = data + data_sent + data_sent\n",
    "random.shuffle(data)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ecf0006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250828\n"
     ]
    }
   ],
   "source": [
    "if max_mt_data_per_lang is not None:\n",
    "    data_mt_swa = [d for d in data if d[\"task\"] == \"mmt\" and d[\"language\"] == \"swahili\"][:max_mt_data_per_lang]\n",
    "    data_mt_hau = [d for d in data if d[\"task\"] == \"mmt\" and d[\"language\"] == \"hausa\"][:max_mt_data_per_lang]\n",
    "    data = [d for d in data if d[\"task\"] != \"mmt\"]\n",
    "\n",
    "    random.seed(123)\n",
    "    data = data + data_mt_swa + data_mt_hau\n",
    "    random.shuffle(data)\n",
    "    print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7af8176-4255-4e92-8c7d-998771733eb8",
   "metadata": {
    "id": "d7af8176-4255-4e92-8c7d-998771733eb8"
   },
   "source": [
    "Look at a few data examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "-LiuBMsHkzQV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-LiuBMsHkzQV",
    "outputId": "a4ee5c2d-db53-4a80-e5ee-0bbcf6fe0450"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example entry:\n",
      " {'task': 'qa', 'instruction': 'You will be given two Hausa sentences. Your job is to  is to predict textual entailment. In other words, does sentence A imply/contradict/neither sentence B. Your response must be one word: entailment, contradiction, or neutral.', 'input': 'Sentence A: Otr ne , vermont , sabon , massachusetts , rhode , rhode , rhode , rhode , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland\\nSentence B: Massachusetts da New York da kuma daga OTR.', 'output': 'contradiction', 'lang': 'hausa'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Example entry:\\n\", data[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac3e3a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example entry:\n",
      " {'task': 'sentiment', 'instruction_orig': \"Gano ra'ayin da aka bayyana a cikin wannan rubutu. Bin waÉ—annan jagororin, kyakkyawa yana na rubutu na nufin kyakkyawan tunani, É—abi'a, da motsin rai. Korau na nuna rubutu na nufin mummunan tunani ko motsin rai. Tsaka-tsaki na nuna rubutu baya nufin magana mai kyau ko mara kyau kai tsaye ko a kaikaice.\", 'input': 'kai amma su buhari baajin kunyaryin karya', 'output': 'Korau', 'language': 'hausa', 'instruction': \"Gano ra'ayin da aka bayyana a cikin wannan rubutu. Bin waÉ—annan jagororin, kyakkyawa yana na rubutu na nufin kyakkyawan tunani, É—abi'a, da motsin rai. Korau na nuna rubutu na nufin mummunan tunani ko motsin rai. Tsaka-tsaki na nuna rubutu baya nufin magana mai kyau ko mara kyau kai tsaye ko a kaikaice.\"}\n"
     ]
    }
   ],
   "source": [
    "print(\"Example entry:\\n\", data[-7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ed4c06",
   "metadata": {},
   "source": [
    "## Prepare data for instruction tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "Jhk37nnJnkBh",
   "metadata": {
    "id": "Jhk37nnJnkBh"
   },
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "F9UQRfjzo4Js",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F9UQRfjzo4Js",
    "outputId": "7b615d35-2a5f-474d-9292-a69bc3850e16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "You will be given two Hausa sentences. Your job is to  is to predict textual entailment. In other words, does sentence A imply/contradict/neither sentence B. Your response must be one word: entailment, contradiction, or neutral.\n",
      "\n",
      "### Input:\n",
      "Sentence A: Otr ne , vermont , sabon , massachusetts , rhode , rhode , rhode , rhode , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland , maryland\n",
      "Sentence B: Massachusetts da New York da kuma daga OTR.\n",
      "\n",
      "### Response:\n",
      "contradiction\n"
     ]
    }
   ],
   "source": [
    "model_input = format_input(data[50])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
    "\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e37d0af",
   "metadata": {},
   "source": [
    "Train/validation/test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fb3e9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(123)\n",
    "\n",
    "train_data, test_data, val_data = [], [], []\n",
    "for example in data:\n",
    "    r = random.random()\n",
    "    if r < 0.85:\n",
    "        train_data.append(example)\n",
    "    elif r < 0.95:\n",
    "        test_data.append(example)\n",
    "    else:\n",
    "        val_data.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "-zf6oht6bIUQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-zf6oht6bIUQ",
    "outputId": "657ec5c6-4caa-4d1a-ba2e-23acd755ab07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 212841\n",
      "Validation set length: 12646\n",
      "Test set length: 25341\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Validation set length:\", len(val_data))\n",
    "print(\"Test set length:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaaf606-f913-4445-8301-632ae10d387d",
   "metadata": {
    "id": "fcaaf606-f913-4445-8301-632ae10d387d"
   },
   "source": [
    "### Create `Dataset` object to pre-tokenize the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adc29dc4-f1c7-4c71-937b-95119d6239bb",
   "metadata": {
    "id": "adc29dc4-f1c7-4c71-937b-95119d6239bb"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "\n",
    "        # Pre-tokenize texts\n",
    "        self.encoded_texts = []\n",
    "        for entry in data:\n",
    "            instruction_plus_input = format_input(entry)\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            self.encoded_texts.append(\n",
    "                tokenizer.encode(full_text)\n",
    "            )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_texts[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59586eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}\n",
      "2\n",
      "8064\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"lelapa/InkubaLM-0.4B\"\n",
    "if tokenizer_path is None:\n",
    "    tokenizer_path = model_name\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "print(tokenizer.special_tokens_map)\n",
    "print(tokenizer.convert_tokens_to_ids(\"</s>\"))\n",
    "print(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201423d2",
   "metadata": {},
   "source": [
    "### Define a custom collate function to create training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41ec6e2d-9eb2-4124-913e-d2af39be4cf2",
   "metadata": {
    "id": "41ec6e2d-9eb2-4124-913e-d2af39be4cf2"
   },
   "outputs": [],
   "source": [
    "def custom_collate_fn(\n",
    "    batch,\n",
    "    pad_token_id=2,\n",
    "    ignore_index=-100,\n",
    "    allowed_max_length=None,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    # Find the longest sequence in the batch\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "\n",
    "    # Pad and prepare inputs and targets\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Add an <|endoftext|> token\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to max_length\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
    "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
    "\n",
    "        # New: Replace all but the first padding tokens in targets by ignore_index\n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index\n",
    "\n",
    "        # New: Optionally truncate to maximum sequence length\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    # Convert list of inputs and targets to tensors and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c2306f",
   "metadata": {},
   "source": [
    "Check that it works as expected with a toy example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdf5eec4-9ebe-4be0-9fca-9a47bee88fdc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cdf5eec4-9ebe-4be0-9fca-9a47bee88fdc",
    "outputId": "e8f709b9-f4c5-428a-a6ac-2a4c1b9358ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3, 4],\n",
      "        [5, 6, 2, 2, 2],\n",
      "        [7, 8, 9, 2, 2]])\n",
      "tensor([[   1,    2,    3,    4, -100],\n",
      "        [   6,    2, -100, -100, -100],\n",
      "        [   8,    9,    2, -100, -100]])\n"
     ]
    }
   ],
   "source": [
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "\n",
    "batch = (\n",
    "    inputs_1,\n",
    "    inputs_2,\n",
    "    inputs_3\n",
    ")\n",
    "\n",
    "inputs, targets = custom_collate_fn(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccaf048-ec95-498c-9155-d5b3ccba6c96",
   "metadata": {
    "id": "bccaf048-ec95-498c-9155-d5b3ccba6c96"
   },
   "source": [
    "### Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "etpqqWh8phKc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "etpqqWh8phKc",
    "outputId": "b4391c33-1a89-455b-faaa-5f874b6eb409"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e47fb30-c2c6-4e6d-a64c-76cc65be4a2c",
   "metadata": {
    "id": "4e47fb30-c2c6-4e6d-a64c-76cc65be4a2c"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "customized_collate_fn = partial(\n",
    "    custom_collate_fn,\n",
    "    device=device,\n",
    "    allowed_max_length=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff42c29-8b81-45e5-ae8d-b97cd1cf447a",
   "metadata": {
    "id": "8ff42c29-8b81-45e5-ae8d-b97cd1cf447a"
   },
   "source": [
    "- Next, we instantiate the data loaders similar to previous chapters, except that we now provide our own collate function for the batching process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "BtWkgir6Hlpe",
   "metadata": {
    "id": "BtWkgir6Hlpe"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d097dc8-ad34-4f05-b435-e4147965f532",
   "metadata": {
    "id": "1d097dc8-ad34-4f05-b435-e4147965f532"
   },
   "outputs": [],
   "source": [
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f67c147-b1a2-4a95-9807-e2d0de0324c0",
   "metadata": {
    "id": "3f67c147-b1a2-4a95-9807-e2d0de0324c0"
   },
   "source": [
    "Let's see what the dimensions of the resulting input and target batches look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "GGs1AI3vHpnX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GGs1AI3vHpnX",
    "outputId": "f6a74c8b-1af3-4bc1-b48c-eda64b0200d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([8, 229]) torch.Size([8, 229])\n",
      "torch.Size([8, 208]) torch.Size([8, 208])\n",
      "torch.Size([8, 271]) torch.Size([8, 271])\n",
      "torch.Size([8, 228]) torch.Size([8, 228])\n",
      "torch.Size([8, 223]) torch.Size([8, 223])\n",
      "torch.Size([8, 221]) torch.Size([8, 221])\n",
      "torch.Size([8, 262]) torch.Size([8, 262])\n",
      "torch.Size([8, 209]) torch.Size([8, 209])\n",
      "torch.Size([8, 205]) torch.Size([8, 205])\n",
      "torch.Size([8, 688]) torch.Size([8, 688])\n",
      "torch.Size([8, 210]) torch.Size([8, 210])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for i, (inputs, targets) in enumerate(train_loader):\n",
    "    print(inputs.shape, targets.shape)\n",
    "    if i == 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21b8fd02-014f-4481-9b71-5bfee8f9dfcd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "21b8fd02-014f-4481-9b71-5bfee8f9dfcd",
    "outputId": "1b8ad342-2b5b-4f12-ad1a-3cb2a6c712ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   1,  347,  294,  337,  335,  382,  296,  302,  540,  419,  390,  517,\n",
      "        4074,  371, 2319,  263, 1768, 4080,  396, 4069,  531,  263,  572, 4077,\n",
      "         265,  341,  390, 1335, 4077, 7467,  736, 1130, 1343,  277,  334,  336,\n",
      "         339, 4080,   13,   13, 1388, 4116,  484,  302,  540,  419, 4092,   13,\n",
      "        7153,  935,  901, 4101, 6600,  897, 4225, 4721,  263,  273,  587,  262,\n",
      "        4325, 6746, 4080, 4013, 3173, 7066, 3362,  423,  348,  271,  271,  262,\n",
      "        4083, 4504, 4086, 1781, 3011, 4236,  845, 6746,  845,  301,  892,  262,\n",
      "        4504, 4086, 1781, 1059,  272,  260, 6188, 4066, 4083, 5231, 2243, 4101,\n",
      "        4065, 4083,  897, 7639,  262, 5760, 4080, 2766,  333, 4075,  845, 4863,\n",
      "        6746,  845,  301,  892,  262, 6225, 4076, 6188,  260, 6188, 4066, 2347,\n",
      "        7639,  262, 5760, 4080,  320, 5680, 4090, 1022, 3001,  845, 4863, 6746,\n",
      "        4774,  301,  892,  262, 5493, 2285, 4685, 2347, 4268, 4685, 4475, 6200,\n",
      "        2347,  263,  409, 5718,  575, 4080,   13,   13, 1388, 4116,  484, 4077,\n",
      "         326, 4092,   13, 6402, 6081, 6146, 2347, 2630,   13,   13, 1388, 4116,\n",
      "         387,  267, 4077,  265,  341, 4092,   13, 4099, 5680, 4090, 1022, 3001,\n",
      "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "           2,    2,    2,    2,    2,    2], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51649ab4-1a7e-4a9e-92c5-950a24fde211",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "51649ab4-1a7e-4a9e-92c5-950a24fde211",
    "outputId": "5e8c23f8-6a05-4c13-9f92-373b75b57ea6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 347,  294,  337,  335,  382,  296,  302,  540,  419,  390,  517, 4074,\n",
      "         371, 2319,  263, 1768, 4080,  396, 4069,  531,  263,  572, 4077,  265,\n",
      "         341,  390, 1335, 4077, 7467,  736, 1130, 1343,  277,  334,  336,  339,\n",
      "        4080,   13,   13, 1388, 4116,  484,  302,  540,  419, 4092,   13, 7153,\n",
      "         935,  901, 4101, 6600,  897, 4225, 4721,  263,  273,  587,  262, 4325,\n",
      "        6746, 4080, 4013, 3173, 7066, 3362,  423,  348,  271,  271,  262, 4083,\n",
      "        4504, 4086, 1781, 3011, 4236,  845, 6746,  845,  301,  892,  262, 4504,\n",
      "        4086, 1781, 1059,  272,  260, 6188, 4066, 4083, 5231, 2243, 4101, 4065,\n",
      "        4083,  897, 7639,  262, 5760, 4080, 2766,  333, 4075,  845, 4863, 6746,\n",
      "         845,  301,  892,  262, 6225, 4076, 6188,  260, 6188, 4066, 2347, 7639,\n",
      "         262, 5760, 4080,  320, 5680, 4090, 1022, 3001,  845, 4863, 6746, 4774,\n",
      "         301,  892,  262, 5493, 2285, 4685, 2347, 4268, 4685, 4475, 6200, 2347,\n",
      "         263,  409, 5718,  575, 4080,   13,   13, 1388, 4116,  484, 4077,  326,\n",
      "        4092,   13, 6402, 6081, 6146, 2347, 2630,   13,   13, 1388, 4116,  387,\n",
      "         267, 4077,  265,  341, 4092,   13, 4099, 5680, 4090, 1022, 3001,    2,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(targets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfedf93",
   "metadata": {},
   "source": [
    "(TODO: mask out tokens corresponding to the instruction.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6aad445-8f19-4238-b9bf-db80767fb91a",
   "metadata": {
    "id": "d6aad445-8f19-4238-b9bf-db80767fb91a"
   },
   "source": [
    "## Load InkubaLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a658a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85ea10755e1c4e528920fe0230d6820a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/763 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f82ee4e43d24a2a8f0e63643d3a0233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vulavulaslm.py:   0%|          | 0.00/42.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/lelapa/InkubaLM-0.4B:\n",
      "- vulavulaslm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.11/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "2025-04-09 11:00:26.294404: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-09 11:00:26.836077: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-09 11:00:26.836210: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-09 11:00:26.918774: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-09 11:00:27.092207: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-09 11:00:29.262726: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23c5b6ca983943b4b968ba4e08858754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98a42816250b442385488179a44b64c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VulavulaLlamaForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory (GB): 0.19\n",
      "num params:  50,480,128\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_name = \"lelapa/InkubaLM-0.4B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    return_dict=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float32,\n",
    ")\n",
    "\n",
    "# Update the model config and load the pruned model weights\n",
    "config_prn = deepcopy(model.config)\n",
    "config_prn.vocab_size = tokenizer.vocab_size\n",
    "config_prn.hidden_size = 1024\n",
    "config_prn.intermediate_size = 2816\n",
    "config_prn.max_position_embeddings = 1024\n",
    "config_prn.tie_word_embeddings = True\n",
    "model = AutoModelForCausalLM.from_config(config_prn).to(torch.float32).to(device)\n",
    "model.load_state_dict(torch.load(basedir/\"pruned_model_init.pth\"))\n",
    "\n",
    "model.eval();\n",
    "print(f\"memory (GB): {model.get_memory_footprint() / 1024**3:.2f}\")\n",
    "print(f\"num params:  {model.num_parameters():,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4c1801",
   "metadata": {},
   "source": [
    "### Define a `generate` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dccb6c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond).logits\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float('-inf')).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d27b9d-a942-4cf5-b797-848c5f01e723",
   "metadata": {
    "id": "70d27b9d-a942-4cf5-b797-848c5f01e723"
   },
   "source": [
    "## Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5193f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch).logits\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00083059-aa41-4d37-8a17-1c72d1b1ca00",
   "metadata": {
    "id": "00083059-aa41-4d37-8a17-1c72d1b1ca00"
   },
   "source": [
    "Check that loss functions work as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d99fc6f8-63b2-43da-adbb-a7b6b92c8dd5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d99fc6f8-63b2-43da-adbb-a7b6b92c8dd5",
    "outputId": "a3f5e1b0-093a-4c51-e7fc-c9cac48c2ea2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.082386016845703\n",
      "Validation loss: 10.069643211364745\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13177744",
   "metadata": {},
   "source": [
    "### Define functions to handle the training and validation loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e7d66a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (B, T) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond).logits\n",
    "\n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_token, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest logits value\n",
    "        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    # context_size = model.pos_emb.weight.shape[0]\n",
    "    context_size = model.config.max_position_embeddings\n",
    "    # encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    encoded = tokenizer.encode(start_context, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=100, context_size=context_size\n",
    "        )\n",
    "        # decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "        decoded_text = tokenizer.decode(token_ids[0].tolist(), skip_special_tokens=True)\n",
    "        print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()\n",
    "\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer, scheduler=None):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()  # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()  # Calculate loss gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()  # Update model weights using loss gradients\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824289a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batched per epoch 26605\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of batches per epoch\", len(train_data)//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "78bcf83a-1fff-4540-97c1-765c4016d5e3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "78bcf83a-1fff-4540-97c1-765c4016d5e3",
    "outputId": "ecb9a3dd-97c0-492d-8a51-fbd175bb139b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 8.660, Val loss 8.686\n",
      "Ep 1 (Step 000200): Train loss 1.987, Val loss 1.919\n",
      "Ep 1 (Step 000400): Train loss 1.824, Val loss 1.736\n",
      "Ep 1 (Step 000600): Train loss 1.521, Val loss 1.654\n",
      "Ep 1 (Step 000800): Train loss 1.562, Val loss 1.572\n",
      "Ep 1 (Step 001000): Train loss 1.583, Val loss 1.511\n",
      "Ep 1 (Step 001200): Train loss 1.581, Val loss 1.468\n",
      "Ep 1 (Step 001400): Train loss 1.320, Val loss 1.443\n",
      "Ep 1 (Step 001600): Train loss 1.488, Val loss 1.417\n",
      "Ep 1 (Step 001800): Train loss 1.270, Val loss 1.400\n",
      "Ep 1 (Step 002000): Train loss 1.349, Val loss 1.382\n",
      "Ep 1 (Step 002200): Train loss 1.473, Val loss 1.370\n",
      "Ep 1 (Step 002400): Train loss 1.235, Val loss 1.350\n",
      "Ep 1 (Step 002600): Train loss 1.238, Val loss 1.337\n",
      "Ep 1 (Step 002800): Train loss 1.121, Val loss 1.322\n",
      "Ep 1 (Step 003000): Train loss 1.396, Val loss 1.311\n",
      "Ep 1 (Step 003200): Train loss 1.207, Val loss 1.308\n",
      "Ep 1 (Step 003400): Train loss 1.414, Val loss 1.292\n",
      "Ep 1 (Step 003600): Train loss 1.274, Val loss 1.285\n",
      "Ep 1 (Step 003800): Train loss 1.329, Val loss 1.278\n",
      "Ep 1 (Step 004000): Train loss 1.363, Val loss 1.273\n",
      "Ep 1 (Step 004200): Train loss 1.193, Val loss 1.269\n",
      "Ep 1 (Step 004400): Train loss 1.291, Val loss 1.268\n",
      "Ep 1 (Step 004600): Train loss 1.124, Val loss 1.260\n",
      "Ep 1 (Step 004800): Train loss 1.219, Val loss 1.258\n",
      "Ep 1 (Step 005000): Train loss 1.212, Val loss 1.248\n",
      "Ep 1 (Step 005200): Train loss 1.365, Val loss 1.240\n",
      "Ep 1 (Step 005400): Train loss 1.114, Val loss 1.243\n",
      "Ep 1 (Step 005600): Train loss 1.208, Val loss 1.231\n",
      "Ep 1 (Step 005800): Train loss 1.150, Val loss 1.224\n",
      "Ep 1 (Step 006000): Train loss 1.223, Val loss 1.225\n",
      "Ep 1 (Step 006200): Train loss 1.257, Val loss 1.219\n",
      "Ep 1 (Step 006400): Train loss 1.188, Val loss 1.218\n",
      "Ep 1 (Step 006600): Train loss 1.073, Val loss 1.207\n",
      "Ep 1 (Step 006800): Train loss 1.082, Val loss 1.208\n",
      "Ep 1 (Step 007000): Train loss 1.074, Val loss 1.202\n",
      "Ep 1 (Step 007200): Train loss 1.099, Val loss 1.193\n",
      "Ep 1 (Step 007400): Train loss 1.096, Val loss 1.181\n",
      "Ep 1 (Step 007600): Train loss 1.217, Val loss 1.188\n",
      "Ep 1 (Step 007800): Train loss 1.053, Val loss 1.184\n",
      "Ep 1 (Step 008000): Train loss 1.257, Val loss 1.172\n",
      "Ep 1 (Step 008200): Train loss 1.188, Val loss 1.177\n",
      "Ep 1 (Step 008400): Train loss 1.083, Val loss 1.164\n",
      "Ep 1 (Step 008600): Train loss 0.977, Val loss 1.171\n",
      "Ep 1 (Step 008800): Train loss 1.173, Val loss 1.168\n",
      "Ep 1 (Step 009000): Train loss 1.135, Val loss 1.167\n",
      "Ep 1 (Step 009200): Train loss 1.106, Val loss 1.162\n",
      "Ep 1 (Step 009400): Train loss 1.093, Val loss 1.166\n",
      "Ep 1 (Step 009600): Train loss 1.183, Val loss 1.160\n",
      "Ep 1 (Step 009800): Train loss 1.082, Val loss 1.167\n",
      "Ep 1 (Step 010000): Train loss 1.232, Val loss 1.150\n",
      "Ep 1 (Step 010200): Train loss 1.244, Val loss 1.160\n",
      "Ep 1 (Step 010400): Train loss 1.237, Val loss 1.166\n",
      "Ep 1 (Step 010600): Train loss 1.293, Val loss 1.163\n",
      "Ep 1 (Step 010800): Train loss 1.147, Val loss 1.158\n",
      "Ep 1 (Step 011000): Train loss 0.991, Val loss 1.154\n",
      "Ep 1 (Step 011200): Train loss 1.182, Val loss 1.147\n",
      "Ep 1 (Step 011400): Train loss 1.224, Val loss 1.150\n",
      "Ep 1 (Step 011600): Train loss 1.167, Val loss 1.148\n",
      "Ep 1 (Step 011800): Train loss 1.116, Val loss 1.129\n",
      "Ep 1 (Step 012000): Train loss 1.087, Val loss 1.129\n",
      "Ep 1 (Step 012200): Train loss 1.104, Val loss 1.129\n",
      "Ep 1 (Step 012400): Train loss 1.115, Val loss 1.137\n",
      "Ep 1 (Step 012600): Train loss 1.099, Val loss 1.127\n",
      "Ep 1 (Step 012800): Train loss 1.145, Val loss 1.133\n",
      "Ep 1 (Step 013000): Train loss 1.117, Val loss 1.130\n",
      "Ep 1 (Step 013200): Train loss 1.100, Val loss 1.136\n",
      "Ep 1 (Step 013400): Train loss 1.148, Val loss 1.124\n",
      "Ep 1 (Step 013600): Train loss 1.173, Val loss 1.137\n",
      "Ep 1 (Step 013800): Train loss 1.068, Val loss 1.130\n",
      "Ep 1 (Step 014000): Train loss 1.132, Val loss 1.132\n",
      "Ep 1 (Step 014200): Train loss 1.188, Val loss 1.132\n",
      "Ep 1 (Step 014400): Train loss 0.948, Val loss 1.126\n",
      "Ep 1 (Step 014600): Train loss 1.090, Val loss 1.128\n",
      "Ep 1 (Step 014800): Train loss 1.078, Val loss 1.122\n",
      "Ep 1 (Step 015000): Train loss 1.181, Val loss 1.124\n",
      "Ep 1 (Step 015200): Train loss 1.050, Val loss 1.109\n",
      "Ep 1 (Step 015400): Train loss 1.104, Val loss 1.107\n",
      "Ep 1 (Step 015600): Train loss 1.022, Val loss 1.119\n",
      "Ep 1 (Step 015800): Train loss 1.190, Val loss 1.118\n",
      "Ep 1 (Step 016000): Train loss 0.993, Val loss 1.118\n",
      "Ep 1 (Step 016200): Train loss 1.068, Val loss 1.123\n",
      "Ep 1 (Step 016400): Train loss 1.141, Val loss 1.113\n",
      "Ep 1 (Step 016600): Train loss 1.072, Val loss 1.120\n",
      "Ep 1 (Step 016800): Train loss 0.944, Val loss 1.125\n",
      "Ep 1 (Step 017000): Train loss 1.131, Val loss 1.120\n",
      "Ep 1 (Step 017200): Train loss 1.096, Val loss 1.099\n",
      "Ep 1 (Step 017400): Train loss 1.150, Val loss 1.110\n",
      "Ep 1 (Step 017600): Train loss 1.158, Val loss 1.116\n",
      "Ep 1 (Step 017800): Train loss 1.033, Val loss 1.120\n",
      "Ep 1 (Step 018000): Train loss 1.059, Val loss 1.109\n",
      "Ep 1 (Step 018200): Train loss 1.074, Val loss 1.107\n",
      "Ep 1 (Step 018400): Train loss 1.119, Val loss 1.098\n",
      "Ep 1 (Step 018600): Train loss 1.042, Val loss 1.103\n",
      "Ep 1 (Step 018800): Train loss 1.108, Val loss 1.099\n",
      "Ep 1 (Step 019000): Train loss 1.185, Val loss 1.097\n",
      "Ep 1 (Step 019200): Train loss 1.136, Val loss 1.092\n",
      "Ep 1 (Step 019400): Train loss 1.054, Val loss 1.091\n",
      "Ep 1 (Step 019600): Train loss 0.979, Val loss 1.097\n",
      "Ep 1 (Step 019800): Train loss 1.108, Val loss 1.102\n",
      "Ep 1 (Step 020000): Train loss 1.113, Val loss 1.093\n",
      "Ep 1 (Step 020200): Train loss 0.897, Val loss 1.089\n",
      "Ep 1 (Step 020400): Train loss 1.183, Val loss 1.096\n",
      "Ep 1 (Step 020600): Train loss 1.001, Val loss 1.089\n",
      "Ep 1 (Step 020800): Train loss 0.974, Val loss 1.091\n",
      "Ep 1 (Step 021000): Train loss 1.009, Val loss 1.086\n",
      "Ep 1 (Step 021200): Train loss 1.271, Val loss 1.088\n",
      "Ep 1 (Step 021400): Train loss 1.011, Val loss 1.088\n",
      "Ep 1 (Step 021600): Train loss 1.005, Val loss 1.092\n",
      "Ep 1 (Step 021800): Train loss 1.041, Val loss 1.089\n",
      "Ep 1 (Step 022000): Train loss 0.999, Val loss 1.079\n",
      "Ep 1 (Step 022200): Train loss 1.174, Val loss 1.089\n",
      "Ep 1 (Step 022400): Train loss 1.192, Val loss 1.070\n",
      "Ep 1 (Step 022600): Train loss 0.991, Val loss 1.076\n",
      "Ep 1 (Step 022800): Train loss 1.117, Val loss 1.078\n",
      "Ep 1 (Step 023000): Train loss 1.074, Val loss 1.073\n",
      "Ep 1 (Step 023200): Train loss 0.942, Val loss 1.071\n",
      "Ep 1 (Step 023400): Train loss 1.004, Val loss 1.070\n",
      "Ep 1 (Step 023600): Train loss 1.041, Val loss 1.068\n",
      "Ep 1 (Step 023800): Train loss 1.140, Val loss 1.059\n",
      "Ep 1 (Step 024000): Train loss 1.024, Val loss 1.065\n",
      "Ep 1 (Step 024200): Train loss 1.202, Val loss 1.071\n",
      "Ep 1 (Step 024400): Train loss 1.013, Val loss 1.070\n",
      "Ep 1 (Step 024600): Train loss 1.001, Val loss 1.064\n",
      "Ep 1 (Step 024800): Train loss 0.847, Val loss 1.071\n",
      "Ep 1 (Step 025000): Train loss 0.956, Val loss 1.060\n",
      "Ep 1 (Step 025200): Train loss 1.143, Val loss 1.053\n",
      "Ep 1 (Step 025400): Train loss 1.192, Val loss 1.059\n",
      "Ep 1 (Step 025600): Train loss 1.155, Val loss 1.060\n",
      "Ep 1 (Step 025800): Train loss 1.052, Val loss 1.057\n",
      "Ep 1 (Step 026000): Train loss 1.057, Val loss 1.062\n",
      "Ep 1 (Step 026200): Train loss 1.069, Val loss 1.063\n",
      "Ep 1 (Step 026400): Train loss 1.030, Val loss 1.055\n",
      "Ep 1 (Step 026600): Train loss 1.047, Val loss 1.050\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Za ka iya tantance yanayin wannan rubutu? Bi waÉ—annan jagororin sharhi: Kyakkyawa: idan rubutu na nuna kyakkyawan tunani, hali, da yanayi. Korau: idan rubutu yana nuna mummunar tunani ko yanayi. Tsaka-tsaki: idan rubutu baya nuna kyakkyawar magana ko mara kyau kai tsaye ko a kaikaice.  ### Input: ya raina shi ne shiyasa gargadi ma yake mashi dama el rufai kayi mawa da ya rushe gidanka  ### Response: Korauzakin sarki ya allah ka kara basira  ### Response: Kyakkyawa\n",
      "Ep 2 (Step 026800): Train loss 1.109, Val loss 1.054\n",
      "Ep 2 (Step 027000): Train loss 1.108, Val loss 1.060\n",
      "Ep 2 (Step 027200): Train loss 1.245, Val loss 1.056\n",
      "Ep 2 (Step 027400): Train loss 1.014, Val loss 1.048\n",
      "Ep 2 (Step 027600): Train loss 1.144, Val loss 1.048\n",
      "Ep 2 (Step 027800): Train loss 1.025, Val loss 1.049\n",
      "Ep 2 (Step 028000): Train loss 1.091, Val loss 1.040\n",
      "Ep 2 (Step 028200): Train loss 0.985, Val loss 1.053\n",
      "Ep 2 (Step 028400): Train loss 0.974, Val loss 1.056\n",
      "Ep 2 (Step 028600): Train loss 1.091, Val loss 1.048\n",
      "Ep 2 (Step 028800): Train loss 0.986, Val loss 1.041\n",
      "Ep 2 (Step 029000): Train loss 1.082, Val loss 1.045\n",
      "Ep 2 (Step 029200): Train loss 0.963, Val loss 1.050\n",
      "Ep 2 (Step 029400): Train loss 0.927, Val loss 1.040\n",
      "Ep 2 (Step 029600): Train loss 1.052, Val loss 1.041\n",
      "Ep 2 (Step 029800): Train loss 1.089, Val loss 1.044\n",
      "Ep 2 (Step 030000): Train loss 0.921, Val loss 1.033\n",
      "Ep 2 (Step 030200): Train loss 1.182, Val loss 1.041\n",
      "Ep 2 (Step 030400): Train loss 1.106, Val loss 1.034\n",
      "Ep 2 (Step 030600): Train loss 1.030, Val loss 1.033\n",
      "Ep 2 (Step 030800): Train loss 1.061, Val loss 1.029\n",
      "Ep 2 (Step 031000): Train loss 0.865, Val loss 1.032\n",
      "Ep 2 (Step 031200): Train loss 1.037, Val loss 1.025\n",
      "Ep 2 (Step 031400): Train loss 0.941, Val loss 1.025\n",
      "Ep 2 (Step 031600): Train loss 1.006, Val loss 1.036\n",
      "Ep 2 (Step 031800): Train loss 0.945, Val loss 1.035\n",
      "Ep 2 (Step 032000): Train loss 1.006, Val loss 1.008\n",
      "Ep 2 (Step 032200): Train loss 0.988, Val loss 1.022\n",
      "Ep 2 (Step 032400): Train loss 1.037, Val loss 1.012\n",
      "Ep 2 (Step 032600): Train loss 1.020, Val loss 1.017\n",
      "Ep 2 (Step 032800): Train loss 0.978, Val loss 1.016\n",
      "Ep 2 (Step 033000): Train loss 1.043, Val loss 1.010\n",
      "Ep 2 (Step 033200): Train loss 0.995, Val loss 1.018\n",
      "Ep 2 (Step 033400): Train loss 0.980, Val loss 1.011\n",
      "Ep 2 (Step 033600): Train loss 0.926, Val loss 1.005\n",
      "Ep 2 (Step 033800): Train loss 0.939, Val loss 1.019\n",
      "Ep 2 (Step 034000): Train loss 1.003, Val loss 1.017\n",
      "Ep 2 (Step 034200): Train loss 1.027, Val loss 1.012\n",
      "Ep 2 (Step 034400): Train loss 1.030, Val loss 1.002\n",
      "Ep 2 (Step 034600): Train loss 0.789, Val loss 1.008\n",
      "Ep 2 (Step 034800): Train loss 0.951, Val loss 1.005\n",
      "Ep 2 (Step 035000): Train loss 0.880, Val loss 1.004\n",
      "Ep 2 (Step 035200): Train loss 0.850, Val loss 1.007\n",
      "Ep 2 (Step 035400): Train loss 0.982, Val loss 1.025\n",
      "Ep 2 (Step 035600): Train loss 0.881, Val loss 1.011\n",
      "Ep 2 (Step 035800): Train loss 1.017, Val loss 1.009\n",
      "Ep 2 (Step 036000): Train loss 1.107, Val loss 1.002\n",
      "Ep 2 (Step 036200): Train loss 0.994, Val loss 0.998\n",
      "Ep 2 (Step 036400): Train loss 0.989, Val loss 0.997\n",
      "Ep 2 (Step 036600): Train loss 1.013, Val loss 0.996\n",
      "Ep 2 (Step 036800): Train loss 0.967, Val loss 1.002\n",
      "Ep 2 (Step 037000): Train loss 0.976, Val loss 1.005\n",
      "Ep 2 (Step 037200): Train loss 0.973, Val loss 1.004\n",
      "Ep 2 (Step 037400): Train loss 0.938, Val loss 1.008\n",
      "Ep 2 (Step 037600): Train loss 0.986, Val loss 0.999\n",
      "Ep 2 (Step 037800): Train loss 0.841, Val loss 1.000\n",
      "Ep 2 (Step 038000): Train loss 0.994, Val loss 1.000\n",
      "Ep 2 (Step 038200): Train loss 1.026, Val loss 0.994\n",
      "Ep 2 (Step 038400): Train loss 0.865, Val loss 0.997\n",
      "Ep 2 (Step 038600): Train loss 0.928, Val loss 0.996\n",
      "Ep 2 (Step 038800): Train loss 0.926, Val loss 0.990\n",
      "Ep 2 (Step 039000): Train loss 0.854, Val loss 0.999\n",
      "Ep 2 (Step 039200): Train loss 0.986, Val loss 0.990\n",
      "Ep 2 (Step 039400): Train loss 0.921, Val loss 0.995\n",
      "Ep 2 (Step 039600): Train loss 0.965, Val loss 0.988\n",
      "Ep 2 (Step 039800): Train loss 0.888, Val loss 0.986\n",
      "Ep 2 (Step 040000): Train loss 1.075, Val loss 0.987\n",
      "Ep 2 (Step 040200): Train loss 0.982, Val loss 0.986\n",
      "Ep 2 (Step 040400): Train loss 0.925, Val loss 0.987\n",
      "Ep 2 (Step 040600): Train loss 0.954, Val loss 0.989\n",
      "Ep 2 (Step 040800): Train loss 0.924, Val loss 0.978\n",
      "Ep 2 (Step 041000): Train loss 0.998, Val loss 0.984\n",
      "Ep 2 (Step 041200): Train loss 0.813, Val loss 0.978\n",
      "Ep 2 (Step 041400): Train loss 0.797, Val loss 0.966\n",
      "Ep 2 (Step 041600): Train loss 0.967, Val loss 0.967\n",
      "Ep 2 (Step 041800): Train loss 0.806, Val loss 0.964\n",
      "Ep 2 (Step 042000): Train loss 0.858, Val loss 0.966\n",
      "Ep 2 (Step 042200): Train loss 1.042, Val loss 0.959\n",
      "Ep 2 (Step 042400): Train loss 0.951, Val loss 0.960\n",
      "Ep 2 (Step 042600): Train loss 0.892, Val loss 0.960\n",
      "Ep 2 (Step 042800): Train loss 0.878, Val loss 0.960\n",
      "Ep 2 (Step 043000): Train loss 0.941, Val loss 0.962\n",
      "Ep 2 (Step 043200): Train loss 0.879, Val loss 0.958\n",
      "Ep 2 (Step 043400): Train loss 0.845, Val loss 0.966\n",
      "Ep 2 (Step 043600): Train loss 0.970, Val loss 0.965\n",
      "Ep 2 (Step 043800): Train loss 0.905, Val loss 0.964\n",
      "Ep 2 (Step 044000): Train loss 0.858, Val loss 0.952\n",
      "Ep 2 (Step 044200): Train loss 0.850, Val loss 0.946\n",
      "Ep 2 (Step 044400): Train loss 0.925, Val loss 0.952\n",
      "Ep 2 (Step 044600): Train loss 0.979, Val loss 0.954\n",
      "Ep 2 (Step 044800): Train loss 0.865, Val loss 0.953\n",
      "Ep 2 (Step 045000): Train loss 0.950, Val loss 0.950\n",
      "Ep 2 (Step 045200): Train loss 0.955, Val loss 0.951\n",
      "Ep 2 (Step 045400): Train loss 0.879, Val loss 0.960\n",
      "Ep 2 (Step 045600): Train loss 0.972, Val loss 0.957\n",
      "Ep 2 (Step 045800): Train loss 0.875, Val loss 0.969\n",
      "Ep 2 (Step 046000): Train loss 0.892, Val loss 0.968\n",
      "Ep 2 (Step 046200): Train loss 0.919, Val loss 0.968\n",
      "Ep 2 (Step 046400): Train loss 0.860, Val loss 0.966\n",
      "Ep 2 (Step 046600): Train loss 0.856, Val loss 0.960\n",
      "Ep 2 (Step 046800): Train loss 0.935, Val loss 0.970\n",
      "Ep 2 (Step 047000): Train loss 0.884, Val loss 0.952\n",
      "Ep 2 (Step 047200): Train loss 0.961, Val loss 0.956\n",
      "Ep 2 (Step 047400): Train loss 0.902, Val loss 0.962\n",
      "Ep 2 (Step 047600): Train loss 0.915, Val loss 0.958\n",
      "Ep 2 (Step 047800): Train loss 0.927, Val loss 0.958\n",
      "Ep 2 (Step 048000): Train loss 0.803, Val loss 0.961\n",
      "Ep 2 (Step 048200): Train loss 0.909, Val loss 0.950\n",
      "Ep 2 (Step 048400): Train loss 0.824, Val loss 0.953\n",
      "Ep 2 (Step 048600): Train loss 0.882, Val loss 0.957\n",
      "Ep 2 (Step 048800): Train loss 0.912, Val loss 0.952\n",
      "Ep 2 (Step 049000): Train loss 0.910, Val loss 0.946\n",
      "Ep 2 (Step 049200): Train loss 0.856, Val loss 0.953\n",
      "Ep 2 (Step 049400): Train loss 0.766, Val loss 0.951\n",
      "Ep 2 (Step 049600): Train loss 0.859, Val loss 0.951\n",
      "Ep 2 (Step 049800): Train loss 1.018, Val loss 0.942\n",
      "Ep 2 (Step 050000): Train loss 0.943, Val loss 0.944\n",
      "Ep 2 (Step 050200): Train loss 1.066, Val loss 0.930\n",
      "Ep 2 (Step 050400): Train loss 0.960, Val loss 0.937\n",
      "Ep 2 (Step 050600): Train loss 0.866, Val loss 0.936\n",
      "Ep 2 (Step 050800): Train loss 0.993, Val loss 0.941\n",
      "Ep 2 (Step 051000): Train loss 0.912, Val loss 0.947\n",
      "Ep 2 (Step 051200): Train loss 0.804, Val loss 0.949\n",
      "Ep 2 (Step 051400): Train loss 0.805, Val loss 0.949\n",
      "Ep 2 (Step 051600): Train loss 1.030, Val loss 0.940\n",
      "Ep 2 (Step 051800): Train loss 0.777, Val loss 0.943\n",
      "Ep 2 (Step 052000): Train loss 0.790, Val loss 0.942\n",
      "Ep 2 (Step 052200): Train loss 0.901, Val loss 0.938\n",
      "Ep 2 (Step 052400): Train loss 0.875, Val loss 0.936\n",
      "Ep 2 (Step 052600): Train loss 0.911, Val loss 0.927\n",
      "Ep 2 (Step 052800): Train loss 0.957, Val loss 0.929\n",
      "Ep 2 (Step 053000): Train loss 0.810, Val loss 0.936\n",
      "Ep 2 (Step 053200): Train loss 0.845, Val loss 0.933\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Za ka iya tantance yanayin wannan rubutu? Bi waÉ—annan jagororin sharhi: Kyakkyawa: idan rubutu na nuna kyakkyawan tunani, hali, da yanayi. Korau: idan rubutu yana nuna mummunar tunani ko yanayi. Tsaka-tsaki: idan rubutu baya nuna kyakkyawar magana ko mara kyau kai tsaye ko a kaikaice.  ### Input: ya raina shi ne shiyasa gargadi ma yake mashi dama el rufai kayi mawa da ya rushe gidanka kuma ya barshi a ciki  ### Response: KorauKorauKorauKorauKorauKorauKorauKorauKorauKorauKorauKorauKorauKorauKorauu-makaranta.//t.co/b4444b4f\n",
      "Ep 3 (Step 053400): Train loss 0.909, Val loss 0.934\n",
      "Ep 3 (Step 053600): Train loss 0.888, Val loss 0.925\n",
      "Ep 3 (Step 053800): Train loss 0.938, Val loss 0.925\n",
      "Ep 3 (Step 054000): Train loss 0.910, Val loss 0.928\n",
      "Ep 3 (Step 054200): Train loss 0.869, Val loss 0.925\n",
      "Ep 3 (Step 054400): Train loss 0.858, Val loss 0.923\n",
      "Ep 3 (Step 054600): Train loss 0.926, Val loss 0.912\n",
      "Ep 3 (Step 054800): Train loss 0.777, Val loss 0.910\n",
      "Ep 3 (Step 055000): Train loss 0.981, Val loss 0.918\n",
      "Ep 3 (Step 055200): Train loss 0.901, Val loss 0.914\n",
      "Ep 3 (Step 055400): Train loss 0.799, Val loss 0.915\n",
      "Ep 3 (Step 055600): Train loss 0.836, Val loss 0.929\n",
      "Ep 3 (Step 055800): Train loss 0.941, Val loss 0.916\n",
      "Ep 3 (Step 056000): Train loss 0.825, Val loss 0.916\n",
      "Ep 3 (Step 056200): Train loss 0.818, Val loss 0.921\n",
      "Ep 3 (Step 056400): Train loss 0.863, Val loss 0.919\n",
      "Ep 3 (Step 056600): Train loss 0.783, Val loss 0.917\n",
      "Ep 3 (Step 056800): Train loss 0.889, Val loss 0.917\n",
      "Ep 3 (Step 057000): Train loss 0.884, Val loss 0.918\n",
      "Ep 3 (Step 057200): Train loss 0.827, Val loss 0.916\n",
      "Ep 3 (Step 057400): Train loss 0.991, Val loss 0.918\n",
      "Ep 3 (Step 057600): Train loss 0.893, Val loss 0.923\n",
      "Ep 3 (Step 057800): Train loss 0.957, Val loss 0.928\n",
      "Ep 3 (Step 058000): Train loss 0.911, Val loss 0.927\n",
      "Ep 3 (Step 058200): Train loss 0.936, Val loss 0.926\n",
      "Ep 3 (Step 058400): Train loss 0.841, Val loss 0.929\n",
      "Ep 3 (Step 058600): Train loss 0.895, Val loss 0.918\n",
      "Ep 3 (Step 058800): Train loss 0.877, Val loss 0.923\n",
      "Ep 3 (Step 059000): Train loss 0.863, Val loss 0.920\n",
      "Ep 3 (Step 059200): Train loss 0.954, Val loss 0.931\n",
      "Ep 3 (Step 059400): Train loss 0.801, Val loss 0.924\n",
      "Ep 3 (Step 059600): Train loss 0.943, Val loss 0.917\n",
      "Ep 3 (Step 059800): Train loss 0.812, Val loss 0.919\n",
      "Ep 3 (Step 060000): Train loss 0.878, Val loss 0.918\n",
      "Ep 3 (Step 060200): Train loss 0.993, Val loss 0.907\n",
      "Ep 3 (Step 060400): Train loss 0.919, Val loss 0.905\n",
      "Ep 3 (Step 060600): Train loss 0.818, Val loss 0.913\n",
      "Ep 3 (Step 060800): Train loss 0.923, Val loss 0.904\n",
      "Ep 3 (Step 061000): Train loss 0.881, Val loss 0.903\n",
      "Ep 3 (Step 061200): Train loss 0.871, Val loss 0.908\n",
      "Ep 3 (Step 061400): Train loss 1.006, Val loss 0.903\n",
      "Ep 3 (Step 061600): Train loss 0.803, Val loss 0.912\n",
      "Ep 3 (Step 061800): Train loss 0.864, Val loss 0.897\n",
      "Ep 3 (Step 062000): Train loss 0.949, Val loss 0.900\n",
      "Ep 3 (Step 062200): Train loss 0.908, Val loss 0.898\n",
      "Ep 3 (Step 062400): Train loss 0.878, Val loss 0.897\n",
      "Ep 3 (Step 062600): Train loss 0.915, Val loss 0.907\n",
      "Ep 3 (Step 062800): Train loss 0.915, Val loss 0.903\n",
      "Ep 3 (Step 063000): Train loss 0.796, Val loss 0.899\n",
      "Ep 3 (Step 063200): Train loss 0.881, Val loss 0.899\n",
      "Ep 3 (Step 063400): Train loss 0.825, Val loss 0.899\n",
      "Ep 3 (Step 063600): Train loss 0.936, Val loss 0.897\n",
      "Ep 3 (Step 063800): Train loss 0.748, Val loss 0.900\n",
      "Ep 3 (Step 064000): Train loss 0.809, Val loss 0.899\n",
      "Ep 3 (Step 064200): Train loss 0.806, Val loss 0.901\n",
      "Ep 3 (Step 064400): Train loss 0.777, Val loss 0.904\n",
      "Ep 3 (Step 064600): Train loss 0.899, Val loss 0.901\n",
      "Ep 3 (Step 064800): Train loss 0.893, Val loss 0.903\n",
      "Ep 3 (Step 065000): Train loss 0.844, Val loss 0.912\n",
      "Ep 3 (Step 065200): Train loss 0.877, Val loss 0.909\n",
      "Ep 3 (Step 065400): Train loss 0.826, Val loss 0.911\n",
      "Ep 3 (Step 065600): Train loss 0.850, Val loss 0.914\n",
      "Ep 3 (Step 065800): Train loss 0.910, Val loss 0.914\n",
      "Ep 3 (Step 066000): Train loss 0.841, Val loss 0.915\n",
      "Ep 3 (Step 066200): Train loss 0.920, Val loss 0.910\n",
      "Ep 3 (Step 066400): Train loss 0.859, Val loss 0.912\n",
      "Ep 3 (Step 066600): Train loss 0.796, Val loss 0.911\n",
      "Ep 3 (Step 066800): Train loss 0.855, Val loss 0.910\n",
      "Ep 3 (Step 067000): Train loss 0.840, Val loss 0.912\n",
      "Ep 3 (Step 067200): Train loss 0.831, Val loss 0.905\n",
      "Ep 3 (Step 067400): Train loss 0.717, Val loss 0.905\n",
      "Ep 3 (Step 067600): Train loss 0.822, Val loss 0.910\n",
      "Ep 3 (Step 067800): Train loss 0.783, Val loss 0.905\n",
      "Ep 3 (Step 068000): Train loss 0.771, Val loss 0.902\n",
      "Ep 3 (Step 068200): Train loss 0.906, Val loss 0.899\n",
      "Ep 3 (Step 068400): Train loss 0.835, Val loss 0.889\n",
      "Ep 3 (Step 068600): Train loss 0.721, Val loss 0.892\n",
      "Ep 3 (Step 068800): Train loss 0.718, Val loss 0.893\n",
      "Ep 3 (Step 069000): Train loss 0.921, Val loss 0.893\n",
      "Ep 3 (Step 069200): Train loss 0.730, Val loss 0.894\n",
      "Ep 3 (Step 069400): Train loss 0.746, Val loss 0.897\n",
      "Ep 3 (Step 069600): Train loss 0.864, Val loss 0.890\n",
      "Ep 3 (Step 069800): Train loss 0.808, Val loss 0.889\n",
      "Ep 3 (Step 070000): Train loss 0.834, Val loss 0.889\n",
      "Ep 3 (Step 070200): Train loss 0.810, Val loss 0.883\n",
      "Ep 3 (Step 070400): Train loss 0.877, Val loss 0.878\n",
      "Ep 3 (Step 070600): Train loss 0.780, Val loss 0.880\n",
      "Ep 3 (Step 070800): Train loss 0.872, Val loss 0.878\n",
      "Ep 3 (Step 071000): Train loss 0.874, Val loss 0.877\n",
      "Ep 3 (Step 071200): Train loss 0.843, Val loss 0.880\n",
      "Ep 3 (Step 071400): Train loss 0.696, Val loss 0.876\n",
      "Ep 3 (Step 071600): Train loss 0.777, Val loss 0.873\n",
      "Ep 3 (Step 071800): Train loss 0.817, Val loss 0.877\n",
      "Ep 3 (Step 072000): Train loss 0.829, Val loss 0.874\n",
      "Ep 3 (Step 072200): Train loss 0.823, Val loss 0.871\n",
      "Ep 3 (Step 072400): Train loss 0.911, Val loss 0.875\n",
      "Ep 3 (Step 072600): Train loss 0.780, Val loss 0.871\n",
      "Ep 3 (Step 072800): Train loss 0.847, Val loss 0.877\n",
      "Ep 3 (Step 073000): Train loss 0.744, Val loss 0.877\n",
      "Ep 3 (Step 073200): Train loss 0.729, Val loss 0.876\n",
      "Ep 3 (Step 073400): Train loss 0.808, Val loss 0.876\n",
      "Ep 3 (Step 073600): Train loss 0.847, Val loss 0.878\n",
      "Ep 3 (Step 073800): Train loss 0.811, Val loss 0.877\n",
      "Ep 3 (Step 074000): Train loss 0.779, Val loss 0.877\n",
      "Ep 3 (Step 074200): Train loss 0.779, Val loss 0.878\n",
      "Ep 3 (Step 074400): Train loss 0.772, Val loss 0.879\n",
      "Ep 3 (Step 074600): Train loss 0.817, Val loss 0.879\n",
      "Ep 3 (Step 074800): Train loss 0.761, Val loss 0.876\n",
      "Ep 3 (Step 075000): Train loss 0.769, Val loss 0.875\n",
      "Ep 3 (Step 075200): Train loss 0.767, Val loss 0.876\n",
      "Ep 3 (Step 075400): Train loss 0.822, Val loss 0.879\n",
      "Ep 3 (Step 075600): Train loss 0.759, Val loss 0.874\n",
      "Ep 3 (Step 075800): Train loss 0.832, Val loss 0.875\n",
      "Ep 3 (Step 076000): Train loss 0.889, Val loss 0.876\n",
      "Ep 3 (Step 076200): Train loss 0.790, Val loss 0.874\n",
      "Ep 3 (Step 076400): Train loss 0.896, Val loss 0.871\n",
      "Ep 3 (Step 076600): Train loss 0.861, Val loss 0.872\n",
      "Ep 3 (Step 076800): Train loss 0.807, Val loss 0.878\n",
      "Ep 3 (Step 077000): Train loss 0.893, Val loss 0.877\n",
      "Ep 3 (Step 077200): Train loss 0.798, Val loss 0.879\n",
      "Ep 3 (Step 077400): Train loss 0.821, Val loss 0.869\n",
      "Ep 3 (Step 077600): Train loss 0.734, Val loss 0.868\n",
      "Ep 3 (Step 077800): Train loss 0.810, Val loss 0.870\n",
      "Ep 3 (Step 078000): Train loss 0.715, Val loss 0.870\n",
      "Ep 3 (Step 078200): Train loss 0.797, Val loss 0.873\n",
      "Ep 3 (Step 078400): Train loss 0.798, Val loss 0.879\n",
      "Ep 3 (Step 078600): Train loss 0.861, Val loss 0.876\n",
      "Ep 3 (Step 078800): Train loss 0.806, Val loss 0.874\n",
      "Ep 3 (Step 079000): Train loss 0.878, Val loss 0.867\n",
      "Ep 3 (Step 079200): Train loss 0.796, Val loss 0.863\n",
      "Ep 3 (Step 079400): Train loss 0.794, Val loss 0.861\n",
      "Ep 3 (Step 079600): Train loss 0.735, Val loss 0.861\n",
      "Ep 3 (Step 079800): Train loss 0.819, Val loss 0.868\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Za ka iya tantance yanayin wannan rubutu? Bi waÉ—annan jagororin sharhi: Kyakkyawa: idan rubutu na nuna kyakkyawan tunani, hali, da yanayi. Korau: idan rubutu yana nuna mummunar tunani ko yanayi. Tsaka-tsaki: idan rubutu baya nuna kyakkyawar magana ko mara kyau kai tsaye ko a kaikaice.  ### Input: ya raina shi ne shiyasa gargadi ma yake mashi dama el rufai kayi mawa da ya rushe gidanka  ### Response: KorauKorauKorau mukayi wa mukayi wa mukayi wa mukayi wa mukayi wa mukayi wa mukayi wa\n",
      "Ep 4 (Step 080000): Train loss 0.726, Val loss 0.867\n",
      "Ep 4 (Step 080200): Train loss 0.880, Val loss 0.863\n",
      "Ep 4 (Step 080400): Train loss 0.929, Val loss 0.857\n",
      "Ep 4 (Step 080600): Train loss 0.660, Val loss 0.857\n",
      "Ep 4 (Step 080800): Train loss 0.805, Val loss 0.862\n",
      "Ep 4 (Step 081000): Train loss 0.737, Val loss 0.863\n",
      "Ep 4 (Step 081200): Train loss 0.854, Val loss 0.854\n",
      "Ep 4 (Step 081400): Train loss 0.848, Val loss 0.851\n",
      "Ep 4 (Step 081600): Train loss 0.768, Val loss 0.861\n",
      "Ep 4 (Step 081800): Train loss 0.789, Val loss 0.858\n",
      "Ep 4 (Step 082000): Train loss 0.807, Val loss 0.856\n",
      "Ep 4 (Step 082200): Train loss 0.775, Val loss 0.859\n",
      "Ep 4 (Step 082400): Train loss 0.826, Val loss 0.850\n",
      "Ep 4 (Step 082600): Train loss 0.697, Val loss 0.850\n",
      "Ep 4 (Step 082800): Train loss 0.768, Val loss 0.853\n",
      "Ep 4 (Step 083000): Train loss 0.762, Val loss 0.850\n",
      "Ep 4 (Step 083200): Train loss 0.826, Val loss 0.853\n",
      "Ep 4 (Step 083400): Train loss 0.721, Val loss 0.847\n",
      "Ep 4 (Step 083600): Train loss 0.746, Val loss 0.849\n",
      "Ep 4 (Step 083800): Train loss 0.825, Val loss 0.852\n",
      "Ep 4 (Step 084000): Train loss 0.773, Val loss 0.844\n",
      "Ep 4 (Step 084200): Train loss 0.805, Val loss 0.841\n",
      "Ep 4 (Step 084400): Train loss 0.857, Val loss 0.841\n",
      "Ep 4 (Step 084600): Train loss 0.747, Val loss 0.838\n",
      "Ep 4 (Step 084800): Train loss 0.751, Val loss 0.841\n",
      "Ep 4 (Step 085000): Train loss 0.729, Val loss 0.840\n",
      "Ep 4 (Step 085200): Train loss 0.742, Val loss 0.835\n",
      "Ep 4 (Step 085400): Train loss 0.789, Val loss 0.840\n",
      "Ep 4 (Step 085600): Train loss 0.702, Val loss 0.840\n",
      "Ep 4 (Step 085800): Train loss 0.745, Val loss 0.842\n",
      "Ep 4 (Step 086000): Train loss 0.692, Val loss 0.846\n",
      "Ep 4 (Step 086200): Train loss 0.682, Val loss 0.849\n",
      "Ep 4 (Step 086400): Train loss 0.701, Val loss 0.847\n",
      "Ep 4 (Step 086600): Train loss 0.817, Val loss 0.846\n",
      "Ep 4 (Step 086800): Train loss 0.720, Val loss 0.841\n",
      "Ep 4 (Step 087000): Train loss 0.750, Val loss 0.839\n",
      "Ep 4 (Step 087200): Train loss 0.872, Val loss 0.832\n",
      "Ep 4 (Step 087400): Train loss 0.853, Val loss 0.833\n",
      "Ep 4 (Step 087600): Train loss 0.768, Val loss 0.822\n",
      "Ep 4 (Step 087800): Train loss 0.768, Val loss 0.825\n",
      "Ep 4 (Step 088000): Train loss 0.700, Val loss 0.832\n",
      "Ep 4 (Step 088200): Train loss 0.818, Val loss 0.828\n",
      "Ep 4 (Step 088400): Train loss 0.733, Val loss 0.830\n",
      "Ep 4 (Step 088600): Train loss 0.776, Val loss 0.831\n",
      "Ep 4 (Step 088800): Train loss 0.667, Val loss 0.830\n",
      "Ep 4 (Step 089000): Train loss 0.710, Val loss 0.828\n",
      "Ep 4 (Step 089200): Train loss 0.805, Val loss 0.831\n",
      "Ep 4 (Step 089400): Train loss 0.775, Val loss 0.821\n",
      "Ep 4 (Step 089600): Train loss 0.715, Val loss 0.825\n",
      "Ep 4 (Step 089800): Train loss 0.736, Val loss 0.826\n",
      "Ep 4 (Step 090000): Train loss 0.771, Val loss 0.822\n",
      "Ep 4 (Step 090200): Train loss 0.755, Val loss 0.818\n",
      "Ep 4 (Step 090400): Train loss 0.743, Val loss 0.818\n",
      "Ep 4 (Step 090600): Train loss 0.725, Val loss 0.818\n",
      "Ep 4 (Step 090800): Train loss 0.710, Val loss 0.819\n",
      "Ep 4 (Step 091000): Train loss 0.695, Val loss 0.821\n",
      "Ep 4 (Step 091200): Train loss 0.704, Val loss 0.819\n",
      "Ep 4 (Step 091400): Train loss 0.655, Val loss 0.815\n",
      "Ep 4 (Step 091600): Train loss 0.765, Val loss 0.817\n",
      "Ep 4 (Step 091800): Train loss 0.810, Val loss 0.822\n",
      "Ep 4 (Step 092000): Train loss 0.719, Val loss 0.827\n",
      "Ep 4 (Step 092200): Train loss 0.701, Val loss 0.818\n",
      "Ep 4 (Step 092400): Train loss 0.685, Val loss 0.816\n",
      "Ep 4 (Step 092600): Train loss 0.734, Val loss 0.815\n",
      "Ep 4 (Step 092800): Train loss 0.743, Val loss 0.817\n",
      "Ep 4 (Step 093000): Train loss 0.857, Val loss 0.817\n",
      "Ep 4 (Step 093200): Train loss 0.737, Val loss 0.815\n",
      "Ep 4 (Step 093400): Train loss 0.775, Val loss 0.817\n",
      "Ep 4 (Step 093600): Train loss 0.692, Val loss 0.814\n",
      "Ep 4 (Step 093800): Train loss 0.664, Val loss 0.818\n",
      "Ep 4 (Step 094000): Train loss 0.762, Val loss 0.819\n",
      "Ep 4 (Step 094200): Train loss 0.679, Val loss 0.820\n",
      "Ep 4 (Step 094400): Train loss 0.625, Val loss 0.817\n",
      "Ep 4 (Step 094600): Train loss 0.688, Val loss 0.819\n",
      "Ep 4 (Step 094800): Train loss 0.724, Val loss 0.819\n",
      "Ep 4 (Step 095000): Train loss 0.742, Val loss 0.820\n",
      "Ep 4 (Step 095200): Train loss 0.698, Val loss 0.819\n",
      "Ep 4 (Step 095400): Train loss 0.762, Val loss 0.817\n",
      "Ep 4 (Step 095600): Train loss 0.791, Val loss 0.814\n",
      "Ep 4 (Step 095800): Train loss 0.828, Val loss 0.815\n",
      "Ep 4 (Step 096000): Train loss 0.784, Val loss 0.816\n",
      "Ep 4 (Step 096200): Train loss 0.694, Val loss 0.813\n",
      "Ep 4 (Step 096400): Train loss 0.688, Val loss 0.817\n",
      "Ep 4 (Step 096600): Train loss 0.633, Val loss 0.817\n",
      "Ep 4 (Step 096800): Train loss 0.811, Val loss 0.816\n",
      "Ep 4 (Step 097000): Train loss 0.659, Val loss 0.815\n",
      "Ep 4 (Step 097200): Train loss 0.643, Val loss 0.814\n",
      "Ep 4 (Step 097400): Train loss 0.759, Val loss 0.816\n",
      "Ep 4 (Step 097600): Train loss 0.685, Val loss 0.815\n",
      "Ep 4 (Step 097800): Train loss 0.694, Val loss 0.813\n",
      "Ep 4 (Step 098000): Train loss 0.734, Val loss 0.808\n",
      "Ep 4 (Step 098200): Train loss 0.638, Val loss 0.810\n",
      "Ep 4 (Step 098400): Train loss 0.753, Val loss 0.800\n",
      "Ep 4 (Step 098600): Train loss 0.702, Val loss 0.799\n",
      "Ep 4 (Step 098800): Train loss 0.697, Val loss 0.797\n",
      "Ep 4 (Step 099000): Train loss 0.652, Val loss 0.790\n",
      "Ep 4 (Step 099200): Train loss 0.694, Val loss 0.793\n",
      "Ep 4 (Step 099400): Train loss 0.675, Val loss 0.791\n",
      "Ep 4 (Step 099600): Train loss 0.580, Val loss 0.786\n",
      "Ep 4 (Step 099800): Train loss 0.628, Val loss 0.778\n",
      "Ep 4 (Step 100000): Train loss 0.694, Val loss 0.777\n",
      "Ep 4 (Step 100200): Train loss 0.638, Val loss 0.782\n",
      "Ep 4 (Step 100400): Train loss 0.642, Val loss 0.784\n",
      "Ep 4 (Step 100600): Train loss 0.636, Val loss 0.777\n",
      "Ep 4 (Step 100800): Train loss 0.751, Val loss 0.779\n",
      "Ep 4 (Step 101000): Train loss 0.750, Val loss 0.777\n",
      "Ep 4 (Step 101200): Train loss 0.685, Val loss 0.779\n",
      "Ep 4 (Step 101400): Train loss 0.699, Val loss 0.779\n",
      "Ep 4 (Step 101600): Train loss 0.626, Val loss 0.779\n",
      "Ep 4 (Step 101800): Train loss 0.571, Val loss 0.779\n",
      "Ep 4 (Step 102000): Train loss 0.626, Val loss 0.772\n",
      "Ep 4 (Step 102200): Train loss 0.684, Val loss 0.768\n",
      "Ep 4 (Step 102400): Train loss 0.784, Val loss 0.773\n",
      "Ep 4 (Step 102600): Train loss 0.635, Val loss 0.764\n",
      "Ep 4 (Step 102800): Train loss 0.588, Val loss 0.765\n",
      "Ep 4 (Step 103000): Train loss 0.576, Val loss 0.762\n",
      "Ep 4 (Step 103200): Train loss 0.722, Val loss 0.764\n",
      "Ep 4 (Step 103400): Train loss 0.681, Val loss 0.761\n",
      "Ep 4 (Step 103600): Train loss 0.563, Val loss 0.764\n",
      "Ep 4 (Step 103800): Train loss 0.574, Val loss 0.762\n",
      "Ep 4 (Step 104000): Train loss 0.672, Val loss 0.757\n",
      "Ep 4 (Step 104200): Train loss 0.681, Val loss 0.756\n",
      "Ep 4 (Step 104400): Train loss 0.674, Val loss 0.753\n",
      "Ep 4 (Step 104600): Train loss 0.653, Val loss 0.756\n",
      "Ep 4 (Step 104800): Train loss 0.629, Val loss 0.757\n",
      "Ep 4 (Step 105000): Train loss 0.651, Val loss 0.758\n",
      "Ep 4 (Step 105200): Train loss 0.703, Val loss 0.758\n",
      "Ep 4 (Step 105400): Train loss 0.705, Val loss 0.749\n",
      "Ep 4 (Step 105600): Train loss 0.674, Val loss 0.747\n",
      "Ep 4 (Step 105800): Train loss 0.582, Val loss 0.744\n",
      "Ep 4 (Step 106000): Train loss 0.589, Val loss 0.743\n",
      "Ep 4 (Step 106200): Train loss 0.656, Val loss 0.743\n",
      "Ep 4 (Step 106400): Train loss 0.678, Val loss 0.743\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Za ka iya tantance yanayin wannan rubutu? Bi waÉ—annan jagororin sharhi: Kyakkyawa: idan rubutu na nuna kyakkyawan tunani, hali, da yanayi. Korau: idan rubutu yana nuna mummunar tunani ko yanayi. Tsaka-tsaki: idan rubutu baya nuna kyakkyawar magana ko mara kyau kai tsaye ko a kaikaice.  ### Input: ya raina shi ne shiyasa gargadi ma yake mashi dama el rufai kayi mawa da ya rushe gidanka  ### Response: Korauaiki da-girma da ya yi wa gwamnatin sa da ta kai ma mai gidanta da ya yi wa gwamnatin sa da ta kai ma mai gidansa ne  ### Response: Korau....a.w.a.w.a\n",
      "Ep 5 (Step 106600): Train loss 0.686, Val loss 0.744\n",
      "Ep 5 (Step 106800): Train loss 0.645, Val loss 0.747\n",
      "Ep 5 (Step 107000): Train loss 0.530, Val loss 0.748\n",
      "Ep 5 (Step 107200): Train loss 0.735, Val loss 0.748\n",
      "Ep 5 (Step 107400): Train loss 0.603, Val loss 0.747\n",
      "Ep 5 (Step 107600): Train loss 0.631, Val loss 0.748\n",
      "Ep 5 (Step 107800): Train loss 0.653, Val loss 0.751\n",
      "Ep 5 (Step 108000): Train loss 0.608, Val loss 0.745\n",
      "Ep 5 (Step 108200): Train loss 0.688, Val loss 0.747\n",
      "Ep 5 (Step 108400): Train loss 0.612, Val loss 0.750\n",
      "Ep 5 (Step 108600): Train loss 0.749, Val loss 0.746\n",
      "Ep 5 (Step 108800): Train loss 0.672, Val loss 0.745\n",
      "Ep 5 (Step 109000): Train loss 0.632, Val loss 0.746\n",
      "Ep 5 (Step 109200): Train loss 0.649, Val loss 0.743\n",
      "Ep 5 (Step 109400): Train loss 0.674, Val loss 0.741\n",
      "Ep 5 (Step 109600): Train loss 0.748, Val loss 0.742\n",
      "Ep 5 (Step 109800): Train loss 0.673, Val loss 0.744\n",
      "Ep 5 (Step 110000): Train loss 0.626, Val loss 0.743\n",
      "Ep 5 (Step 110200): Train loss 0.571, Val loss 0.737\n",
      "Ep 5 (Step 110400): Train loss 0.653, Val loss 0.739\n",
      "Ep 5 (Step 110600): Train loss 0.650, Val loss 0.740\n",
      "Ep 5 (Step 110800): Train loss 0.710, Val loss 0.737\n",
      "Ep 5 (Step 111000): Train loss 0.594, Val loss 0.737\n",
      "Ep 5 (Step 111200): Train loss 0.696, Val loss 0.736\n",
      "Ep 5 (Step 111400): Train loss 0.494, Val loss 0.735\n",
      "Ep 5 (Step 111600): Train loss 0.659, Val loss 0.731\n",
      "Ep 5 (Step 111800): Train loss 0.570, Val loss 0.732\n",
      "Ep 5 (Step 112000): Train loss 0.545, Val loss 0.733\n",
      "Ep 5 (Step 112200): Train loss 0.654, Val loss 0.734\n",
      "Ep 5 (Step 112400): Train loss 0.682, Val loss 0.733\n",
      "Ep 5 (Step 112600): Train loss 0.655, Val loss 0.733\n",
      "Ep 5 (Step 112800): Train loss 0.557, Val loss 0.732\n",
      "Ep 5 (Step 113000): Train loss 0.624, Val loss 0.731\n",
      "Ep 5 (Step 113200): Train loss 0.602, Val loss 0.725\n",
      "Ep 5 (Step 113400): Train loss 0.545, Val loss 0.725\n",
      "Ep 5 (Step 113600): Train loss 0.700, Val loss 0.724\n",
      "Ep 5 (Step 113800): Train loss 0.601, Val loss 0.722\n",
      "Ep 5 (Step 114000): Train loss 0.588, Val loss 0.723\n",
      "Ep 5 (Step 114200): Train loss 0.640, Val loss 0.723\n",
      "Ep 5 (Step 114400): Train loss 0.640, Val loss 0.723\n",
      "Ep 5 (Step 114600): Train loss 0.536, Val loss 0.723\n",
      "Ep 5 (Step 114800): Train loss 0.602, Val loss 0.725\n",
      "Ep 5 (Step 115000): Train loss 0.595, Val loss 0.726\n",
      "Ep 5 (Step 115200): Train loss 0.651, Val loss 0.723\n",
      "Ep 5 (Step 115400): Train loss 0.567, Val loss 0.718\n",
      "Ep 5 (Step 115600): Train loss 0.669, Val loss 0.718\n",
      "Ep 5 (Step 115800): Train loss 0.594, Val loss 0.718\n",
      "Ep 5 (Step 116000): Train loss 0.495, Val loss 0.718\n",
      "Ep 5 (Step 116200): Train loss 0.553, Val loss 0.718\n",
      "Ep 5 (Step 116400): Train loss 0.670, Val loss 0.717\n",
      "Ep 5 (Step 116600): Train loss 0.628, Val loss 0.717\n",
      "Ep 5 (Step 116800): Train loss 0.680, Val loss 0.715\n",
      "Ep 5 (Step 117000): Train loss 0.719, Val loss 0.715\n",
      "Ep 5 (Step 117200): Train loss 0.618, Val loss 0.711\n",
      "Ep 5 (Step 117400): Train loss 0.618, Val loss 0.709\n",
      "Ep 5 (Step 117600): Train loss 0.613, Val loss 0.705\n",
      "Ep 5 (Step 117800): Train loss 0.541, Val loss 0.705\n",
      "Ep 5 (Step 118000): Train loss 0.596, Val loss 0.707\n",
      "Ep 5 (Step 118200): Train loss 0.622, Val loss 0.704\n",
      "Ep 5 (Step 118400): Train loss 0.689, Val loss 0.703\n",
      "Ep 5 (Step 118600): Train loss 0.527, Val loss 0.702\n",
      "Ep 5 (Step 118800): Train loss 0.600, Val loss 0.702\n",
      "Ep 5 (Step 119000): Train loss 0.631, Val loss 0.701\n",
      "Ep 5 (Step 119200): Train loss 0.594, Val loss 0.701\n",
      "Ep 5 (Step 119400): Train loss 0.583, Val loss 0.700\n",
      "Ep 5 (Step 119600): Train loss 0.591, Val loss 0.698\n",
      "Ep 5 (Step 119800): Train loss 0.588, Val loss 0.697\n",
      "Ep 5 (Step 120000): Train loss 0.568, Val loss 0.698\n",
      "Ep 5 (Step 120200): Train loss 0.631, Val loss 0.699\n",
      "Ep 5 (Step 120400): Train loss 0.563, Val loss 0.698\n",
      "Ep 5 (Step 120600): Train loss 0.584, Val loss 0.696\n",
      "Ep 5 (Step 120800): Train loss 0.548, Val loss 0.695\n",
      "Ep 5 (Step 121000): Train loss 0.602, Val loss 0.696\n",
      "Ep 5 (Step 121200): Train loss 0.655, Val loss 0.696\n",
      "Ep 5 (Step 121400): Train loss 0.622, Val loss 0.696\n",
      "Ep 5 (Step 121600): Train loss 0.622, Val loss 0.695\n",
      "Ep 5 (Step 121800): Train loss 0.553, Val loss 0.695\n",
      "Ep 5 (Step 122000): Train loss 0.640, Val loss 0.695\n",
      "Ep 5 (Step 122200): Train loss 0.572, Val loss 0.694\n",
      "Ep 5 (Step 122400): Train loss 0.568, Val loss 0.692\n",
      "Ep 5 (Step 122600): Train loss 0.584, Val loss 0.691\n",
      "Ep 5 (Step 122800): Train loss 0.636, Val loss 0.691\n",
      "Ep 5 (Step 123000): Train loss 0.506, Val loss 0.690\n",
      "Ep 5 (Step 123200): Train loss 0.659, Val loss 0.690\n",
      "Ep 5 (Step 123400): Train loss 0.579, Val loss 0.689\n",
      "Ep 5 (Step 123600): Train loss 0.527, Val loss 0.688\n",
      "Ep 5 (Step 123800): Train loss 0.571, Val loss 0.688\n",
      "Ep 5 (Step 124000): Train loss 0.566, Val loss 0.688\n",
      "Ep 5 (Step 124200): Train loss 0.580, Val loss 0.688\n",
      "Ep 5 (Step 124400): Train loss 0.600, Val loss 0.687\n",
      "Ep 5 (Step 124600): Train loss 0.563, Val loss 0.688\n",
      "Ep 5 (Step 124800): Train loss 0.567, Val loss 0.687\n",
      "Ep 5 (Step 125000): Train loss 0.626, Val loss 0.687\n",
      "Ep 5 (Step 125200): Train loss 0.629, Val loss 0.686\n",
      "Ep 5 (Step 125400): Train loss 0.513, Val loss 0.686\n",
      "Ep 5 (Step 125600): Train loss 0.580, Val loss 0.686\n",
      "Ep 5 (Step 125800): Train loss 0.546, Val loss 0.685\n",
      "Ep 5 (Step 126000): Train loss 0.598, Val loss 0.685\n",
      "Ep 5 (Step 126200): Train loss 0.639, Val loss 0.685\n",
      "Ep 5 (Step 126400): Train loss 0.629, Val loss 0.685\n",
      "Ep 5 (Step 126600): Train loss 0.651, Val loss 0.685\n",
      "Ep 5 (Step 126800): Train loss 0.584, Val loss 0.685\n",
      "Ep 5 (Step 127000): Train loss 0.631, Val loss 0.685\n",
      "Ep 5 (Step 127200): Train loss 0.559, Val loss 0.685\n",
      "Ep 5 (Step 127400): Train loss 0.560, Val loss 0.684\n",
      "Ep 5 (Step 127600): Train loss 0.538, Val loss 0.684\n",
      "Ep 5 (Step 127800): Train loss 0.554, Val loss 0.684\n",
      "Ep 5 (Step 128000): Train loss 0.532, Val loss 0.684\n",
      "Ep 5 (Step 128200): Train loss 0.599, Val loss 0.684\n",
      "Ep 5 (Step 128400): Train loss 0.523, Val loss 0.684\n",
      "Ep 5 (Step 128600): Train loss 0.606, Val loss 0.684\n",
      "Ep 5 (Step 128800): Train loss 0.637, Val loss 0.683\n",
      "Ep 5 (Step 129000): Train loss 0.648, Val loss 0.684\n",
      "Ep 5 (Step 129200): Train loss 0.576, Val loss 0.684\n",
      "Ep 5 (Step 129400): Train loss 0.670, Val loss 0.684\n",
      "Ep 5 (Step 129600): Train loss 0.541, Val loss 0.684\n",
      "Ep 5 (Step 129800): Train loss 0.535, Val loss 0.684\n",
      "Ep 5 (Step 130000): Train loss 0.672, Val loss 0.684\n",
      "Ep 5 (Step 130200): Train loss 0.584, Val loss 0.683\n",
      "Ep 5 (Step 130400): Train loss 0.721, Val loss 0.684\n",
      "Ep 5 (Step 130600): Train loss 0.631, Val loss 0.684\n",
      "Ep 5 (Step 130800): Train loss 0.473, Val loss 0.683\n",
      "Ep 5 (Step 131000): Train loss 0.606, Val loss 0.683\n",
      "Ep 5 (Step 131200): Train loss 0.565, Val loss 0.683\n",
      "Ep 5 (Step 131400): Train loss 0.575, Val loss 0.683\n",
      "Ep 5 (Step 131600): Train loss 0.572, Val loss 0.683\n",
      "Ep 5 (Step 131800): Train loss 0.561, Val loss 0.683\n",
      "Ep 5 (Step 132000): Train loss 0.589, Val loss 0.683\n",
      "Ep 5 (Step 132200): Train loss 0.598, Val loss 0.683\n",
      "Ep 5 (Step 132400): Train loss 0.613, Val loss 0.683\n",
      "Ep 5 (Step 132600): Train loss 0.523, Val loss 0.683\n",
      "Ep 5 (Step 132800): Train loss 0.601, Val loss 0.683\n",
      "Ep 5 (Step 133000): Train loss 0.637, Val loss 0.683\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Za ka iya tantance yanayin wannan rubutu? Bi waÉ—annan jagororin sharhi: Kyakkyawa: idan rubutu na nuna kyakkyawan tunani, hali, da yanayi. Korau: idan rubutu yana nuna mummunar tunani ko yanayi. Tsaka-tsaki: idan rubutu baya nuna kyakkyawar magana ko mara kyau kai tsaye ko a kaikaice.  ### Input: ya raina shi ne shiyasa gargadi ma yake mashi dama el rufai kayi mawa da ya rushe gidanka  ### Response: Korauikum ne ya aikata hakan saboda gwamnatin kano ta dakatar da ita a wannan gwamnatin ta gaza taimakawa gwamnatin shugaba buhari ta gaza  ### Response: Korau\n",
      "Training completed in 246.06 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "num_epochs = 5\n",
    "lr = 0.0005\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, total_steps=len(train_data)//batch_size*num_epochs)\n",
    "\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=200, eval_iter=10,\n",
    "    start_context=format_input(val_data[0]), tokenizer=tokenizer,\n",
    "    scheduler=scheduler\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf10e95",
   "metadata": {},
   "source": [
    "Let's look at a few validation examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "VQ2NZMbfucAc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VQ2NZMbfucAc",
    "outputId": "066c56ff-b52a-4ee6-eae7-1bddfc74d0c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Translate the following from English into Swahili.\n",
      "\n",
      "### Input:\n",
      "stop thinking of joseph and his...\n",
      "\n",
      "Correct response:\n",
      ">> kama hunielewi kamsome yusufu na ndoto yake.....\n",
      "\n",
      "Model response:\n",
      ">> joseph na josefu wake...\n",
      "-------------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Da fatan za a gano ra'ayin da ke cikin wannan rubutu bisa ga jagorori masu zuwa: Kyakkyawa: idan rubutu na nuna kyakkyawan tunani, hali, da yanayi. Korau: idan rubutu yana nuna mummunar tunani ko yanayi. Tsaka-tsaki: idan rubutu baya nuna kyakkyawar magana ko mara kyau kai tsaye ko a kaikaice.\n",
      "\n",
      "### Input:\n",
      "tohh fah gayya tan mu kekeyi muga wata version na shegantaka ki toh anki kallo\n",
      "\n",
      "Correct response:\n",
      ">> Korau\n",
      "\n",
      "Model response:\n",
      ">> Korau\n",
      "-------------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "You will be given two Swahili sentences. Your job is to  is to predict textual entailment. In other words, does sentence A imply/contradict/neither sentence B. Your response must be one word: entailment, contradiction, or neutral.\n",
      "\n",
      "### Input:\n",
      "Sentence A: Yeah yeah i think hiyo ' s\n",
      "Sentence B: Ilo ilo i\n",
      "\n",
      "Correct response:\n",
      ">> entailment\n",
      "\n",
      "Model response:\n",
      ">> think hiyo ni\n",
      "\n",
      "\n",
      "entailment\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "\n",
    "for entry in test_data[:3]:\n",
    "\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=tokenizer.encode(input_text, return_tensors=\"pt\").to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=1024,\n",
    "        eos_id=2\n",
    "    )\n",
    "    generated_text = tokenizer.decode(token_ids[0].tolist(), skip_special_tokens=True)\n",
    "    response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "    print(input_text)\n",
    "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
    "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
    "    print(\"-------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b2f3f6-8569-405a-9db6-d47cba65608a",
   "metadata": {
    "id": "c1b2f3f6-8569-405a-9db6-d47cba65608a"
   },
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8cBU0iHmVfOI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8cBU0iHmVfOI",
    "outputId": "135849ed-9acd-43a2-f438-053d07dae9b2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as lelapa_buzuzu_mavi/vulavula_inkuba_instruct_tokenizer8k_pruned.pth\n"
     ]
    }
   ],
   "source": [
    "file_name = basedir/\"vulavula_inkuba_instruct_tokenizer8k_pruned.pth\"\n",
    "torch.save(model.state_dict(), file_name)\n",
    "print(f\"Model saved as {file_name}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
