{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "basedir = Path(\"./\")\n",
    "data_path = basedir/\"inkuba_instruct_sample.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import LlamaTokenizer, LlamaTokenizerFast, PreTrainedTokenizerFast\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "import sentencepiece as spm\n",
    "from sentencepiece import sentencepiece_model_pb2 as sp_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note on reproducibility\n",
    "\n",
    "The tokenizer is modified by removing tokens that do not appear frequently in the languages that we are working with. The tokenizer was modified by removing tokens that appeared less than 100 times in a previous version of the training dataset. The same methodology applied to a different dataset will produce a different tokenizer. For one, the initial training dataset was smaller, mainly because fewer machine translation examples were included. Thus, a cut-off of 100 tokens on a bigger dataset will produce a bigger vocabulary. This notebook is setup to produce a tokenizer with the same vocabulary size as the one used in model training, although it will not be exactly the same. To produce the exact same tokenizer, set `reproducibility = True` to load the vocab derived from the initial dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reproducibility = True\n",
    "\n",
    "if reproducibility:\n",
    "    with open(basedir/\"keep_tokens.txt\") as f:\n",
    "        keep_tokens = set(f.read().splitlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 350815\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'task': 'sentiment',\n",
       " 'instruction_orig': 'Changanua mawazo ya matini yanayofuata na uainishe matini hayo katika mojawapo ya lebo zifuatazo. Chanya: iwapo matini yanadokeza mawazo, mtazamo na hali chanya ya kihisia. Hasi: iwapo matini yanadokeza mawazo au hisia hasi. Wastani: iwapo matini hayadokezi lugha chanya au hasi kwa njia ya moja kwa moja au isiyo ya moja kwa moja.',\n",
       " 'input': 'habari tunaomba radhi kwa usumbufu unaojitokeza tafadhali tuandikie mita namba yako kwenye dm kwa msaada zaidi pj',\n",
       " 'output': 'Wastani',\n",
       " 'language': 'swahili',\n",
       " 'instruction': 'Changanua mawazo ya matini yanayofuata na uainishe matini hayo katika mojawapo ya lebo zifuatazo. Chanya: iwapo matini yanadokeza mawazo, mtazamo na hali chanya ya kihisia. Hasi: iwapo matini yanadokeza mawazo au hisia hasi. Wastani: iwapo matini hayadokezi lugha chanya au hasi kwa njia ya moja kwa moja au isiyo ya moja kwa moja.'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(data_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "print(\"Number of entries:\", len(data))\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "comp_data_dir = basedir/\"zindi-inkuba-notebook/Notebooks/data\"\n",
    "comp_data_fns = [f for f in os.listdir(comp_data_dir) if f.endswith(\".csv\")]\n",
    "for fn in comp_data_fns:\n",
    "    df = pd.read_csv(comp_data_dir/fn)[[\"instruction\", \"inputs\", \"targets\"]]\n",
    "    for _, row in df.iterrows():\n",
    "        data.append({\n",
    "            \"task\": \"\", \"instruction\": row[\"instruction\"], \"input\": row[\"inputs\"],\n",
    "            \"output\": row[\"targets\"] if fn[:7] == \"MTTrain\" else \"\"\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PROMPT = (\n",
    "    \"Below is an instruction that describes a task. \"\n",
    "    \"Write a response that appropriately completes the request.\"\n",
    "    \"\\n\\n### Instruction:\\n{}\"\n",
    "    \"\\n\\n### Input:\\n{}\"\n",
    "    \"\\n\\n### Response:\\n{}\"\n",
    ")\n",
    "# BASE_PROMPT = \"Instruction: {} Input: {}, Response: {}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Changanua mawazo ya matini yanayofuata na uainishe matini hayo katika mojawapo ya lebo zifuatazo. Chanya: iwapo matini yanadokeza mawazo, mtazamo na hali chanya ya kihisia. Hasi: iwapo matini yanadokeza mawazo au hisia hasi. Wastani: iwapo matini hayadokezi lugha chanya au hasi kwa njia ya moja kwa moja au isiyo ya moja kwa moja.\n",
      "\n",
      "### Input:\n",
      "habari tunaomba radhi kwa usumbufu unaojitokeza tafadhali tuandikie mita namba yako kwenye dm kwa msaada zaidi pj\n",
      "\n",
      "### Response:\n",
      "Wastani\n"
     ]
    }
   ],
   "source": [
    "data = [BASE_PROMPT.format(d[\"instruction\"], d[\"input\"], d[\"output\"]) for d in data]\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore token frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"lelapa/InkubaLM-0.4B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(tokenizer.special_tokens_map)\n",
    "print(tokenizer.convert_tokens_to_ids(\"</s>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>&lt;0x0A&gt;</td>\n",
       "      <td>3236335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>.</td>\n",
       "      <td>1787554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>:</td>\n",
       "      <td>1529828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>#</td>\n",
       "      <td>1060221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>##</td>\n",
       "      <td>1059347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37349</th>\n",
       "      <td>▁McK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37352</th>\n",
       "      <td>▁Even</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37354</th>\n",
       "      <td>▁Code</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37355</th>\n",
       "      <td>Coll</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37356</th>\n",
       "      <td>▁Aar</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37515 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        token    count\n",
       "19     <0x0A>  3236335\n",
       "8           .  1787554\n",
       "24          :  1529828\n",
       "21          #  1060221\n",
       "20         ##  1059347\n",
       "...       ...      ...\n",
       "37349    ▁McK        1\n",
       "37352   ▁Even        1\n",
       "37354   ▁Code        1\n",
       "37355    Coll        1\n",
       "37356    ▁Aar        1\n",
       "\n",
       "[37515 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counts = {}\n",
    "for text in data:\n",
    "    tokens = tokenizer.tokenize(text.strip())\n",
    "    for t in tokens:\n",
    "        token_counts[t] = token_counts.get(t, 0) + 1\n",
    "token_counts = pd.DataFrame({\"token\": token_counts.keys(), \"count\": token_counts.values()}).sort_values(by=\"count\", ascending=False)\n",
    "token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35170"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_counts[token_counts[\"count\"] >= 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31182"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_counts[token_counts[\"count\"] >= 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27536"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_counts[token_counts[\"count\"] >= 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16396"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_counts[token_counts[\"count\"] >= 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11527"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_counts[token_counts[\"count\"] >= 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8992"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_counts[token_counts[\"count\"] >= 150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7383"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_counts[token_counts[\"count\"] >= 200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the tokenizer by only keeping frequencly occurring tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the InkubaLM's tokenizer artifacts and just update the vocabulary. Let's see what the tokenizer's files look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pytorch_model.bin',\n",
       " 'tokenizer.json',\n",
       " 'special_tokens_map.json',\n",
       " 'config.json',\n",
       " 'tokenizer_config.json',\n",
       " 'tokenizer.model',\n",
       " 'vulavulaslm.py',\n",
       " 'generation_config.json']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_file = Path(tokenizer.vocab_file)\n",
    "# get parent directory\n",
    "vocab_dir = vocab_file.parent\n",
    "os.listdir(vocab_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'version': '1.0',\n",
       " 'truncation': {'direction': 'Right',\n",
       "  'max_length': 1024,\n",
       "  'strategy': 'LongestFirst',\n",
       "  'stride': 0},\n",
       " 'padding': None,\n",
       " 'added_tokens': [{'id': 0,\n",
       "   'content': '<unk>',\n",
       "   'single_word': False,\n",
       "   'lstrip': False,\n",
       "   'rstrip': False,\n",
       "   'normalized': False,\n",
       "   'special': True},\n",
       "  {'id': 1,\n",
       "   'content': '<s>',\n",
       "   'single_word': False,\n",
       "   'lstrip': False,\n",
       "   'rstrip': False,\n",
       "   'normalized': False,\n",
       "   'special': True},\n",
       "  {'id': 2,\n",
       "   'content': '</s>',\n",
       "   'single_word': False,\n",
       "   'lstrip': False,\n",
       "   'rstrip': False,\n",
       "   'normalized': False,\n",
       "   'special': True}],\n",
       " 'normalizer': {'type': 'Sequence',\n",
       "  'normalizers': [{'type': 'Prepend', 'prepend': '▁'},\n",
       "   {'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]},\n",
       " 'pre_tokenizer': None,\n",
       " 'post_processor': {'type': 'TemplateProcessing',\n",
       "  'single': [{'SpecialToken': {'id': '<s>', 'type_id': 0}},\n",
       "   {'Sequence': {'id': 'A', 'type_id': 0}}],\n",
       "  'pair': [{'SpecialToken': {'id': '<s>', 'type_id': 0}},\n",
       "   {'Sequence': {'id': 'A', 'type_id': 0}},\n",
       "   {'SpecialToken': {'id': '<s>', 'type_id': 1}},\n",
       "   {'Sequence': {'id': 'B', 'type_id': 1}}],\n",
       "  'special_tokens': {'<s>': {'id': '<s>', 'ids': [1], 'tokens': ['<s>']}}},\n",
       " 'decoder': {'type': 'Sequence',\n",
       "  'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '},\n",
       "   {'type': 'ByteFallback'},\n",
       "   {'type': 'Fuse'},\n",
       "   {'type': 'Strip', 'content': ' ', 'start': 1, 'stop': 0}]},\n",
       " 'model': {'type': 'BPE',\n",
       "  'dropout': None,\n",
       "  'unk_token': '<unk>',\n",
       "  'continuing_subword_prefix': None,\n",
       "  'end_of_word_suffix': None,\n",
       "  'fuse_unk': True,\n",
       "  'byte_fallback': True,\n",
       "  'ignore_merges': False,\n",
       "  'vocab': {'<unk>': 0,\n",
       "   '<s>': 1,\n",
       "   '</s>': 2,\n",
       "   '<0x00>': 3,\n",
       "   '<0x01>': 4,\n",
       "   '<0x02>': 5,\n",
       "   '<0x03>': 6,\n",
       "   '<0x04>': 7,\n",
       "   '<0x05>': 8,\n",
       "   '<0x06>': 9,\n",
       "   '<0x07>': 10,\n",
       "   '<0x08>': 11,\n",
       "   '<0x09>': 12,\n",
       "   '<0x0A>': 13,\n",
       "   '<0x0B>': 14,\n",
       "   '<0x0C>': 15,\n",
       "   '<0x0D>': 16,\n",
       "   '<0x0E>': 17,\n",
       "   '<0x0F>': 18,\n",
       "   '<0x10>': 19,\n",
       "   '<0x11>': 20,\n",
       "   '<0x12>': 21,\n",
       "   '<0x13>': 22,\n",
       "   '<0x14>': 23,\n",
       "   '<0x15>': 24,\n",
       "   '<0x16>': 25,\n",
       "   '<0x17>': 26,\n",
       "   '<0x18>': 27,\n",
       "   '<0x19>': 28,\n",
       "   '<0x1A>': 29,\n",
       "   '<0x1B>': 30,\n",
       "   '<0x1C>': 31,\n",
       "   '<0x1D>': 32,\n",
       "   '<0x1E>': 33,\n",
       "   '<0x1F>': 34,\n",
       "   '<0x20>': 35,\n",
       "   '<0x21>': 36,\n",
       "   '<0x22>': 37,\n",
       "   '<0x23>': 38,\n",
       "   '<0x24>': 39,\n",
       "   '<0x25>': 40,\n",
       "   '<0x26>': 41,\n",
       "   '<0x27>': 42,\n",
       "   '<0x28>': 43,\n",
       "   '<0x29>': 44,\n",
       "   '<0x2A>': 45,\n",
       "   '<0x2B>': 46,\n",
       "   '<0x2C>': 47,\n",
       "   '<0x2D>': 48,\n",
       "   '<0x2E>': 49,\n",
       "   '<0x2F>': 50,\n",
       "   '<0x30>': 51,\n",
       "   '<0x31>': 52,\n",
       "   '<0x32>': 53,\n",
       "   '<0x33>': 54,\n",
       "   '<0x34>': 55,\n",
       "   '<0x35>': 56,\n",
       "   '<0x36>': 57,\n",
       "   '<0x37>': 58,\n",
       "   '<0x38>': 59,\n",
       "   '<0x39>': 60,\n",
       "   '<0x3A>': 61,\n",
       "   '<0x3B>': 62,\n",
       "   '<0x3C>': 63,\n",
       "   '<0x3D>': 64,\n",
       "   '<0x3E>': 65,\n",
       "   '<0x3F>': 66,\n",
       "   '<0x40>': 67,\n",
       "   '<0x41>': 68,\n",
       "   '<0x42>': 69,\n",
       "   '<0x43>': 70,\n",
       "   '<0x44>': 71,\n",
       "   '<0x45>': 72,\n",
       "   '<0x46>': 73,\n",
       "   '<0x47>': 74,\n",
       "   '<0x48>': 75,\n",
       "   '<0x49>': 76,\n",
       "   '<0x4A>': 77,\n",
       "   '<0x4B>': 78,\n",
       "   '<0x4C>': 79,\n",
       "   '<0x4D>': 80,\n",
       "   '<0x4E>': 81,\n",
       "   '<0x4F>': 82,\n",
       "   '<0x50>': 83,\n",
       "   '<0x51>': 84,\n",
       "   '<0x52>': 85,\n",
       "   '<0x53>': 86,\n",
       "   '<0x54>': 87,\n",
       "   '<0x55>': 88,\n",
       "   '<0x56>': 89,\n",
       "   '<0x57>': 90,\n",
       "   '<0x58>': 91,\n",
       "   '<0x59>': 92,\n",
       "   '<0x5A>': 93,\n",
       "   '<0x5B>': 94,\n",
       "   '<0x5C>': 95,\n",
       "   '<0x5D>': 96,\n",
       "   '<0x5E>': 97,\n",
       "   '<0x5F>': 98,\n",
       "   '<0x60>': 99,\n",
       "   '<0x61>': 100,\n",
       "   '<0x62>': 101,\n",
       "   '<0x63>': 102,\n",
       "   '<0x64>': 103,\n",
       "   '<0x65>': 104,\n",
       "   '<0x66>': 105,\n",
       "   '<0x67>': 106,\n",
       "   '<0x68>': 107,\n",
       "   '<0x69>': 108,\n",
       "   '<0x6A>': 109,\n",
       "   '<0x6B>': 110,\n",
       "   '<0x6C>': 111,\n",
       "   '<0x6D>': 112,\n",
       "   '<0x6E>': 113,\n",
       "   '<0x6F>': 114,\n",
       "   '<0x70>': 115,\n",
       "   '<0x71>': 116,\n",
       "   '<0x72>': 117,\n",
       "   '<0x73>': 118,\n",
       "   '<0x74>': 119,\n",
       "   '<0x75>': 120,\n",
       "   '<0x76>': 121,\n",
       "   '<0x77>': 122,\n",
       "   '<0x78>': 123,\n",
       "   '<0x79>': 124,\n",
       "   '<0x7A>': 125,\n",
       "   '<0x7B>': 126,\n",
       "   '<0x7C>': 127,\n",
       "   '<0x7D>': 128,\n",
       "   '<0x7E>': 129,\n",
       "   '<0x7F>': 130,\n",
       "   '<0x80>': 131,\n",
       "   '<0x81>': 132,\n",
       "   '<0x82>': 133,\n",
       "   '<0x83>': 134,\n",
       "   '<0x84>': 135,\n",
       "   '<0x85>': 136,\n",
       "   '<0x86>': 137,\n",
       "   '<0x87>': 138,\n",
       "   '<0x88>': 139,\n",
       "   '<0x89>': 140,\n",
       "   '<0x8A>': 141,\n",
       "   '<0x8B>': 142,\n",
       "   '<0x8C>': 143,\n",
       "   '<0x8D>': 144,\n",
       "   '<0x8E>': 145,\n",
       "   '<0x8F>': 146,\n",
       "   '<0x90>': 147,\n",
       "   '<0x91>': 148,\n",
       "   '<0x92>': 149,\n",
       "   '<0x93>': 150,\n",
       "   '<0x94>': 151,\n",
       "   '<0x95>': 152,\n",
       "   '<0x96>': 153,\n",
       "   '<0x97>': 154,\n",
       "   '<0x98>': 155,\n",
       "   '<0x99>': 156,\n",
       "   '<0x9A>': 157,\n",
       "   '<0x9B>': 158,\n",
       "   '<0x9C>': 159,\n",
       "   '<0x9D>': 160,\n",
       "   '<0x9E>': 161,\n",
       "   '<0x9F>': 162,\n",
       "   '<0xA0>': 163,\n",
       "   '<0xA1>': 164,\n",
       "   '<0xA2>': 165,\n",
       "   '<0xA3>': 166,\n",
       "   '<0xA4>': 167,\n",
       "   '<0xA5>': 168,\n",
       "   '<0xA6>': 169,\n",
       "   '<0xA7>': 170,\n",
       "   '<0xA8>': 171,\n",
       "   '<0xA9>': 172,\n",
       "   '<0xAA>': 173,\n",
       "   '<0xAB>': 174,\n",
       "   '<0xAC>': 175,\n",
       "   '<0xAD>': 176,\n",
       "   '<0xAE>': 177,\n",
       "   '<0xAF>': 178,\n",
       "   '<0xB0>': 179,\n",
       "   '<0xB1>': 180,\n",
       "   '<0xB2>': 181,\n",
       "   '<0xB3>': 182,\n",
       "   '<0xB4>': 183,\n",
       "   '<0xB5>': 184,\n",
       "   '<0xB6>': 185,\n",
       "   '<0xB7>': 186,\n",
       "   '<0xB8>': 187,\n",
       "   '<0xB9>': 188,\n",
       "   '<0xBA>': 189,\n",
       "   '<0xBB>': 190,\n",
       "   '<0xBC>': 191,\n",
       "   '<0xBD>': 192,\n",
       "   '<0xBE>': 193,\n",
       "   '<0xBF>': 194,\n",
       "   '<0xC0>': 195,\n",
       "   '<0xC1>': 196,\n",
       "   '<0xC2>': 197,\n",
       "   '<0xC3>': 198,\n",
       "   '<0xC4>': 199,\n",
       "   '<0xC5>': 200,\n",
       "   '<0xC6>': 201,\n",
       "   '<0xC7>': 202,\n",
       "   '<0xC8>': 203,\n",
       "   '<0xC9>': 204,\n",
       "   '<0xCA>': 205,\n",
       "   '<0xCB>': 206,\n",
       "   '<0xCC>': 207,\n",
       "   '<0xCD>': 208,\n",
       "   '<0xCE>': 209,\n",
       "   '<0xCF>': 210,\n",
       "   '<0xD0>': 211,\n",
       "   '<0xD1>': 212,\n",
       "   '<0xD2>': 213,\n",
       "   '<0xD3>': 214,\n",
       "   '<0xD4>': 215,\n",
       "   '<0xD5>': 216,\n",
       "   '<0xD6>': 217,\n",
       "   '<0xD7>': 218,\n",
       "   '<0xD8>': 219,\n",
       "   '<0xD9>': 220,\n",
       "   '<0xDA>': 221,\n",
       "   '<0xDB>': 222,\n",
       "   '<0xDC>': 223,\n",
       "   '<0xDD>': 224,\n",
       "   '<0xDE>': 225,\n",
       "   '<0xDF>': 226,\n",
       "   '<0xE0>': 227,\n",
       "   '<0xE1>': 228,\n",
       "   '<0xE2>': 229,\n",
       "   '<0xE3>': 230,\n",
       "   '<0xE4>': 231,\n",
       "   '<0xE5>': 232,\n",
       "   '<0xE6>': 233,\n",
       "   '<0xE7>': 234,\n",
       "   '<0xE8>': 235,\n",
       "   '<0xE9>': 236,\n",
       "   '<0xEA>': 237,\n",
       "   '<0xEB>': 238,\n",
       "   '<0xEC>': 239,\n",
       "   '<0xED>': 240,\n",
       "   '<0xEE>': 241,\n",
       "   '<0xEF>': 242,\n",
       "   '<0xF0>': 243,\n",
       "   '<0xF1>': 244,\n",
       "   '<0xF2>': 245,\n",
       "   '<0xF3>': 246,\n",
       "   '<0xF4>': 247,\n",
       "   '<0xF5>': 248,\n",
       "   '<0xF6>': 249,\n",
       "   '<0xF7>': 250,\n",
       "   '<0xF8>': 251,\n",
       "   '<0xF9>': 252,\n",
       "   '<0xFA>': 253,\n",
       "   '<0xFB>': 254,\n",
       "   '<0xFC>': 255,\n",
       "   '<0xFD>': 256,\n",
       "   '<0xFE>': 257,\n",
       "   '<0xFF>': 258,\n",
       "   '▁▁': 259,\n",
       "   '▁t': 260,\n",
       "   'er': 261,\n",
       "   'in': 262,\n",
       "   '▁a': 263,\n",
       "   'en': 264,\n",
       "   'on': 265,\n",
       "   '▁th': 266,\n",
       "   'es': 267,\n",
       "   '▁▁▁▁': 268,\n",
       "   '▁s': 269,\n",
       "   '▁d': 270,\n",
       "   'at': 271,\n",
       "   'or': 272,\n",
       "   'an': 273,\n",
       "   '▁c': 274,\n",
       "   'is': 275,\n",
       "   're': 276,\n",
       "   'it': 277,\n",
       "   '▁the': 278,\n",
       "   'ar': 279,\n",
       "   'le': 280,\n",
       "   '▁w': 281,\n",
       "   '▁p': 282,\n",
       "   'ou': 283,\n",
       "   'al': 284,\n",
       "   '▁f': 285,\n",
       "   '▁m': 286,\n",
       "   'ed': 287,\n",
       "   '▁o': 288,\n",
       "   '▁b': 289,\n",
       "   'om': 290,\n",
       "   'ion': 291,\n",
       "   'ing': 292,\n",
       "   'ic': 293,\n",
       "   'as': 294,\n",
       "   'el': 295,\n",
       "   'ent': 296,\n",
       "   '▁in': 297,\n",
       "   '▁h': 298,\n",
       "   'nd': 299,\n",
       "   'et': 300,\n",
       "   '▁l': 301,\n",
       "   '▁n': 302,\n",
       "   'st': 303,\n",
       "   '▁to': 304,\n",
       "   'ch': 305,\n",
       "   '▁I': 306,\n",
       "   'ro': 307,\n",
       "   '▁▁▁▁▁▁▁▁': 308,\n",
       "   'il': 309,\n",
       "   '▁of': 310,\n",
       "   'de': 311,\n",
       "   'ct': 312,\n",
       "   '▁(': 313,\n",
       "   'am': 314,\n",
       "   '▁C': 315,\n",
       "   '▁de': 316,\n",
       "   '▁S': 317,\n",
       "   '▁u': 318,\n",
       "   '▁A': 319,\n",
       "   '▁\\\\': 320,\n",
       "   '▁e': 321,\n",
       "   '▁and': 322,\n",
       "   '▁T': 323,\n",
       "   'ol': 324,\n",
       "   '▁v': 325,\n",
       "   'im': 326,\n",
       "   'ot': 327,\n",
       "   'ad': 328,\n",
       "   'ut': 329,\n",
       "   '▁g': 330,\n",
       "   'em': 331,\n",
       "   'ur': 332,\n",
       "   'id': 333,\n",
       "   '▁*': 334,\n",
       "   'ig': 335,\n",
       "   'ra': 336,\n",
       "   '▁re': 337,\n",
       "   '▁is': 338,\n",
       "   'qu': 339,\n",
       "   'ow': 340,\n",
       "   '▁M': 341,\n",
       "   'est': 342,\n",
       "   '▁y': 343,\n",
       "   'se': 344,\n",
       "   've': 345,\n",
       "   'ce': 346,\n",
       "   'ie': 347,\n",
       "   'un': 348,\n",
       "   '▁P': 349,\n",
       "   '▁B': 350,\n",
       "   'ag': 351,\n",
       "   'ul': 352,\n",
       "   '▁=': 353,\n",
       "   'he': 354,\n",
       "   'end': 355,\n",
       "   'ode': 356,\n",
       "   'ter': 357,\n",
       "   'ment': 358,\n",
       "   'os': 359,\n",
       "   '▁D': 360,\n",
       "   'if': 361,\n",
       "   'ation': 362,\n",
       "   '▁for': 363,\n",
       "   '▁r': 364,\n",
       "   '▁L': 365,\n",
       "   '▁you': 366,\n",
       "   '▁be': 367,\n",
       "   'ly': 368,\n",
       "   'ver': 369,\n",
       "   'ab': 370,\n",
       "   'te': 371,\n",
       "   '▁it': 372,\n",
       "   '▁on': 373,\n",
       "   'ri': 374,\n",
       "   'us': 375,\n",
       "   '▁\"': 376,\n",
       "   '▁wh': 377,\n",
       "   '▁con': 378,\n",
       "   '▁H': 379,\n",
       "   '▁st': 380,\n",
       "   'ir': 381,\n",
       "   '▁E': 382,\n",
       "   '▁F': 383,\n",
       "   'ck': 384,\n",
       "   '▁an': 385,\n",
       "   'th': 386,\n",
       "   'eg': 387,\n",
       "   'ay': 388,\n",
       "   'ith': 389,\n",
       "   '▁R': 390,\n",
       "   'ist': 391,\n",
       "   'and': 392,\n",
       "   '▁that': 393,\n",
       "   '▁al': 394,\n",
       "   '▁$': 395,\n",
       "   '▁#': 396,\n",
       "   'od': 397,\n",
       "   'um': 398,\n",
       "   '▁W': 399,\n",
       "   'ht': 400,\n",
       "   'code': 401,\n",
       "   '▁G': 402,\n",
       "   'ate': 403,\n",
       "   'ess': 404,\n",
       "   '▁N': 405,\n",
       "   'ere': 406,\n",
       "   'pp': 407,\n",
       "   '▁as': 408,\n",
       "   '▁se': 409,\n",
       "   '▁pro': 410,\n",
       "   '▁with': 411,\n",
       "   'pe': 412,\n",
       "   '▁k': 413,\n",
       "   'ers': 414,\n",
       "   'pt': 415,\n",
       "   ');': 416,\n",
       "   'lo': 417,\n",
       "   '▁▁▁▁▁': 418,\n",
       "   '▁com': 419,\n",
       "   'ame': 420,\n",
       "   '▁`': 421,\n",
       "   '▁Com': 422,\n",
       "   'ia': 423,\n",
       "   'ant': 424,\n",
       "   '▁la': 425,\n",
       "   '▁{': 426,\n",
       "   '▁en': 427,\n",
       "   'ction': 428,\n",
       "   '▁ex': 429,\n",
       "   'ld': 430,\n",
       "   'ub': 431,\n",
       "   '▁j': 432,\n",
       "   'la': 433,\n",
       "   'ue': 434,\n",
       "   '▁J': 435,\n",
       "   'ich': 436,\n",
       "   '▁do': 437,\n",
       "   '▁O': 438,\n",
       "   '▁qu': 439,\n",
       "   'iv': 440,\n",
       "   'ort': 441,\n",
       "   'art': 442,\n",
       "   '▁un': 443,\n",
       "   '▁##': 444,\n",
       "   '▁this': 445,\n",
       "   'ke': 446,\n",
       "   '▁ha': 447,\n",
       "   '▁-': 448,\n",
       "   'out': 449,\n",
       "   '▁The': 450,\n",
       "   '▁not': 451,\n",
       "   '▁ne': 452,\n",
       "   'ill': 453,\n",
       "   '▁le': 454,\n",
       "   'ci': 455,\n",
       "   'rom': 456,\n",
       "   'ine': 457,\n",
       "   '//': 458,\n",
       "   'op': 459,\n",
       "   'egin': 460,\n",
       "   '▁Comment': 461,\n",
       "   '▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁': 462,\n",
       "   'begin': 463,\n",
       "   'ст': 464,\n",
       "   'ass': 465,\n",
       "   'iz': 466,\n",
       "   ').': 467,\n",
       "   'og': 468,\n",
       "   '▁п': 469,\n",
       "   '▁or': 470,\n",
       "   '▁was': 471,\n",
       "   '▁at': 472,\n",
       "   'our': 473,\n",
       "   '▁i': 474,\n",
       "   'ain': 475,\n",
       "   '▁K': 476,\n",
       "   'на': 477,\n",
       "   '▁V': 478,\n",
       "   'ge': 479,\n",
       "   '▁su': 480,\n",
       "   'ap': 481,\n",
       "   'age': 482,\n",
       "   'ould': 483,\n",
       "   'ne': 484,\n",
       "   'av': 485,\n",
       "   'xt': 486,\n",
       "   'ore': 487,\n",
       "   'ile': 488,\n",
       "   '--': 489,\n",
       "   '▁в': 490,\n",
       "   '▁by': 491,\n",
       "   'li': 492,\n",
       "   'ath': 493,\n",
       "   'ра': 494,\n",
       "   'ber': 495,\n",
       "   'ach': 496,\n",
       "   'all': 497,\n",
       "   '▁Th': 498,\n",
       "   'ult': 499,\n",
       "   '▁}': 500,\n",
       "   '▁U': 501,\n",
       "   '▁us': 502,\n",
       "   '▁z': 503,\n",
       "   'ust': 504,\n",
       "   '▁have': 505,\n",
       "   'lic': 506,\n",
       "   'ни': 507,\n",
       "   '▁can': 508,\n",
       "   'tr': 509,\n",
       "   'com': 510,\n",
       "   '),': 511,\n",
       "   '▁In': 512,\n",
       "   'ind': 513,\n",
       "   'ell': 514,\n",
       "   '▁from': 515,\n",
       "   'ов': 516,\n",
       "   'to': 517,\n",
       "   '▁[': 518,\n",
       "   'able': 519,\n",
       "   'ost': 520,\n",
       "   '▁ch': 521,\n",
       "   'ect': 522,\n",
       "   'ight': 523,\n",
       "   'int': 524,\n",
       "   \"▁'\": 525,\n",
       "   '▁are': 526,\n",
       "   '▁im': 527,\n",
       "   '▁sh': 528,\n",
       "   '▁<': 529,\n",
       "   '▁An': 530,\n",
       "   '▁с': 531,\n",
       "   'ata': 532,\n",
       "   'ire': 533,\n",
       "   '▁tr': 534,\n",
       "   'con': 535,\n",
       "   'ord': 536,\n",
       "   'ity': 537,\n",
       "   'ard': 538,\n",
       "   '▁▁▁▁▁▁': 539,\n",
       "   '▁he': 540,\n",
       "   '▁but': 541,\n",
       "   'oc': 542,\n",
       "   '=\"': 543,\n",
       "   '▁pr': 544,\n",
       "   'ure': 545,\n",
       "   'per': 546,\n",
       "   'ack': 547,\n",
       "   'ork': 548,\n",
       "   'ong': 549,\n",
       "   'ans': 550,\n",
       "   'ко': 551,\n",
       "   'ple': 552,\n",
       "   '▁des': 553,\n",
       "   'ok': 554,\n",
       "   'orm': 555,\n",
       "   'wer': 556,\n",
       "   'ak': 557,\n",
       "   'pr': 558,\n",
       "   'ase': 559,\n",
       "   '▁el': 560,\n",
       "   'ph': 561,\n",
       "   'ac': 562,\n",
       "   '▁und': 563,\n",
       "   '▁ar': 564,\n",
       "   '▁if': 565,\n",
       "   'ud': 566,\n",
       "   'ps': 567,\n",
       "   'ite': 568,\n",
       "   'ble': 569,\n",
       "   'но': 570,\n",
       "   'fer': 571,\n",
       "   'pl': 572,\n",
       "   'ive': 573,\n",
       "   'ang': 574,\n",
       "   'ens': 575,\n",
       "   'ро': 576,\n",
       "   '▁so': 577,\n",
       "   'so': 578,\n",
       "   'ast': 579,\n",
       "   '()': 580,\n",
       "   'swer': 581,\n",
       "   'ru': 582,\n",
       "   'ies': 583,\n",
       "   '▁:': 584,\n",
       "   'au': 585,\n",
       "   'ov': 586,\n",
       "   'ре': 587,\n",
       "   'го': 588,\n",
       "   '▁der': 589,\n",
       "   '▁my': 590,\n",
       "   '▁we': 591,\n",
       "   '▁me': 592,\n",
       "   'nt': 593,\n",
       "   '▁ad': 594,\n",
       "   'urn': 595,\n",
       "   '▁your': 596,\n",
       "   '://': 597,\n",
       "   'are': 598,\n",
       "   '▁all': 599,\n",
       "   'ff': 600,\n",
       "   'io': 601,\n",
       "   'estion': 602,\n",
       "   'ime': 603,\n",
       "   '▁er': 604,\n",
       "   'lass': 605,\n",
       "   '▁и': 606,\n",
       "   '▁which': 607,\n",
       "   'ome': 608,\n",
       "   'ont': 609,\n",
       "   '▁par': 610,\n",
       "   '▁ma': 611,\n",
       "   '▁Y': 612,\n",
       "   '\",': 613,\n",
       "   '▁о': 614,\n",
       "   'ft': 615,\n",
       "   'ial': 616,\n",
       "   'cc': 617,\n",
       "   'ound': 618,\n",
       "   '▁li': 619,\n",
       "   '▁res': 620,\n",
       "   'eth': 621,\n",
       "   'ject': 622,\n",
       "   '▁app': 623,\n",
       "   '▁St': 624,\n",
       "   'ice': 625,\n",
       "   '▁am': 626,\n",
       "   'act': 627,\n",
       "   '▁del': 628,\n",
       "   'gr': 629,\n",
       "   'ated': 630,\n",
       "   'ier': 631,\n",
       "   '▁▁▁▁▁▁▁▁▁▁▁▁': 632,\n",
       "   '▁ab': 633,\n",
       "   '▁et': 634,\n",
       "   'ally': 635,\n",
       "   '..': 636,\n",
       "   'port': 637,\n",
       "   'ik': 638,\n",
       "   '▁per': 639,\n",
       "   '▁cont': 640,\n",
       "   'ри': 641,\n",
       "   'ка': 642,\n",
       "   'ser': 643,\n",
       "   'ли': 644,\n",
       "   'll': 645,\n",
       "   'iew': 646,\n",
       "   'ign': 647,\n",
       "   '_{': 648,\n",
       "   'put': 649,\n",
       "   'one': 650,\n",
       "   'unction': 651,\n",
       "   '▁di': 652,\n",
       "   'ary': 653,\n",
       "   'ition': 654,\n",
       "   'ma': 655,\n",
       "   'ен': 656,\n",
       "   'get': 657,\n",
       "   '▁lo': 658,\n",
       "   '▁val': 659,\n",
       "   '▁Q': 660,\n",
       "   'ran': 661,\n",
       "   '▁д': 662,\n",
       "   'ence': 663,\n",
       "   '▁work': 664,\n",
       "   '▁на': 665,\n",
       "   'ip': 666,\n",
       "   'item': 667,\n",
       "   'ype': 668,\n",
       "   '▁&': 669,\n",
       "   '▁his': 670,\n",
       "   '▁use': 671,\n",
       "   'der': 672,\n",
       "   '▁Answer': 673,\n",
       "   '▁will': 674,\n",
       "   'ize': 675,\n",
       "   'та': 676,\n",
       "   'low': 677,\n",
       "   '▁Ch': 678,\n",
       "   '▁get': 679,\n",
       "   'ide': 680,\n",
       "   'ous': 681,\n",
       "   'ink': 682,\n",
       "   'ption': 683,\n",
       "   'ла': 684,\n",
       "   'turn': 685,\n",
       "   'ung': 686,\n",
       "   'ec': 687,\n",
       "   'ug': 688,\n",
       "   'form': 689,\n",
       "   'res': 690,\n",
       "   'htt': 691,\n",
       "   'oug': 692,\n",
       "   'ль': 693,\n",
       "   '▁no': 694,\n",
       "   'cl': 695,\n",
       "   '▁ro': 696,\n",
       "   '▁one': 697,\n",
       "   'tt': 698,\n",
       "   'cri': 699,\n",
       "   'du': 700,\n",
       "   '▁up': 701,\n",
       "   'то': 702,\n",
       "   '(\"': 703,\n",
       "   '▁ob': 704,\n",
       "   'we': 705,\n",
       "   'ory': 706,\n",
       "   '▁est': 707,\n",
       "   'ery': 708,\n",
       "   'iel': 709,\n",
       "   'str': 710,\n",
       "   'ob': 711,\n",
       "   '▁que': 712,\n",
       "   'ian': 713,\n",
       "   '▁out': 714,\n",
       "   '▁pl': 715,\n",
       "   '▁new': 716,\n",
       "   'ки': 717,\n",
       "   '▁+': 718,\n",
       "   'ry': 719,\n",
       "   'oth': 720,\n",
       "   'ther': 721,\n",
       "   '▁var': 722,\n",
       "   '▁would': 723,\n",
       "   '▁ser': 724,\n",
       "   'tern': 725,\n",
       "   'text': 726,\n",
       "   '▁there': 727,\n",
       "   'ish': 728,\n",
       "   'ror': 729,\n",
       "   'те': 730,\n",
       "   '▁set': 731,\n",
       "   '▁@': 732,\n",
       "   '▁по': 733,\n",
       "   '▁te': 734,\n",
       "   'ex': 735,\n",
       "   '▁return': 736,\n",
       "   'ail': 737,\n",
       "   '▁any': 738,\n",
       "   '▁It': 739,\n",
       "   '▁function': 740,\n",
       "   '{\\\\': 741,\n",
       "   \"',\": 742,\n",
       "   'és': 743,\n",
       "   'ale': 744,\n",
       "   'ан': 745,\n",
       "   '▁when': 746,\n",
       "   'ib': 747,\n",
       "   '▁go': 748,\n",
       "   'ance': 749,\n",
       "   '▁had': 750,\n",
       "   '▁Qu': 751,\n",
       "   '▁comp': 752,\n",
       "   'ле': 753,\n",
       "   '▁з': 754,\n",
       "   'math': 755,\n",
       "   '▁has': 756,\n",
       "   '▁м': 757,\n",
       "   '▁pre': 758,\n",
       "   'ener': 759,\n",
       "   '▁part': 760,\n",
       "   'elf': 761,\n",
       "   '▁die': 762,\n",
       "   '▁like': 763,\n",
       "   'ray': 764,\n",
       "   'irst': 765,\n",
       "   '▁dis': 766,\n",
       "   '▁man': 767,\n",
       "   'rit': 768,\n",
       "   '▁then': 769,\n",
       "   '▁class': 770,\n",
       "   'pro': 771,\n",
       "   '▁po': 772,\n",
       "   '▁using': 773,\n",
       "   'eb': 774,\n",
       "   '▁code': 775,\n",
       "   'own': 776,\n",
       "   '▁some': 777,\n",
       "   'ces': 778,\n",
       "   '▁$\\\\': 779,\n",
       "   'ер': 780,\n",
       "   'lect': 781,\n",
       "   '▁au': 782,\n",
       "   'isch': 783,\n",
       "   '▁col': 784,\n",
       "   '▁–': 785,\n",
       "   'up': 786,\n",
       "   'ons': 787,\n",
       "   '▁add': 788,\n",
       "   'ild': 789,\n",
       "   'iss': 790,\n",
       "   'val': 791,\n",
       "   'ount': 792,\n",
       "   'les': 793,\n",
       "   'vent': 794,\n",
       "   '▁▁▁▁▁▁▁▁▁▁▁▁▁': 795,\n",
       "   '▁Z': 796,\n",
       "   'In': 797,\n",
       "   'row': 798,\n",
       "   'ear': 799,\n",
       "   'ations': 800,\n",
       "   'ah': 801,\n",
       "   'que': 802,\n",
       "   'ublic': 803,\n",
       "   'ank': 804,\n",
       "   '▁sp': 805,\n",
       "   '▁Wh': 806,\n",
       "   '----': 807,\n",
       "   'sk': 808,\n",
       "   'ew': 809,\n",
       "   'ags': 810,\n",
       "   'ти': 811,\n",
       "   'ann': 812,\n",
       "   '▁—': 813,\n",
       "   'ert': 814,\n",
       "   'ace': 815,\n",
       "   'sch': 816,\n",
       "   '▁need': 817,\n",
       "   '▁à': 818,\n",
       "   'ien': 819,\n",
       "   'ough': 820,\n",
       "   'не': 821,\n",
       "   '▁def': 822,\n",
       "   'ij': 823,\n",
       "   'ern': 824,\n",
       "   '▁what': 825,\n",
       "   '▁Ar': 826,\n",
       "   'wo': 827,\n",
       "   'ml': 828,\n",
       "   '</': 829,\n",
       "   '▁Re': 830,\n",
       "   '▁es': 831,\n",
       "   '▁inst': 832,\n",
       "   'bo': 833,\n",
       "   'az': 834,\n",
       "   '▁###': 835,\n",
       "   '▁б': 836,\n",
       "   'erm': 837,\n",
       "   '▁Al': 838,\n",
       "   'led': 839,\n",
       "   'да': 840,\n",
       "   'ten': 841,\n",
       "   'set': 842,\n",
       "   'ло': 843,\n",
       "   '▁comm': 844,\n",
       "   'sh': 845,\n",
       "   'ва': 846,\n",
       "   '▁/': 847,\n",
       "   '▁data': 848,\n",
       "   '▁//': 849,\n",
       "   '](': 850,\n",
       "   '▁str': 851,\n",
       "   'ose': 852,\n",
       "   '▁Un': 853,\n",
       "   'ven': 854,\n",
       "   'St': 855,\n",
       "   '...': 856,\n",
       "   '▁С': 857,\n",
       "   'yst': 858,\n",
       "   '▁«': 859,\n",
       "   'ick': 860,\n",
       "   'ix': 861,\n",
       "   'par': 862,\n",
       "   '▁у': 863,\n",
       "   '▁want': 864,\n",
       "   'ng': 865,\n",
       "   'ote': 866,\n",
       "   '▁gr': 867,\n",
       "   '▁du': 868,\n",
       "   '▁.': 869,\n",
       "   'und': 870,\n",
       "   '▁only': 871,\n",
       "   '▁sa': 872,\n",
       "   'ely': 873,\n",
       "   'vers': 874,\n",
       "   '▁ent': 875,\n",
       "   '))': 876,\n",
       "   \"('\": 877,\n",
       "   '▁mod': 878,\n",
       "   'ava': 879,\n",
       "   'ton': 880,\n",
       "   '▁should': 881,\n",
       "   'ement': 882,\n",
       "   '▁form': 883,\n",
       "   '▁also': 884,\n",
       "   '▁sc': 885,\n",
       "   'ings': 886,\n",
       "   '▁You': 887,\n",
       "   'ón': 888,\n",
       "   '▁kn': 889,\n",
       "   '();': 890,\n",
       "   '▁|': 891,\n",
       "   '▁were': 892,\n",
       "   'ss': 893,\n",
       "   '▁Question': 894,\n",
       "   'ise': 895,\n",
       "   '▁they': 896,\n",
       "   '▁De': 897,\n",
       "   'ond': 898,\n",
       "   '▁sol': 899,\n",
       "   '▁fol': 900,\n",
       "   '▁more': 901,\n",
       "   '▁her': 902,\n",
       "   '▁_': 903,\n",
       "   '▁é': 904,\n",
       "   'atch': 905,\n",
       "   'fter': 906,\n",
       "   '▁cre': 907,\n",
       "   'lock': 908,\n",
       "   'tring': 909,\n",
       "   '▁This': 910,\n",
       "   'ze': 911,\n",
       "   'ado': 912,\n",
       "   'ull': 913,\n",
       "   'ger': 914,\n",
       "   'be': 915,\n",
       "   '▁other': 916,\n",
       "   '▁Tags': 917,\n",
       "   'ution': 918,\n",
       "   'ict': 919,\n",
       "   '▁how': 920,\n",
       "   '▁x': 921,\n",
       "   '▁Se': 922,\n",
       "   '▁che': 923,\n",
       "   'cript': 924,\n",
       "   '▁just': 925,\n",
       "   '▁pos': 926,\n",
       "   'ange': 927,\n",
       "   'ific': 928,\n",
       "   'ree': 929,\n",
       "   '}}': 930,\n",
       "   '▁time': 931,\n",
       "   'app': 932,\n",
       "   'ны': 933,\n",
       "   '▁file': 934,\n",
       "   'ark': 935,\n",
       "   'ical': 936,\n",
       "   '▁first': 937,\n",
       "   '▁int': 938,\n",
       "   '▁В': 939,\n",
       "   '▁He': 940,\n",
       "   'ta': 941,\n",
       "   'ument': 942,\n",
       "   'ors': 943,\n",
       "   'lement': 944,\n",
       "   'rac': 945,\n",
       "   '▁ag': 946,\n",
       "   '▁does': 947,\n",
       "   'yn': 948,\n",
       "   'read': 949,\n",
       "   'ual': 950,\n",
       "   '▁Le': 951,\n",
       "   'ys': 952,\n",
       "   '▁em': 953,\n",
       "   '▁num': 954,\n",
       "   'vel': 955,\n",
       "   'ди': 956,\n",
       "   'over': 957,\n",
       "   '▁dif': 958,\n",
       "   'ethod': 959,\n",
       "   '▁If': 960,\n",
       "   '▁spe': 961,\n",
       "   'ym': 962,\n",
       "   '▁them': 963,\n",
       "   '▁into': 964,\n",
       "   '▁▁▁▁▁▁▁▁▁▁': 965,\n",
       "   '▁les': 966,\n",
       "   '▁its': 967,\n",
       "   'ese': 968,\n",
       "   'ield': 969,\n",
       "   '▁public': 970,\n",
       "   '▁П': 971,\n",
       "   '▁den': 972,\n",
       "   'ystem': 973,\n",
       "   'of': 974,\n",
       "   '▁over': 975,\n",
       "   '->': 976,\n",
       "   '▁fil': 977,\n",
       "   'name': 978,\n",
       "   'inal': 979,\n",
       "   '▁il': 980,\n",
       "   'ample': 981,\n",
       "   '▁way': 982,\n",
       "   'ica': 983,\n",
       "   'во': 984,\n",
       "   'cess': 985,\n",
       "   'itt': 986,\n",
       "   'uch': 987,\n",
       "   '▁where': 988,\n",
       "   'ми': 989,\n",
       "   'org': 990,\n",
       "   'https': 991,\n",
       "   '▁vo': 992,\n",
       "   'ient': 993,\n",
       "   'ove': 994,\n",
       "   '▁value': 995,\n",
       "   'eng': 996,\n",
       "   '▁La': 997,\n",
       "   '^{': 998,\n",
       "   'ref': 999,\n",
       "   ...},\n",
       "  'merges': ['▁k u',\n",
       "   '▁ ku',\n",
       "   '▁k wa',\n",
       "   '▁kw a',\n",
       "   '▁ kwa',\n",
       "   'mb a',\n",
       "   'm ba',\n",
       "   '▁ 2',\n",
       "   '▁k ati',\n",
       "   '▁ka ti',\n",
       "   '▁kat i',\n",
       "   '▁ kati',\n",
       "   '▁ 1',\n",
       "   '▁ka tika',\n",
       "   '▁kat ika',\n",
       "   '▁ katika',\n",
       "   '▁kati ka',\n",
       "   '▁k uwa',\n",
       "   '▁ kuwa',\n",
       "   '▁ku wa',\n",
       "   '▁ 20',\n",
       "   '▁2 0',\n",
       "   'ku wa',\n",
       "   'k uwa',\n",
       "   '▁w ana',\n",
       "   '▁wa na',\n",
       "   '▁ wana',\n",
       "   '▁wan a',\n",
       "   '▁a mba',\n",
       "   '▁am ba',\n",
       "   '▁amb a',\n",
       "   '▁ amba',\n",
       "   'is ha',\n",
       "   'ish a',\n",
       "   'i sha',\n",
       "   '▁k ama',\n",
       "   '▁kam a',\n",
       "   '▁ka ma',\n",
       "   '▁ kama',\n",
       "   '▁w an',\n",
       "   '▁wa n',\n",
       "   '▁ wan',\n",
       "   'we nye',\n",
       "   'wen ye',\n",
       "   'w enye',\n",
       "   'th i',\n",
       "   't hi',\n",
       "   '▁u ku',\n",
       "   '▁uk u',\n",
       "   '▁ uku',\n",
       "   'un gu',\n",
       "   'ung u',\n",
       "   'u ngu',\n",
       "   'sh i',\n",
       "   's hi',\n",
       "   '0 0',\n",
       "   '▁m wa',\n",
       "   '▁ mwa',\n",
       "   '▁mw a',\n",
       "   '▁a na',\n",
       "   '▁an a',\n",
       "   '▁ ana',\n",
       "   '▁ 201',\n",
       "   '▁2 01',\n",
       "   '▁20 1',\n",
       "   '▁k uto',\n",
       "   '▁ kuto',\n",
       "   '▁ku to',\n",
       "   '▁kut o',\n",
       "   '▁in a',\n",
       "   '▁i na',\n",
       "   '▁ ina',\n",
       "   'vy o',\n",
       "   'v yo',\n",
       "   'mb o',\n",
       "   'm bo',\n",
       "   'ri ka',\n",
       "   'rik a',\n",
       "   'r ika',\n",
       "   '▁z i',\n",
       "   '▁ zi',\n",
       "   'in gi',\n",
       "   'ing i',\n",
       "   'i ngi',\n",
       "   '▁w ali',\n",
       "   '▁wa li',\n",
       "   '▁wal i',\n",
       "   '▁ wali',\n",
       "   'se ma',\n",
       "   'sem a',\n",
       "   's ema',\n",
       "   'ka ti',\n",
       "   'k ati',\n",
       "   'kat i',\n",
       "   'y u',\n",
       "   'fa nya',\n",
       "   'fan ya',\n",
       "   'f anya',\n",
       "   'fany a',\n",
       "   'kw a',\n",
       "   'k wa',\n",
       "   '▁k wenye',\n",
       "   '▁kw enye',\n",
       "   '▁ kwenye',\n",
       "   '▁kwe nye',\n",
       "   '▁kwen ye',\n",
       "   'dh a',\n",
       "   'd ha',\n",
       "   'ki wa',\n",
       "   'k iwa',\n",
       "   'mo ja',\n",
       "   '▁hi yo',\n",
       "   '▁ hiyo',\n",
       "   '▁s hi',\n",
       "   '▁sh i',\n",
       "   '▁ shi',\n",
       "   '▁ 3',\n",
       "   '▁w i',\n",
       "   '▁ wi',\n",
       "   'dh i',\n",
       "   'd hi',\n",
       "   '▁y ake',\n",
       "   '▁ya ke',\n",
       "   '▁ yake',\n",
       "   '▁yak e',\n",
       "   'za ni',\n",
       "   'zan i',\n",
       "   'z ani',\n",
       "   'mb e',\n",
       "   'm be',\n",
       "   '▁m ku',\n",
       "   '▁mk u',\n",
       "   'ta ka',\n",
       "   't aka',\n",
       "   'tak a',\n",
       "   'ka na',\n",
       "   'kan a',\n",
       "   'k ana',\n",
       "   'ọ n',\n",
       "   'mb u',\n",
       "   'm bu',\n",
       "   '▁w atu',\n",
       "   '▁wa tu',\n",
       "   '▁wat u',\n",
       "   '▁ watu',\n",
       "   '▁k uma',\n",
       "   '▁ kuma',\n",
       "   '▁ku ma',\n",
       "   '▁kum a',\n",
       "   'mi a',\n",
       "   'm ia',\n",
       "   'ki ni',\n",
       "   'kin i',\n",
       "   'k ini',\n",
       "   '▁v ya',\n",
       "   '▁vy a',\n",
       "   '▁ vya',\n",
       "   '▁y i',\n",
       "   '▁ yi',\n",
       "   'b wa',\n",
       "   'bw a',\n",
       "   'we za',\n",
       "   'w eza',\n",
       "   'wez a',\n",
       "   'za nia',\n",
       "   'zan ia',\n",
       "   'z ania',\n",
       "   'zani a',\n",
       "   'es ha',\n",
       "   'esh a',\n",
       "   'e sha',\n",
       "   '▁a ka',\n",
       "   '▁ak a',\n",
       "   '▁ aka',\n",
       "   '▁s ana',\n",
       "   '▁sa na',\n",
       "   '▁san a',\n",
       "   '▁ sana',\n",
       "   '▁a me',\n",
       "   '▁am e',\n",
       "   '▁ ame',\n",
       "   '▁a ki',\n",
       "   '▁ak i',\n",
       "   '▁ aki',\n",
       "   'en dele',\n",
       "   'end ele',\n",
       "   'ende le',\n",
       "   'mb i',\n",
       "   'm bi',\n",
       "   '▁kw amba',\n",
       "   '▁kwam ba',\n",
       "   '▁ kwamba',\n",
       "   '▁kwa mba',\n",
       "   '▁wa kati',\n",
       "   '▁ wakati',\n",
       "   '▁waka ti',\n",
       "   '▁wak ati',\n",
       "   'sh in',\n",
       "   's hin',\n",
       "   'shi n',\n",
       "   '▁s ha',\n",
       "   '▁sh a',\n",
       "   '▁ sha',\n",
       "   'ch u',\n",
       "   'c hu',\n",
       "   'ba li',\n",
       "   'bal i',\n",
       "   'b ali',\n",
       "   '▁ba ada',\n",
       "   '▁ baada',\n",
       "   '▁baa da',\n",
       "   '▁y ana',\n",
       "   '▁ya na',\n",
       "   '▁ yana',\n",
       "   '▁yan a',\n",
       "   '▁h ii',\n",
       "   '▁hi i',\n",
       "   '▁ hii',\n",
       "   '2 0',\n",
       "   '▁ 4',\n",
       "   'pa ta',\n",
       "   'pat a',\n",
       "   'p ata',\n",
       "   '▁n go',\n",
       "   '▁ng o',\n",
       "   '▁ ngo',\n",
       "   '▁ 5',\n",
       "   '▁w ake',\n",
       "   '▁wa ke',\n",
       "   '▁ wake',\n",
       "   '▁wak e',\n",
       "   '▁u ta',\n",
       "   '▁ut a',\n",
       "   '▁ uta',\n",
       "   '▁ 0',\n",
       "   '▁za idi',\n",
       "   '▁ zaidi',\n",
       "   '▁zai di',\n",
       "   '▁zaid i',\n",
       "   'w ọn',\n",
       "   'wọ n',\n",
       "   '▁ kutoka',\n",
       "   '▁ku toka',\n",
       "   '▁kuto ka',\n",
       "   '▁kut oka',\n",
       "   '▁n di',\n",
       "   '▁ ndi',\n",
       "   '▁nd i',\n",
       "   '▁la kini',\n",
       "   '▁lak ini',\n",
       "   '▁ lakini',\n",
       "   '▁laki ni',\n",
       "   '▁lakin i',\n",
       "   'li wa',\n",
       "   'l iwa',\n",
       "   'ri kali',\n",
       "   'rik ali',\n",
       "   'rika li',\n",
       "   'ti a',\n",
       "   't ia',\n",
       "   '▁u li',\n",
       "   '▁ul i',\n",
       "   '▁ uli',\n",
       "   'on ge',\n",
       "   'ong e',\n",
       "   'o nge',\n",
       "   '▁h ata',\n",
       "   '▁ha ta',\n",
       "   '▁hat a',\n",
       "   '▁ hata',\n",
       "   'ki a',\n",
       "   'k ia',\n",
       "   'zw a',\n",
       "   'z wa',\n",
       "   'bar i',\n",
       "   'ba ri',\n",
       "   'b ari',\n",
       "   '▁m waka',\n",
       "   '▁ mwaka',\n",
       "   '▁mwa ka',\n",
       "   '▁mw aka',\n",
       "   '▁s hu',\n",
       "   '▁sh u',\n",
       "   '▁ shu',\n",
       "   '▁s asa',\n",
       "   '▁sa sa',\n",
       "   '▁ sasa',\n",
       "   '▁sas a',\n",
       "   'ma ni',\n",
       "   'man i',\n",
       "   'm ani',\n",
       "   '▁k azi',\n",
       "   '▁ka zi',\n",
       "   '▁ kazi',\n",
       "   '▁kaz i',\n",
       "   '▁hi vyo',\n",
       "   '▁ hivyo',\n",
       "   '▁hiv yo',\n",
       "   '▁ci kin',\n",
       "   '▁ciki n',\n",
       "   '▁k um',\n",
       "   '▁ kum',\n",
       "   '▁ku m',\n",
       "   '▁i me',\n",
       "   '▁im e',\n",
       "   '▁ ime',\n",
       "   '▁y an',\n",
       "   '▁ya n',\n",
       "   '▁ yan',\n",
       "   '▁mo ja',\n",
       "   '▁ moja',\n",
       "   'kh o',\n",
       "   'k ho',\n",
       "   '▁s iku',\n",
       "   '▁si ku',\n",
       "   '▁ siku',\n",
       "   '▁sik u',\n",
       "   '▁m ata',\n",
       "   '▁ma ta',\n",
       "   '▁mat a',\n",
       "   '▁ mata',\n",
       "   '▁h uo',\n",
       "   '▁hu o',\n",
       "   '▁ huo',\n",
       "   '▁m tu',\n",
       "   '▁ mtu',\n",
       "   '▁mt u',\n",
       "   '▁w ata',\n",
       "   '▁wa ta',\n",
       "   '▁wat a',\n",
       "   '▁ wata',\n",
       "   '▁m ara',\n",
       "   '▁ma ra',\n",
       "   '▁mar a',\n",
       "   '▁ mara',\n",
       "   'yi n',\n",
       "   'y in',\n",
       "   '▁se rikali',\n",
       "   '▁ serikali',\n",
       "   '▁serika li',\n",
       "   '▁n ga',\n",
       "   '▁ng a',\n",
       "   '▁ nga',\n",
       "   'ku n',\n",
       "   'k un',\n",
       "   'in gine',\n",
       "   'ing ine',\n",
       "   'ingi ne',\n",
       "   'es hi',\n",
       "   'esh i',\n",
       "   'e shi',\n",
       "   '▁n chi',\n",
       "   '▁ nchi',\n",
       "   '▁nc hi',\n",
       "   '▁k we',\n",
       "   '▁kw e',\n",
       "   '▁ kwe',\n",
       "   'ri ki',\n",
       "   'rik i',\n",
       "   'r iki',\n",
       "   '▁k ila',\n",
       "   '▁kil a',\n",
       "   '▁ki la',\n",
       "   '▁ kila',\n",
       "   'kh u',\n",
       "   'k hu',\n",
       "   'ch ini',\n",
       "   'chi ni',\n",
       "   'chin i',\n",
       "   'c hini',\n",
       "   '▁al isema',\n",
       "   '▁ali sema',\n",
       "   '▁ alisema',\n",
       "   '▁k asa',\n",
       "   '▁ka sa',\n",
       "   '▁ kasa',\n",
       "   '▁kas a',\n",
       "   'si li',\n",
       "   'sil i',\n",
       "   's ili',\n",
       "   '▁m kuu',\n",
       "   '▁mk uu',\n",
       "   '▁ mkuu',\n",
       "   '▁mku u',\n",
       "   '▁t una',\n",
       "   '▁tu na',\n",
       "   '▁tun a',\n",
       "   '▁ tuna',\n",
       "   '▁a wọn',\n",
       "   '▁aw ọn',\n",
       "   '▁ awọn',\n",
       "   '▁awọ n',\n",
       "   '▁pa moja',\n",
       "   '▁ pamoja',\n",
       "   'ke a',\n",
       "   'k ea',\n",
       "   '▁a ma',\n",
       "   '▁am a',\n",
       "   '▁ ama',\n",
       "   '▁n chini',\n",
       "   '▁nchi ni',\n",
       "   '▁nc hini',\n",
       "   '▁m aa',\n",
       "   '▁ma a',\n",
       "   '▁ maa',\n",
       "   '▁am bayo',\n",
       "   '▁amb ayo',\n",
       "   '▁ ambayo',\n",
       "   '▁amba yo',\n",
       "   'ji ni',\n",
       "   'jin i',\n",
       "   'j ini',\n",
       "   '▁w ame',\n",
       "   '▁wa me',\n",
       "   '▁ wame',\n",
       "   '▁wam e',\n",
       "   'da ni',\n",
       "   'dan i',\n",
       "   'd ani',\n",
       "   '▁t sa',\n",
       "   '▁ts a',\n",
       "   '▁ tsa',\n",
       "   '▁k wan',\n",
       "   '▁kw an',\n",
       "   '▁ kwan',\n",
       "   '▁kwa n',\n",
       "   '▁ kuji',\n",
       "   '▁ku ji',\n",
       "   '▁kuj i',\n",
       "   '▁h ilo',\n",
       "   '▁hi lo',\n",
       "   '▁ hilo',\n",
       "   '▁it a',\n",
       "   '▁i ta',\n",
       "   '▁ ita',\n",
       "   '▁s aba',\n",
       "   '▁sa ba',\n",
       "   '▁sab a',\n",
       "   '▁ saba',\n",
       "   'gi ne',\n",
       "   'gin e',\n",
       "   'g ine',\n",
       "   '▁ 6',\n",
       "   '▁z ina',\n",
       "   '▁ zina',\n",
       "   '▁zi na',\n",
       "   '▁zin a',\n",
       "   '▁h ali',\n",
       "   '▁ha li',\n",
       "   '▁hal i',\n",
       "   '▁ hali',\n",
       "   '▁k una',\n",
       "   '▁kun a',\n",
       "   '▁ kuna',\n",
       "   '▁ku na',\n",
       "   '▁m wenye',\n",
       "   '▁ mwenye',\n",
       "   '▁mw enye',\n",
       "   '▁mwen ye',\n",
       "   '▁mwe nye',\n",
       "   'to a',\n",
       "   't oa',\n",
       "   'zu ngu',\n",
       "   'zung u',\n",
       "   'z ungu',\n",
       "   'zun gu',\n",
       "   '▁k ab',\n",
       "   '▁ka b',\n",
       "   'za ji',\n",
       "   'z aji',\n",
       "   'hu su',\n",
       "   'hus u',\n",
       "   'h usu',\n",
       "   '▁j uu',\n",
       "   '▁ju u',\n",
       "   '▁ juu',\n",
       "   '▁tan zania',\n",
       "   '▁ tanzania',\n",
       "   '▁tanzani a',\n",
       "   '▁h awa',\n",
       "   '▁ha wa',\n",
       "   '▁ hawa',\n",
       "   'kw e',\n",
       "   'k we',\n",
       "   'on gozi',\n",
       "   'ongo zi',\n",
       "   'di li',\n",
       "   'd ili',\n",
       "   'dil i',\n",
       "   '▁ha bari',\n",
       "   '▁hab ari',\n",
       "   '▁ habari',\n",
       "   '▁haba ri',\n",
       "   '▁d aga',\n",
       "   '▁da ga',\n",
       "   '▁dag a',\n",
       "   '▁ daga',\n",
       "   'zu ri',\n",
       "   'z uri',\n",
       "   'zur i',\n",
       "   '▁m aka',\n",
       "   '▁ma ka',\n",
       "   '▁mak a',\n",
       "   '▁ maka',\n",
       "   '▁is i',\n",
       "   '▁i si',\n",
       "   '▁ isi',\n",
       "   'zi ri',\n",
       "   'z iri',\n",
       "   'zir i',\n",
       "   '▁m bali',\n",
       "   '▁ mbali',\n",
       "   '▁mba li',\n",
       "   '▁mb ali',\n",
       "   'to to',\n",
       "   'tot o',\n",
       "   't oto',\n",
       "   'th o',\n",
       "   't ho',\n",
       "   'dw a',\n",
       "   'd wa',\n",
       "   '▁ 19',\n",
       "   '▁1 9',\n",
       "   'end elea',\n",
       "   'ende lea',\n",
       "   'endele a',\n",
       "   '▁wa lio',\n",
       "   '▁wal io',\n",
       "   '▁ walio',\n",
       "   '▁wali o',\n",
       "   'tu mia',\n",
       "   't umia',\n",
       "   'tumi a',\n",
       "   'tum ia',\n",
       "   '▁w aki',\n",
       "   '▁wa ki',\n",
       "   '▁ waki',\n",
       "   '▁wak i',\n",
       "   '▁n gu',\n",
       "   '▁ng u',\n",
       "   '▁ ngu',\n",
       "   '▁r ais',\n",
       "   '▁ra is',\n",
       "   '▁ rais',\n",
       "   '▁rai s',\n",
       "   '▁ 7',\n",
       "   '▁a ji',\n",
       "   '▁aj i',\n",
       "   '▁ aji',\n",
       "   '▁m w',\n",
       "   '▁ mw',\n",
       "   'bi li',\n",
       "   'bil i',\n",
       "   'b ili',\n",
       "   'sh u',\n",
       "   's hu',\n",
       "   '▁h uu',\n",
       "   '▁hu u',\n",
       "   '▁ huu',\n",
       "   'ji a',\n",
       "   'j ia',\n",
       "   '▁ku pi',\n",
       "   '▁kup i',\n",
       "   '▁h uku',\n",
       "   '▁hu ku',\n",
       "   '▁ huku',\n",
       "   '▁ili yo',\n",
       "   '▁ iliyo',\n",
       "   '▁le o',\n",
       "   '▁ leo',\n",
       "   '▁ 10',\n",
       "   '▁1 0',\n",
       "   '▁w annan',\n",
       "   '▁ wannan',\n",
       "   '▁wan nan',\n",
       "   '▁wanna n',\n",
       "   '▁m wana',\n",
       "   '▁ mwana',\n",
       "   '▁mwa na',\n",
       "   '▁mw ana',\n",
       "   '▁mwan a',\n",
       "   'fa ni',\n",
       "   'fan i',\n",
       "   'f ani',\n",
       "   '▁ kufanya',\n",
       "   '▁ku fanya',\n",
       "   '▁kufa nya',\n",
       "   '▁kuf anya',\n",
       "   'kh a',\n",
       "   'k ha',\n",
       "   '▁ 8',\n",
       "   '▁s uka',\n",
       "   '▁su ka',\n",
       "   '▁ suka',\n",
       "   'fu a',\n",
       "   'f ua',\n",
       "   '▁t are',\n",
       "   '▁tar e',\n",
       "   '▁ta re',\n",
       "   '▁ tare',\n",
       "   '▁am bao',\n",
       "   '▁amb ao',\n",
       "   '▁ ambao',\n",
       "   '▁amba o',\n",
       "   'hl a',\n",
       "   'h la',\n",
       "   '▁u me',\n",
       "   '▁um e',\n",
       "   '▁ ume',\n",
       "   'ku a',\n",
       "   'k ua',\n",
       "   'l wa',\n",
       "   'en ye',\n",
       "   'eny e',\n",
       "   'e nye',\n",
       "   'ti n',\n",
       "   't in',\n",
       "   '▁w ao',\n",
       "   '▁wa o',\n",
       "   '▁ wao',\n",
       "   'tu mi',\n",
       "   't umi',\n",
       "   'tum i',\n",
       "   'is ho',\n",
       "   'ish o',\n",
       "   'i sho',\n",
       "   '▁a ba',\n",
       "   '▁ab a',\n",
       "   '▁ aba',\n",
       "   'g wa',\n",
       "   'ka ma',\n",
       "   'k ama',\n",
       "   'kam a',\n",
       "   '▁y ao',\n",
       "   '▁ya o',\n",
       "   '▁ yao',\n",
       "   '▁ ƙ',\n",
       "   '▁k ui',\n",
       "   '▁ku i',\n",
       "   '▁ ọ',\n",
       "   '▁ 9',\n",
       "   '▁ce wa',\n",
       "   '▁ cewa',\n",
       "   'th u',\n",
       "   't hu',\n",
       "   'ku u',\n",
       "   'k uu',\n",
       "   '▁m ia',\n",
       "   '▁mi a',\n",
       "   '▁ mia',\n",
       "   'sh o',\n",
       "   's ho',\n",
       "   'an chi',\n",
       "   'anc hi',\n",
       "   'anch i',\n",
       "   'a nchi',\n",
       "   'tu a',\n",
       "   't ua',\n",
       "   'py a',\n",
       "   'p ya',\n",
       "   '▁i ya',\n",
       "   '▁ iya',\n",
       "   '▁iy a',\n",
       "   '▁n yu',\n",
       "   '▁ny u',\n",
       "   '▁k uli',\n",
       "   '▁ kuli',\n",
       "   '▁ku li',\n",
       "   '▁kul i',\n",
       "   '▁m ko',\n",
       "   '▁mk o',\n",
       "   '▁ mko',\n",
       "   'an zi',\n",
       "   'anz i',\n",
       "   'ka ta',\n",
       "   'k ata',\n",
       "   'kat a',\n",
       "   'tu n',\n",
       "   't un',\n",
       "   '▁m asu',\n",
       "   '▁ma su',\n",
       "   '▁mas u',\n",
       "   '▁ masu',\n",
       "   'ha ri',\n",
       "   'har i',\n",
       "   'h ari',\n",
       "   'ka ni',\n",
       "   'kan i',\n",
       "   'k ani',\n",
       "   'zung um',\n",
       "   'zungu m',\n",
       "   'zun gum',\n",
       "   'zi ma',\n",
       "   'z ima',\n",
       "   '▁sa babu',\n",
       "   '▁sab abu',\n",
       "   '▁ sababu',\n",
       "   '▁saba bu',\n",
       "   'pi ta',\n",
       "   'pit a',\n",
       "   'p ita',\n",
       "   '▁e li',\n",
       "   '▁el i',\n",
       "   '▁ eli',\n",
       "   'mi i',\n",
       "   'm ii',\n",
       "   'sw a',\n",
       "   's wa',\n",
       "   '▁h ayo',\n",
       "   '▁ha yo',\n",
       "   '▁hay o',\n",
       "   '▁ hayo',\n",
       "   '▁h apa',\n",
       "   '▁ha pa',\n",
       "   '▁ hapa',\n",
       "   '▁hap a',\n",
       "   '▁n dani',\n",
       "   '▁ ndani',\n",
       "   '▁nda ni',\n",
       "   '▁nd ani',\n",
       "   '▁ndan i',\n",
       "   'si a',\n",
       "   's ia',\n",
       "   '▁hu yo',\n",
       "   '▁ huyo',\n",
       "   'ju a',\n",
       "   'j ua',\n",
       "   'es ema',\n",
       "   'ese ma',\n",
       "   'e sema',\n",
       "   '▁y ako',\n",
       "   '▁ya ko',\n",
       "   '▁ yako',\n",
       "   '▁yak o',\n",
       "   'sh wa',\n",
       "   's hwa',\n",
       "   '▁h adi',\n",
       "   '▁ha di',\n",
       "   '▁had i',\n",
       "   '▁ hadi',\n",
       "   'hi di',\n",
       "   'h idi',\n",
       "   'hid i',\n",
       "   'ta ji',\n",
       "   't aji',\n",
       "   'yi ka',\n",
       "   'y ika',\n",
       "   'ts a',\n",
       "   't sa',\n",
       "   'w u',\n",
       "   'mn a',\n",
       "   'm na',\n",
       "   '0 00',\n",
       "   '00 0',\n",
       "   '▁s ala',\n",
       "   '▁sa la',\n",
       "   '▁sal a',\n",
       "   '▁ sala',\n",
       "   '2 01',\n",
       "   '20 1',\n",
       "   '▁k uu',\n",
       "   '▁ kuu',\n",
       "   '▁ku u',\n",
       "   '▁m aji',\n",
       "   '▁ma ji',\n",
       "   '▁maj i',\n",
       "   '▁ maji',\n",
       "   'mi li',\n",
       "   'mil i',\n",
       "   'm ili',\n",
       "   '▁b ara',\n",
       "   '▁bar a',\n",
       "   '▁ba ra',\n",
       "   '▁ bara',\n",
       "   '▁k wanza',\n",
       "   '▁kw anza',\n",
       "   '▁ kwanza',\n",
       "   '▁kwan za',\n",
       "   '▁ kubwa',\n",
       "   '▁ku bwa',\n",
       "   '▁kub wa',\n",
       "   '▁w ani',\n",
       "   '▁wa ni',\n",
       "   '▁ wani',\n",
       "   '▁wan i',\n",
       "   '▁m ungu',\n",
       "   '▁mu ngu',\n",
       "   '▁ mungu',\n",
       "   '▁mun gu',\n",
       "   'we zi',\n",
       "   'w ezi',\n",
       "   'wez i',\n",
       "   'ch agu',\n",
       "   'cha gu',\n",
       "   '▁k itu',\n",
       "   '▁ki tu',\n",
       "   '▁ kitu',\n",
       "   '▁kit u',\n",
       "   '▁m ambo',\n",
       "   '▁ma mbo',\n",
       "   '▁ mambo',\n",
       "   '▁mam bo',\n",
       "   'is hwa',\n",
       "   'ish wa',\n",
       "   'i shwa',\n",
       "   '▁z ili',\n",
       "   '▁ zili',\n",
       "   '▁zi li',\n",
       "   'wa na',\n",
       "   'wan a',\n",
       "   'w ana',\n",
       "   '▁mi aka',\n",
       "   '▁ miaka',\n",
       "   '▁mia ka',\n",
       "   '▁m aha',\n",
       "   '▁ma ha',\n",
       "   '▁ maha',\n",
       "   '▁mah a',\n",
       "   '▁ kuhusu',\n",
       "   '▁ku husu',\n",
       "   '▁kuhu su',\n",
       "   '▁kuh usu',\n",
       "   'kh e',\n",
       "   'k he',\n",
       "   'un ga',\n",
       "   'ung a',\n",
       "   'u nga',\n",
       "   'do go',\n",
       "   'dog o',\n",
       "   'd ogo',\n",
       "   '▁h aki',\n",
       "   '▁ha ki',\n",
       "   '▁ haki',\n",
       "   '▁hak i',\n",
       "   '▁u sa',\n",
       "   '▁us a',\n",
       "   '▁ usa',\n",
       "   'la ya',\n",
       "   'lay a',\n",
       "   'l aya',\n",
       "   '▁ wananchi',\n",
       "   '▁wana nchi',\n",
       "   '▁wan anchi',\n",
       "   '▁wanan chi',\n",
       "   '▁m uda',\n",
       "   '▁mu da',\n",
       "   '▁mud a',\n",
       "   '▁ muda',\n",
       "   'ta no',\n",
       "   'tan o',\n",
       "   't ano',\n",
       "   'ti c',\n",
       "   't ic',\n",
       "   'wi li',\n",
       "   'w ili',\n",
       "   'wil i',\n",
       "   '▁w ote',\n",
       "   '▁wo te',\n",
       "   '▁ wote',\n",
       "   '▁u we',\n",
       "   '▁ uwe',\n",
       "   '▁uw e',\n",
       "   'che zo',\n",
       "   'chez o',\n",
       "   '▁m moja',\n",
       "   '▁ mmoja',\n",
       "   'le la',\n",
       "   'l ela',\n",
       "   'mb ali',\n",
       "   'm bali',\n",
       "   'mba li',\n",
       "   '▁h ai',\n",
       "   '▁ha i',\n",
       "   '▁ hai',\n",
       "   '▁k ili',\n",
       "   '▁kil i',\n",
       "   '▁ki li',\n",
       "   '▁ kili',\n",
       "   '▁a ta',\n",
       "   '▁at a',\n",
       "   '▁ ata',\n",
       "   '▁h aku',\n",
       "   '▁ha ku',\n",
       "   '▁ haku',\n",
       "   '▁hak u',\n",
       "   '▁m wan',\n",
       "   '▁ mwan',\n",
       "   '▁mwa n',\n",
       "   '▁mw an',\n",
       "   '▁w enye',\n",
       "   '▁we nye',\n",
       "   '▁wen ye',\n",
       "   '▁ wenye',\n",
       "   '▁ma ana',\n",
       "   '▁ maana',\n",
       "   '▁maa na',\n",
       "   '▁n je',\n",
       "   '▁nj e',\n",
       "   '▁ nje',\n",
       "   'ka zi',\n",
       "   'kaz i',\n",
       "   'k azi',\n",
       "   'le wa',\n",
       "   'ti ons',\n",
       "   'tion s',\n",
       "   't ions',\n",
       "   'tio ns',\n",
       "   'ki sha',\n",
       "   'k isha',\n",
       "   'kis ha',\n",
       "   'kish a',\n",
       "   '▁wa nao',\n",
       "   '▁ wanao',\n",
       "   '▁wana o',\n",
       "   '▁wan ao',\n",
       "   '▁ 200',\n",
       "   '▁2 00',\n",
       "   '▁20 0',\n",
       "   '▁y ali',\n",
       "   '▁ya li',\n",
       "   '▁ yali',\n",
       "   '1 0',\n",
       "   'he mu',\n",
       "   'hem u',\n",
       "   'h emu',\n",
       "   'hu si',\n",
       "   'hus i',\n",
       "   'h usi',\n",
       "   '▁k uku',\n",
       "   '▁ kuku',\n",
       "   '▁ku ku',\n",
       "   '▁am baye',\n",
       "   '▁amb aye',\n",
       "   '▁ ambaye',\n",
       "   '▁amba ye',\n",
       "   'ma li',\n",
       "   'mal i',\n",
       "   'm ali',\n",
       "   '▁a ti',\n",
       "   '▁at i',\n",
       "   '▁ ati',\n",
       "   'sh ara',\n",
       "   'sha ra',\n",
       "   's hara',\n",
       "   'shar a',\n",
       "   '▁k usa',\n",
       "   '▁ kusa',\n",
       "   '▁ku sa',\n",
       "   '▁kus a',\n",
       "   '▁e zi',\n",
       "   '▁ez i',\n",
       "   '▁ ezi',\n",
       "   'ar ifa',\n",
       "   'ari fa',\n",
       "   '▁al iye',\n",
       "   '▁ali ye',\n",
       "   '▁ aliye',\n",
       "   '▁aliy e',\n",
       "   'ad hi',\n",
       "   'a dhi',\n",
       "   'adh i',\n",
       "   'mi wa',\n",
       "   'm iwa',\n",
       "   '▁h ao',\n",
       "   '▁ha o',\n",
       "   '▁ hao',\n",
       "   'j wa',\n",
       "   'jw a',\n",
       "   'ki ti',\n",
       "   'kit i',\n",
       "   'k iti',\n",
       "   '▁a jili',\n",
       "   '▁aj ili',\n",
       "   '▁aji li',\n",
       "   'we nda',\n",
       "   'wend a',\n",
       "   'wen da',\n",
       "   'w enda',\n",
       "   'shi riki',\n",
       "   'shiri ki',\n",
       "   'shir iki',\n",
       "   'shirik i',\n",
       "   '▁k uta',\n",
       "   '▁ kuta',\n",
       "   '▁ku ta',\n",
       "   '▁kut a',\n",
       "   '▁ 12',\n",
       "   '▁1 2',\n",
       "   'dd a',\n",
       "   'd da',\n",
       "   '▁ ṣ',\n",
       "   'tw a',\n",
       "   't wa',\n",
       "   '▁w engi',\n",
       "   '▁we ngi',\n",
       "   '▁wen gi',\n",
       "   '▁ wengi',\n",
       "   'mb ia',\n",
       "   'm bia',\n",
       "   'mbi a',\n",
       "   ...]}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(vocab_dir/\"tokenizer.json\", \"r\") as f:\n",
    "    vocab = json.load(f)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"lelapa/InkubaLM-0.4B\"\n",
    "tokenizer_old = AutoTokenizer.from_pretrained(model_name)\n",
    "min_token_freq = 183"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the token frequencies again, but this time keep the indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a count of how frequently each token appears in the dataset\n",
    "token_counts = {}\n",
    "for text in data:\n",
    "    tokens = tokenizer_old.encode(text.strip())\n",
    "    for t in tokens:\n",
    "        token_counts[t] = token_counts.get(t, 0) + 1\n",
    "token_ids = sorted(list(token_counts.keys()))\n",
    "\n",
    "# Covert to a Pandas `DataFrame`:\n",
    "token_counts = pd.DataFrame(\n",
    "    {\"token\": token_counts.keys(), \"count\": token_counts.values()}\n",
    ").sort_values(by=\"count\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limit the vocabulary to tokens with at least `min_token_freq` occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens used: 7886\n"
     ]
    }
   ],
   "source": [
    "token_ids = sorted(token_counts[token_counts[\"count\"] >= min_token_freq][\"token\"].tolist())\n",
    "print(\"Number of tokens used:\", len(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure all byte-level tokens are included in the new vocab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated number of tokens used: 8076\n"
     ]
    }
   ],
   "source": [
    "token_ids = sorted(list(set(token_ids + list(range(259)))))\n",
    "print(\"Updated number of tokens used:\", len(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure single letter tokens are included:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated number of tokens used: 8106\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "letters = [l for l in string.ascii_lowercase + string.ascii_uppercase]\n",
    "letters = letters + [\"▁\" + l for l in letters]\n",
    "for l in letters:\n",
    "    t = tokenizer_old.convert_tokens_to_ids(l)\n",
    "    if t == 0:\n",
    "        print(f\"Token {l} not found in old tokenizer.\")\n",
    "    else:\n",
    "        token_ids.append(t)\n",
    "token_ids = sorted(list(set(token_ids)))\n",
    "print(\"Updated number of tokens used:\", len(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the old tokenizer's configuration files. We will then load the json files and update them as required for our new tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_old.save_pretrained(\"/tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New vocabulary size: 8064\n"
     ]
    }
   ],
   "source": [
    "if reproducibility:\n",
    "    new_vocab = set(keep_tokens)\n",
    "else:\n",
    "    new_vocab = {tokenizer_old.convert_ids_to_tokens(i) for i in token_ids}\n",
    "print(\"New vocabulary size:\", len(new_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the model using the `sentencepiece` library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by loading the original model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "991189"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = sp_model.ModelProto()\n",
    "m.ParseFromString(open(\"/tmp/tokenizer.model\", \"rb\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <unk> 2\n",
      "1 <s> 3\n",
      "2 </s> 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2: 1, 3: 2, 6: 256, 1: 61529}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "piece_types = {}\n",
    "for i, piece in enumerate(m.pieces):\n",
    "    piece_types[piece.type] = piece_types.get(piece.type, 0) + 1\n",
    "    if piece.type not in (1, 6):\n",
    "        print(i, piece.piece, piece.type)\n",
    "piece_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(m.pieces) == tokenizer_old.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over `m.pieces` and keep only keep the tokens that are in the new vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8064\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "pieces_old = copy.deepcopy(m.pieces)\n",
    "while len(m.pieces) > 0:\n",
    "    _  = m.pieces.pop()\n",
    "for piece in pieces_old:\n",
    "    if piece.piece in new_vocab or piece.type != 1:\n",
    "        m.pieces.add().CopyFrom(piece)\n",
    "print(len(m.pieces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over `m.pieces` and keep only keep the tokens that are in the new vocab.\n",
    "# Also keep all byte pieces (`piece.type == 6`)\n",
    "# seen = set()\n",
    "# while True:\n",
    "#     if m.pieces[0].piece in seen:\n",
    "#         break\n",
    "#     x = m.pieces.pop(0)\n",
    "#     seen.add(x.piece)\n",
    "#     if x.piece in new_vocab or x.type == 6:\n",
    "#         m.pieces.append(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the updated model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "new_tokenizer_dir = basedir/\"tokenizer_new\"\n",
    "if os.path.exists(new_tokenizer_dir):\n",
    "    shutil.rmtree(new_tokenizer_dir)\n",
    "os.makedirs(new_tokenizer_dir, exist_ok=True)\n",
    "\n",
    "# Serialize the sentencepiece model\n",
    "with open(new_tokenizer_dir/'tokenizer.model', 'wb') as f:\n",
    "    f.write(m.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tokenizer files remain the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = shutil.copy(\"/tmp/tokenizer_config.json\", new_tokenizer_dir)\n",
    "_ = shutil.copy(\"/tmp/special_tokens_map.json\", new_tokenizer_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test loading the updated model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "assert sp.load(str(new_tokenizer_dir/'tokenizer.model'))\n",
    "\n",
    "# Test loading the new tokenizer with `transformers`:\n",
    "try:\n",
    "    tokenizer = LlamaTokenizerFast.from_pretrained(new_tokenizer_dir)\n",
    "except Exception:\n",
    "    print(\"Cannot load tokenizer with `transformers`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wa', 'stani']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.encode(\"Wastani\", out_type=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8064"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the `tokenizers.json` vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/tmp/tokenizer.json\", \"r\") as f:\n",
    "    tokenizer_json = json.load(f)\n",
    "tokenizer_json[\"model\"][\"vocab\"] = {tokenizer.convert_ids_to_tokens(i): i for i in range(tokenizer.vocab_size)}\n",
    "\n",
    "new_tokens = set(tokenizer_json[\"model\"][\"vocab\"].keys())\n",
    "new_merges = []\n",
    "for m in tokenizer_json[\"model\"][\"merges\"]:\n",
    "    if m[0] in new_tokens and m[1] in new_tokens and \"\".join(m) in new_tokens:\n",
    "        new_merges.append(m)\n",
    "tokenizer_json[\"model\"][\"merges\"] = new_merges\n",
    "\n",
    "with open(new_tokenizer_dir/\"tokenizer.json\", \"w\") as f:\n",
    "    json.dump(tokenizer_json, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the updated tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁Wa', 'stani']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_new = AutoTokenizer.from_pretrained(new_tokenizer_dir)\n",
    "tokenizer_new.tokenize(\"Wastani\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁Below',\n",
       " '▁is',\n",
       " '▁an',\n",
       " '▁instruction',\n",
       " '▁that',\n",
       " '▁describes',\n",
       " '▁a',\n",
       " '▁task',\n",
       " '.',\n",
       " '▁Write',\n",
       " '▁a',\n",
       " '▁response',\n",
       " '▁that',\n",
       " '▁appro',\n",
       " 'p',\n",
       " 'riat',\n",
       " 'ely',\n",
       " '▁comple',\n",
       " 'tes',\n",
       " '▁the',\n",
       " '▁request',\n",
       " '.',\n",
       " '<0x0A>',\n",
       " '<0x0A>',\n",
       " '##',\n",
       " '#',\n",
       " '▁Instru',\n",
       " 'ction',\n",
       " ':',\n",
       " '<0x0A>',\n",
       " 'Ch',\n",
       " 'ang',\n",
       " 'anua',\n",
       " '▁mawazo',\n",
       " '▁ya',\n",
       " '▁ma',\n",
       " 'tini',\n",
       " '▁yana',\n",
       " 'y',\n",
       " 'of',\n",
       " 'u',\n",
       " 'ata',\n",
       " '▁na',\n",
       " '▁ua',\n",
       " 'in',\n",
       " 'ishe',\n",
       " '▁ma',\n",
       " 'tini',\n",
       " '▁hayo',\n",
       " '▁katika',\n",
       " '▁mo',\n",
       " 'j',\n",
       " 'aw',\n",
       " 'ap',\n",
       " 'o',\n",
       " '▁ya',\n",
       " '▁lebo',\n",
       " '▁z',\n",
       " 'ifu',\n",
       " 'ata',\n",
       " 'zo',\n",
       " '.',\n",
       " '▁Ch',\n",
       " 'anya',\n",
       " ':',\n",
       " '▁iwapo',\n",
       " '▁ma',\n",
       " 'tini',\n",
       " '▁yan',\n",
       " 'ado',\n",
       " 'keza',\n",
       " '▁mawazo',\n",
       " ',',\n",
       " '▁mtazamo',\n",
       " '▁na',\n",
       " '▁hali',\n",
       " '▁chanya',\n",
       " '▁ya',\n",
       " '▁k',\n",
       " 'ih',\n",
       " 'isia',\n",
       " '.',\n",
       " '▁H',\n",
       " 'asi',\n",
       " ':',\n",
       " '▁iwapo',\n",
       " '▁ma',\n",
       " 'tini',\n",
       " '▁yan',\n",
       " 'ado',\n",
       " 'keza',\n",
       " '▁mawazo',\n",
       " '▁au',\n",
       " '▁hisia',\n",
       " '▁hasi',\n",
       " '.',\n",
       " '▁Wa',\n",
       " 'stani',\n",
       " ':',\n",
       " '▁iwapo',\n",
       " '▁ma',\n",
       " 'tini',\n",
       " '▁hay',\n",
       " 'ado',\n",
       " 'kezi',\n",
       " '▁lugha',\n",
       " '▁chanya',\n",
       " '▁au',\n",
       " '▁hasi',\n",
       " '▁kwa',\n",
       " '▁njia',\n",
       " '▁ya',\n",
       " '▁moja',\n",
       " '▁kwa',\n",
       " '▁moja',\n",
       " '▁au',\n",
       " '▁isiyo',\n",
       " '▁ya',\n",
       " '▁moja',\n",
       " '▁kwa',\n",
       " '▁moja',\n",
       " '.',\n",
       " '<0x0A>',\n",
       " '<0x0A>',\n",
       " '##',\n",
       " '#',\n",
       " '▁Input',\n",
       " ':',\n",
       " '<0x0A>',\n",
       " 'habari',\n",
       " '▁tunaomba',\n",
       " '▁radhi',\n",
       " '▁kwa',\n",
       " '▁us',\n",
       " 'umb',\n",
       " 'uf',\n",
       " 'u',\n",
       " '▁unao',\n",
       " 'j',\n",
       " 'ito',\n",
       " 'keza',\n",
       " '▁taf',\n",
       " 'adh',\n",
       " 'ali',\n",
       " '▁tu',\n",
       " 'andi',\n",
       " 'kie',\n",
       " '▁mita',\n",
       " '▁namba',\n",
       " '▁yako',\n",
       " '▁kwenye',\n",
       " '▁d',\n",
       " 'm',\n",
       " '▁kwa',\n",
       " '▁msaada',\n",
       " '▁zaidi',\n",
       " '▁p',\n",
       " 'j',\n",
       " '<0x0A>',\n",
       " '<0x0A>',\n",
       " '##',\n",
       " '#',\n",
       " '▁Response',\n",
       " ':',\n",
       " '<0x0A>',\n",
       " 'Wa',\n",
       " 'stani']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_old.tokenize(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁B',\n",
       " 'el',\n",
       " 'ow',\n",
       " '▁is',\n",
       " '▁an',\n",
       " '▁in',\n",
       " 'st',\n",
       " 'ru',\n",
       " 'ction',\n",
       " '▁that',\n",
       " '▁des',\n",
       " 'c',\n",
       " 'ri',\n",
       " 'bes',\n",
       " '▁a',\n",
       " '▁task',\n",
       " '.',\n",
       " '▁W',\n",
       " 'r',\n",
       " 'ite',\n",
       " '▁a',\n",
       " '▁res',\n",
       " 'p',\n",
       " 'on',\n",
       " 'se',\n",
       " '▁that',\n",
       " '▁appro',\n",
       " 'p',\n",
       " 'riat',\n",
       " 'ely',\n",
       " '▁comple',\n",
       " 'tes',\n",
       " '▁the',\n",
       " '▁re',\n",
       " 'qu',\n",
       " 'est',\n",
       " '.',\n",
       " '<0x0A>',\n",
       " '<0x0A>',\n",
       " '##',\n",
       " '#',\n",
       " '▁In',\n",
       " 'st',\n",
       " 'ru',\n",
       " 'ction',\n",
       " ':',\n",
       " '<0x0A>',\n",
       " 'Ch',\n",
       " 'ang',\n",
       " 'anua',\n",
       " '▁mawazo',\n",
       " '▁ya',\n",
       " '▁ma',\n",
       " 'tini',\n",
       " '▁yana',\n",
       " 'y',\n",
       " 'of',\n",
       " 'u',\n",
       " 'ata',\n",
       " '▁na',\n",
       " '▁ua',\n",
       " 'in',\n",
       " 'ishe',\n",
       " '▁ma',\n",
       " 'tini',\n",
       " '▁hayo',\n",
       " '▁katika',\n",
       " '▁mo',\n",
       " 'j',\n",
       " 'aw',\n",
       " 'ap',\n",
       " 'o',\n",
       " '▁ya',\n",
       " '▁lebo',\n",
       " '▁z',\n",
       " 'ifu',\n",
       " 'ata',\n",
       " 'zo',\n",
       " '.',\n",
       " '▁Ch',\n",
       " 'anya',\n",
       " ':',\n",
       " '▁i',\n",
       " 'w',\n",
       " 'ap',\n",
       " 'o',\n",
       " '▁ma',\n",
       " 'tini',\n",
       " '▁yan',\n",
       " 'ado',\n",
       " 'keza',\n",
       " '▁mawazo',\n",
       " ',',\n",
       " '▁mta',\n",
       " 'zam',\n",
       " 'o',\n",
       " '▁na',\n",
       " '▁hali',\n",
       " '▁chanya',\n",
       " '▁ya',\n",
       " '▁k',\n",
       " 'ih',\n",
       " 'isia',\n",
       " '.',\n",
       " '▁H',\n",
       " 'asi',\n",
       " ':',\n",
       " '▁i',\n",
       " 'w',\n",
       " 'ap',\n",
       " 'o',\n",
       " '▁ma',\n",
       " 'tini',\n",
       " '▁yan',\n",
       " 'ado',\n",
       " 'keza',\n",
       " '▁mawazo',\n",
       " '▁au',\n",
       " '▁hisia',\n",
       " '▁hasi',\n",
       " '.',\n",
       " '▁Wa',\n",
       " 'stani',\n",
       " ':',\n",
       " '▁i',\n",
       " 'w',\n",
       " 'ap',\n",
       " 'o',\n",
       " '▁ma',\n",
       " 'tini',\n",
       " '▁hay',\n",
       " 'ado',\n",
       " 'kezi',\n",
       " '▁l',\n",
       " 'ug',\n",
       " 'ha',\n",
       " '▁chanya',\n",
       " '▁au',\n",
       " '▁hasi',\n",
       " '▁kwa',\n",
       " '▁njia',\n",
       " '▁ya',\n",
       " '▁moja',\n",
       " '▁kwa',\n",
       " '▁moja',\n",
       " '▁au',\n",
       " '▁isiyo',\n",
       " '▁ya',\n",
       " '▁moja',\n",
       " '▁kwa',\n",
       " '▁moja',\n",
       " '.',\n",
       " '<0x0A>',\n",
       " '<0x0A>',\n",
       " '##',\n",
       " '#',\n",
       " '▁In',\n",
       " 'p',\n",
       " 'ut',\n",
       " ':',\n",
       " '<0x0A>',\n",
       " 'habari',\n",
       " '▁tunaomba',\n",
       " '▁r',\n",
       " 'adh',\n",
       " 'i',\n",
       " '▁kwa',\n",
       " '▁us',\n",
       " 'umb',\n",
       " 'uf',\n",
       " 'u',\n",
       " '▁unao',\n",
       " 'j',\n",
       " 'ito',\n",
       " 'keza',\n",
       " '▁taf',\n",
       " 'adh',\n",
       " 'ali',\n",
       " '▁tu',\n",
       " 'andi',\n",
       " 'kie',\n",
       " '▁mit',\n",
       " 'a',\n",
       " '▁namba',\n",
       " '▁yako',\n",
       " '▁kw',\n",
       " 'en',\n",
       " 'ye',\n",
       " '▁d',\n",
       " 'm',\n",
       " '▁kwa',\n",
       " '▁m',\n",
       " 'sa',\n",
       " 'ada',\n",
       " '▁zaidi',\n",
       " '▁p',\n",
       " 'j',\n",
       " '<0x0A>',\n",
       " '<0x0A>',\n",
       " '##',\n",
       " '#',\n",
       " '▁R',\n",
       " 'es',\n",
       " 'p',\n",
       " 'on',\n",
       " 'se',\n",
       " ':',\n",
       " '<0x0A>',\n",
       " 'Wa',\n",
       " 'stani']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_new.tokenize(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
