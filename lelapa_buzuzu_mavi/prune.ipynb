{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prune InkubaLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.26.3\n",
      "matplotlib version: 3.7.3\n",
      "torch version: 2.6.0\n",
      "transformers version: 4.49.0\n",
      "accelerate version: 1.5.2\n",
      "sentencepiece version: 0.2.0\n",
      "protobuf version: 4.25.6\n",
      "sacrebleu version: 2.0.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "    \"numpy\",\n",
    "    \"matplotlib\",\n",
    "    \"torch\",\n",
    "    \"transformers\",\n",
    "    \"accelerate\",\n",
    "    \"sentencepiece\",\n",
    "    \"protobuf\",\n",
    "    \"sacrebleu\"\n",
    "]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "basedir = Path(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = {\n",
    "    \"50M\": {\n",
    "        \"save_model_weights_path\": basedir/\"pruned_model_init.pth\",\n",
    "        # if `custom_tokenizer_path` is not specified then it will use the default tokenizer from the model\n",
    "        \"custom_tokenizer_path\": basedir/\"tokenizer\",\n",
    "        \"reduce_dim_ratio\": 2,\n",
    "    },\n",
    "    \"100M\": {\n",
    "        \"save_model_weights_path\": basedir/\"pruned_model_60k_init.pth\",\n",
    "        \"reduce_dim_ratio\": 2,\n",
    "    },\n",
    "    \"40M\": {\n",
    "        \"save_model_weights_path\": basedir/\"pruned_model_6layer_init.pth\",\n",
    "        \"custom_tokenizer_path\": basedir/\"tokenizer\",\n",
    "        \"reduce_dim_ratio\": 2,\n",
    "        \"num_hidden_layers\": 6,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"50M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = \"lelapa/InkubaLM-0.4B\"\n",
    "custom_config = model_configs[model_type]\n",
    "save_path = custom_config[\"save_model_weights_path\"]\n",
    "tokenizer_path = custom_config.get(\"custom_tokenizer_path\", base_model_name)\n",
    "reduce_dim_ratio = custom_config.get(\"reduce_dim_ratio\", 1)\n",
    "num_hidden_layers = custom_config.get(\"num_hidden_layers\", 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "if custom_config is not None:\n",
    "    tokenizer_path = custom_config.get(\"custom_tokenizer_path\", base_model_name)\n",
    "else:\n",
    "    tokenizer_path = base_model_name\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.11/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "2025-04-09 15:37:04.586882: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-09 15:37:04.626513: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-09 15:37:04.626566: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-09 15:37:04.627777: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-09 15:37:04.635292: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-09 15:37:05.722377: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "VulavulaLlamaForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from collections import OrderedDict\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_name = \"lelapa/InkubaLM-0.4B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    return_dict=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float32,\n",
    ")\n",
    "\n",
    "# Remap the tokenizer weights. For example, if the token for \"hello\" is 1234 in the\n",
    "# old tokenizer and 5678 in the new tokenizer, we need to remap the weights accordingly.\n",
    "# The resulting model will have the same weights as the original model, the only difference\n",
    "# is that weights for tokens that are not in the new tokenizer are removed from the \n",
    "# embedding layers.\n",
    "old_weights = deepcopy(model.state_dict())\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "vocab = [tokenizer.convert_ids_to_tokens(i) for i in range(tokenizer.vocab_size)]\n",
    "keep = [old_tokenizer.convert_tokens_to_ids(t) for t in vocab]\n",
    "\n",
    "new_weights = OrderedDict()\n",
    "for k, v in old_weights.items():\n",
    "    if k in (\"model.embed_tokens.weight\", \"lm_head.weight\"):\n",
    "        new_weights[k] = v[keep].clone()\n",
    "    else:\n",
    "        new_weights[k] = v.clone()\n",
    "\n",
    "config = deepcopy(model.config)\n",
    "config.vocab_size = tokenizer.vocab_size\n",
    "model = AutoModelForCausalLM.from_config(config).to(torch.float32).to(device)\n",
    "model.load_state_dict(new_weights)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size of the model with the modified tokenizer, but before any dimension or layer pruning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory (GB): 0.76\n",
      "num params:  201,885,696\n"
     ]
    }
   ],
   "source": [
    "print(f\"memory (GB): {model.get_memory_footprint() / 1024**3:.2f}\")\n",
    "print(f\"num params:  {model.num_parameters():,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the number of parameters per layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight                          torch.Size([8064, 2048])\n",
      "model.layers.0.self_attn.q_proj.weight             torch.Size([2048, 2048])\n",
      "model.layers.0.self_attn.k_proj.weight             torch.Size([2048, 2048])\n",
      "model.layers.0.self_attn.v_proj.weight             torch.Size([2048, 2048])\n",
      "model.layers.0.self_attn.o_proj.weight             torch.Size([2048, 2048])\n",
      "model.layers.0.mlp.gate_proj.weight                torch.Size([5632, 2048])\n",
      "model.layers.0.mlp.down_proj.weight                torch.Size([2048, 5632])\n",
      "model.layers.0.mlp.up_proj.weight                  torch.Size([5632, 2048])\n",
      "model.layers.0.input_layernorm.weight              torch.Size([2048])\n",
      "model.layers.0.post_attention_layernorm.weight     torch.Size([2048])\n",
      "model.layers.1.self_attn.q_proj.weight             torch.Size([2048, 2048])\n",
      "model.layers.1.self_attn.k_proj.weight             torch.Size([2048, 2048])\n",
      "model.layers.1.self_attn.v_proj.weight             torch.Size([2048, 2048])\n",
      "model.layers.1.self_attn.o_proj.weight             torch.Size([2048, 2048])\n",
      "model.layers.1.input_layernorm.weight              torch.Size([2048])\n",
      "model.layers.1.post_attention_layernorm.weight     torch.Size([2048])\n",
      "model.layers.2.self_attn.q_proj.weight             torch.Size([2048, 2048])\n",
      "model.layers.2.self_attn.k_proj.weight             torch.Size([2048, 2048])\n",
      "model.layers.2.self_attn.v_proj.weight             torch.Size([2048, 2048])\n",
      "model.layers.2.self_attn.o_proj.weight             torch.Size([2048, 2048])\n",
      "model.layers.2.input_layernorm.weight              torch.Size([2048])\n",
      "model.layers.2.post_attention_layernorm.weight     torch.Size([2048])\n",
      "model.layers.3.self_attn.q_proj.weight             torch.Size([2048, 2048])\n",
      "model.layers.3.self_attn.k_proj.weight             torch.Size([2048, 2048])\n",
      "model.layers.3.self_attn.v_proj.weight             torch.Size([2048, 2048])\n",
      "model.layers.3.self_attn.o_proj.weight             torch.Size([2048, 2048])\n",
      "model.layers.3.input_layernorm.weight              torch.Size([2048])\n",
      "model.layers.3.post_attention_layernorm.weight     torch.Size([2048])\n",
      "model.layers.4.self_attn.q_proj.weight             torch.Size([2048, 2048])\n",
      "model.layers.4.self_attn.k_proj.weight             torch.Size([2048, 2048])\n",
      "model.layers.4.self_attn.v_proj.weight             torch.Size([2048, 2048])\n",
      "model.layers.4.self_attn.o_proj.weight             torch.Size([2048, 2048])\n",
      "model.layers.4.input_layernorm.weight              torch.Size([2048])\n",
      "model.layers.4.post_attention_layernorm.weight     torch.Size([2048])\n",
      "model.layers.5.self_attn.q_proj.weight             torch.Size([2048, 2048])\n",
      "model.layers.5.self_attn.k_proj.weight             torch.Size([2048, 2048])\n",
      "model.layers.5.self_attn.v_proj.weight             torch.Size([2048, 2048])\n",
      "model.layers.5.self_attn.o_proj.weight             torch.Size([2048, 2048])\n",
      "model.layers.5.input_layernorm.weight              torch.Size([2048])\n",
      "model.layers.5.post_attention_layernorm.weight     torch.Size([2048])\n",
      "model.layers.6.self_attn.q_proj.weight             torch.Size([2048, 2048])\n",
      "model.layers.6.self_attn.k_proj.weight             torch.Size([2048, 2048])\n",
      "model.layers.6.self_attn.v_proj.weight             torch.Size([2048, 2048])\n",
      "model.layers.6.self_attn.o_proj.weight             torch.Size([2048, 2048])\n",
      "model.layers.6.input_layernorm.weight              torch.Size([2048])\n",
      "model.layers.6.post_attention_layernorm.weight     torch.Size([2048])\n",
      "model.layers.7.self_attn.q_proj.weight             torch.Size([2048, 2048])\n",
      "model.layers.7.self_attn.k_proj.weight             torch.Size([2048, 2048])\n",
      "model.layers.7.self_attn.v_proj.weight             torch.Size([2048, 2048])\n",
      "model.layers.7.self_attn.o_proj.weight             torch.Size([2048, 2048])\n",
      "model.layers.7.input_layernorm.weight              torch.Size([2048])\n",
      "model.layers.7.post_attention_layernorm.weight     torch.Size([2048])\n",
      "model.norm.weight                                  torch.Size([2048])\n",
      "lm_head.weight                                     torch.Size([8064, 2048])\n"
     ]
    }
   ],
   "source": [
    "for n, p in model.named_parameters():\n",
    "    print(f\"{n :<50}\", p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the model config by reducing dimensions and/or reducing the number of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"_name_or_path\": \"lelapa/InkubaLM-0.4B\",\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"auto_map\": {\n",
       "    \"AutoModelForCausalLM\": \"lelapa/InkubaLM-0.4B--vulavulaslm.VulavulaLlamaForCausalLM\"\n",
       "  },\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"head_dim\": 64,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 2048,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 5632,\n",
       "  \"max_position_embeddings\": 2048,\n",
       "  \"mlp_bias\": false,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 8,\n",
       "  \"num_key_value_heads\": 32,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.49.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 8064\n",
       "}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"_name_or_path\": \"lelapa/InkubaLM-0.4B\",\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"auto_map\": {\n",
       "    \"AutoModelForCausalLM\": \"lelapa/InkubaLM-0.4B--vulavulaslm.VulavulaLlamaForCausalLM\"\n",
       "  },\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"head_dim\": 64,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 1024,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 2816,\n",
       "  \"max_position_embeddings\": 1024,\n",
       "  \"mlp_bias\": false,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 8,\n",
       "  \"num_key_value_heads\": 32,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.49.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 8064\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_prn = deepcopy(model.config)\n",
    "config_prn.hidden_size = config.hidden_size // reduce_dim_ratio\n",
    "config_prn.intermediate_size = config.intermediate_size // reduce_dim_ratio\n",
    "config_prn.max_position_embeddings = config.max_position_embeddings // reduce_dim_ratio\n",
    "config_prn.num_hidden_layers = num_hidden_layers\n",
    "config_prn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the pruned model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory (GB): 0.22\n",
      "num params:  58,737,664\n"
     ]
    }
   ],
   "source": [
    "model_prn = AutoModelForCausalLM.from_config(config_prn).to(device)\n",
    "model_prn.eval();\n",
    "print(f\"memory (GB): {model_prn.get_memory_footprint() / 1024**3:.2f}\")\n",
    "print(f\"num params:  {model_prn.num_parameters():,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the model can be further reduced by setting `tie_word_embeddings = True` in the config. This changes the model so that the token embeddings in the first and last layer of the model are shared. We do that in the `finetune` notebooks.\n",
    " - For the `50M` model the number of parameters is reduced by a further 8064*1024 = 8,257,536\n",
    " - For the `40M` model the number of parameters is reduced by a further 8064*1024 = 8,257,536\n",
    " - For the `100M` model the number of parameters is reduced by a further 61788*1024 = 63,270912 parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the pruned model to the original layer-by-layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight                          torch.Size([8064, 2048])  ->  torch.Size([8064, 1024])\n",
      "model.layers.0.self_attn.q_proj.weight             torch.Size([2048, 2048])  ->  torch.Size([1024, 1024])\n",
      "model.layers.0.self_attn.k_proj.weight             torch.Size([2048, 2048])  ->  torch.Size([1024, 1024])\n",
      "model.layers.0.self_attn.v_proj.weight             torch.Size([2048, 2048])  ->  torch.Size([1024, 1024])\n",
      "model.layers.0.self_attn.o_proj.weight             torch.Size([2048, 2048])  ->  torch.Size([1024, 1024])\n",
      "model.layers.0.mlp.gate_proj.weight                torch.Size([5632, 2048])  ->  torch.Size([2816, 1024])\n",
      "model.layers.0.mlp.down_proj.weight                torch.Size([2048, 5632])  ->  torch.Size([1024, 2816])\n",
      "model.layers.0.mlp.up_proj.weight                  torch.Size([5632, 2048])  ->  torch.Size([2816, 1024])\n",
      "model.layers.0.input_layernorm.weight              torch.Size([2048])        ->  torch.Size([1024])\n",
      "model.layers.0.post_attention_layernorm.weight     torch.Size([2048])        ->  torch.Size([1024])\n",
      "model.layers.1.self_attn.q_proj.weight             torch.Size([2048, 2048])  ->  torch.Size([1024, 1024])\n",
      "model.layers.1.self_attn.k_proj.weight             torch.Size([2048, 2048])  ->  torch.Size([1024, 1024])\n",
      "model.layers.1.self_attn.v_proj.weight             torch.Size([2048, 2048])  ->  torch.Size([1024, 1024])\n",
      "model.layers.1.self_attn.o_proj.weight             torch.Size([2048, 2048])  ->  torch.Size([1024, 1024])\n",
      "model.layers.1.input_layernorm.weight              torch.Size([2048])        ->  torch.Size([1024])\n",
      "model.layers.1.post_attention_layernorm.weight     torch.Size([2048])        ->  torch.Size([1024])\n",
      "model.layers.2.self_attn.q_proj.weight             torch.Size([2048, 2048])  ->  torch.Size([1024, 1024])\n",
      "model.layers.2.self_attn.k_proj.weight             torch.Size([2048, 2048])  ->  torch.Size([1024, 1024])\n",
      "model.layers.2.self_attn.v_proj.weight             torch.Size([2048, 2048])  ->  torch.Size([1024, 1024])\n",
      "model.layers.2.self_attn.o_proj.weight             torch.Size([2048, 2048])  ->  torch.Size([1024, 1024])\n",
      "model.layers.2.input_layernorm.weight              torch.Size([2048])        ->  torch.Size([1024])\n",
      "model.layers.2.post_attention_layernorm.weight     torch.Size([2048])        ->  torch.Size([1024])\n",
      "model.layers.3.self_attn.q_proj.weight             torch.Size([2048, 2048])  ->  torch.Size([1024, 1024])\n",
      "model.layers.3.self_attn.k_proj.weight             torch.Size([2048, 2048])  ->  torch.Size([1024, 1024])\n",
      "model.layers.3.self_attn.v_proj.weight             torch.Size([2048, 2048])  ->  torch.Size([1024, 1024])\n",
      "model.layers.3.self_attn.o_proj.weight             torch.Size([2048, 2048])  ->  torch.Size([1024, 1024])\n",
      "model.layers.3.input_layernorm.weight              torch.Size([2048])        ->  torch.Size([1024])\n",
      "model.layers.3.post_attention_layernorm.weight     torch.Size([2048])        ->  torch.Size([1024])\n",
      "model.layers.4.self_attn.q_proj.weight             torch.Size([2048, 2048])  ->  torch.Size([1024, 1024])\n",
      "model.layers.4.self_attn.k_proj.weight             torch.Size([2048, 2048])  ->  torch.Size([1024, 1024])\n",
      "model.layers.4.self_attn.v_proj.weight             torch.Size([2048, 2048])  ->  torch.Size([1024, 1024])\n",
      "model.layers.4.self_attn.o_proj.weight             torch.Size([2048, 2048])  ->  torch.Size([1024, 1024])\n",
      "model.layers.4.input_layernorm.weight              torch.Size([2048])        ->  torch.Size([1024])\n",
      "model.layers.4.post_attention_layernorm.weight     torch.Size([2048])        ->  torch.Size([1024])\n",
      "model.layers.5.self_attn.q_proj.weight             torch.Size([2048, 2048])  ->  torch.Size([1024, 1024])\n",
      "model.layers.5.self_attn.k_proj.weight             torch.Size([2048, 2048])  ->  torch.Size([1024, 1024])\n",
      "model.layers.5.self_attn.v_proj.weight             torch.Size([2048, 2048])  ->  torch.Size([1024, 1024])\n",
      "model.layers.5.self_attn.o_proj.weight             torch.Size([2048, 2048])  ->  torch.Size([1024, 1024])\n",
      "model.layers.5.input_layernorm.weight              torch.Size([2048])        ->  torch.Size([1024])\n",
      "model.layers.5.post_attention_layernorm.weight     torch.Size([2048])        ->  torch.Size([1024])\n",
      "model.layers.6.self_attn.q_proj.weight             torch.Size([2048, 2048])  ->  torch.Size([1024, 1024])\n",
      "model.layers.6.self_attn.k_proj.weight             torch.Size([2048, 2048])  ->  torch.Size([1024, 1024])\n",
      "model.layers.6.self_attn.v_proj.weight             torch.Size([2048, 2048])  ->  torch.Size([1024, 1024])\n",
      "model.layers.6.self_attn.o_proj.weight             torch.Size([2048, 2048])  ->  torch.Size([1024, 1024])\n",
      "model.layers.6.input_layernorm.weight              torch.Size([2048])        ->  torch.Size([1024])\n",
      "model.layers.6.post_attention_layernorm.weight     torch.Size([2048])        ->  torch.Size([1024])\n",
      "model.layers.7.self_attn.q_proj.weight             torch.Size([2048, 2048])  ->  torch.Size([1024, 1024])\n",
      "model.layers.7.self_attn.k_proj.weight             torch.Size([2048, 2048])  ->  torch.Size([1024, 1024])\n",
      "model.layers.7.self_attn.v_proj.weight             torch.Size([2048, 2048])  ->  torch.Size([1024, 1024])\n",
      "model.layers.7.self_attn.o_proj.weight             torch.Size([2048, 2048])  ->  torch.Size([1024, 1024])\n",
      "model.layers.7.input_layernorm.weight              torch.Size([2048])        ->  torch.Size([1024])\n",
      "model.layers.7.post_attention_layernorm.weight     torch.Size([2048])        ->  torch.Size([1024])\n",
      "model.norm.weight                                  torch.Size([2048])        ->  torch.Size([1024])\n",
      "lm_head.weight                                     torch.Size([8064, 2048])  ->  torch.Size([8064, 1024])\n"
     ]
    }
   ],
   "source": [
    "for n, p in model.named_parameters():\n",
    "    try:\n",
    "        print(f\"{n :<50} {str(p.shape) :<25} ->  {model_prn.get_parameter(n).shape}\")\n",
    "    except AttributeError:\n",
    "        print(f\"{n :<50} {str(p.shape) :<25} ->  Parameter not in pruned model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to account for shared parameters when mapping weights from the original model to the pruned version. The function below identifies all shared weights in InkubaLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_shared_params(model):\n",
    "    param_names = set([n for n, _ in model.named_parameters()])\n",
    "    weight_names = list(model.state_dict().keys())\n",
    "    shared_params = []\n",
    "    shared_params_names = []\n",
    "    for n1 in weight_names:\n",
    "        # Check if the weight is a nn.Parameter object\n",
    "        try:\n",
    "            _ = model.get_parameter(n1)\n",
    "        except AttributeError:\n",
    "            continue\n",
    "        # Check if n1 is already in the list of shared params\n",
    "        for i, p in enumerate(shared_params):\n",
    "            if model.get_parameter(n1).data_ptr() == p.data_ptr():\n",
    "                shared_params_names[i].append(n1)\n",
    "                break\n",
    "        else:\n",
    "            shared_params.append(model.get_parameter(n1))\n",
    "            shared_params_names.append([n1])\n",
    "    shared_params_names = [group for group in shared_params_names if len(group) > 1]\n",
    "\n",
    "    # create dict of shared params\n",
    "    shared_params_dict = {}\n",
    "    for group in shared_params_names:\n",
    "        name = set(group)&param_names\n",
    "        assert len(name) == 1\n",
    "        shared_params_dict[name.pop()] = group\n",
    "    return shared_params_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model.layers.0.mlp.gate_proj.weight': ['model.layers.0.mlp.gate_proj.weight',\n",
       "  'model.layers.1.mlp.gate_proj.weight',\n",
       "  'model.layers.2.mlp.gate_proj.weight',\n",
       "  'model.layers.3.mlp.gate_proj.weight',\n",
       "  'model.layers.4.mlp.gate_proj.weight',\n",
       "  'model.layers.5.mlp.gate_proj.weight',\n",
       "  'model.layers.6.mlp.gate_proj.weight',\n",
       "  'model.layers.7.mlp.gate_proj.weight'],\n",
       " 'model.layers.0.mlp.down_proj.weight': ['model.layers.0.mlp.down_proj.weight',\n",
       "  'model.layers.1.mlp.down_proj.weight',\n",
       "  'model.layers.2.mlp.down_proj.weight',\n",
       "  'model.layers.3.mlp.down_proj.weight',\n",
       "  'model.layers.4.mlp.down_proj.weight',\n",
       "  'model.layers.5.mlp.down_proj.weight',\n",
       "  'model.layers.6.mlp.down_proj.weight',\n",
       "  'model.layers.7.mlp.down_proj.weight'],\n",
       " 'model.layers.0.mlp.up_proj.weight': ['model.layers.0.mlp.up_proj.weight',\n",
       "  'model.layers.1.mlp.up_proj.weight',\n",
       "  'model.layers.2.mlp.up_proj.weight',\n",
       "  'model.layers.3.mlp.up_proj.weight',\n",
       "  'model.layers.4.mlp.up_proj.weight',\n",
       "  'model.layers.5.mlp.up_proj.weight',\n",
       "  'model.layers.6.mlp.up_proj.weight',\n",
       "  'model.layers.7.mlp.up_proj.weight']}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared_params_names = _get_shared_params(model)\n",
    "shared_params_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model.layers.0.mlp.gate_proj.weight': 'model.layers.0.mlp.gate_proj.weight',\n",
       " 'model.layers.1.mlp.gate_proj.weight': 'model.layers.0.mlp.gate_proj.weight',\n",
       " 'model.layers.2.mlp.gate_proj.weight': 'model.layers.0.mlp.gate_proj.weight',\n",
       " 'model.layers.3.mlp.gate_proj.weight': 'model.layers.0.mlp.gate_proj.weight',\n",
       " 'model.layers.4.mlp.gate_proj.weight': 'model.layers.0.mlp.gate_proj.weight',\n",
       " 'model.layers.5.mlp.gate_proj.weight': 'model.layers.0.mlp.gate_proj.weight',\n",
       " 'model.layers.6.mlp.gate_proj.weight': 'model.layers.0.mlp.gate_proj.weight',\n",
       " 'model.layers.7.mlp.gate_proj.weight': 'model.layers.0.mlp.gate_proj.weight',\n",
       " 'model.layers.0.mlp.down_proj.weight': 'model.layers.0.mlp.down_proj.weight',\n",
       " 'model.layers.1.mlp.down_proj.weight': 'model.layers.0.mlp.down_proj.weight',\n",
       " 'model.layers.2.mlp.down_proj.weight': 'model.layers.0.mlp.down_proj.weight',\n",
       " 'model.layers.3.mlp.down_proj.weight': 'model.layers.0.mlp.down_proj.weight',\n",
       " 'model.layers.4.mlp.down_proj.weight': 'model.layers.0.mlp.down_proj.weight',\n",
       " 'model.layers.5.mlp.down_proj.weight': 'model.layers.0.mlp.down_proj.weight',\n",
       " 'model.layers.6.mlp.down_proj.weight': 'model.layers.0.mlp.down_proj.weight',\n",
       " 'model.layers.7.mlp.down_proj.weight': 'model.layers.0.mlp.down_proj.weight',\n",
       " 'model.layers.0.mlp.up_proj.weight': 'model.layers.0.mlp.up_proj.weight',\n",
       " 'model.layers.1.mlp.up_proj.weight': 'model.layers.0.mlp.up_proj.weight',\n",
       " 'model.layers.2.mlp.up_proj.weight': 'model.layers.0.mlp.up_proj.weight',\n",
       " 'model.layers.3.mlp.up_proj.weight': 'model.layers.0.mlp.up_proj.weight',\n",
       " 'model.layers.4.mlp.up_proj.weight': 'model.layers.0.mlp.up_proj.weight',\n",
       " 'model.layers.5.mlp.up_proj.weight': 'model.layers.0.mlp.up_proj.weight',\n",
       " 'model.layers.6.mlp.up_proj.weight': 'model.layers.0.mlp.up_proj.weight',\n",
       " 'model.layers.7.mlp.up_proj.weight': 'model.layers.0.mlp.up_proj.weight'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reverse the dict\n",
    "lup_shared_params = {}\n",
    "for k, v in shared_params_names.items():\n",
    "    for n in v:\n",
    "        lup_shared_params[n] = k\n",
    "lup_shared_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's do the pruning!\n",
    "\n",
    "We reduce the number of dimensions of a layer by essentially applying dropout to the layer. In other words, we randomly remove a percentage of neurons in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dim(x, dim, out_sz):\n",
    "    rank = len(x.shape)\n",
    "    assert 0 <= dim < rank, \"`dim` must be in the range [0, rank of x)\"\n",
    "    keep = torch.rand_like(x).argsort(dim=dim)\n",
    "    if dim == 0:\n",
    "        keep = keep[:out_sz]\n",
    "    else:\n",
    "        keep = keep[:, :out_sz]\n",
    "    keep.shape, keep\n",
    "\n",
    "    w = torch.zeros(keep.shape, dtype=x.dtype, device=x.device)\n",
    "    if dim == 0:\n",
    "        if rank == 1:\n",
    "            keep = keep.sort().values\n",
    "            w = x[keep]\n",
    "        else:\n",
    "            for i in range(x.shape[1]):\n",
    "                keep_i = keep[:, i].sort().values\n",
    "                w[:, i] = x[keep_i, i]\n",
    "    else:\n",
    "        for i in range(x.shape[0]):\n",
    "            keep_i = keep[i].sort().values\n",
    "            w[i] = x[i, keep_i]\n",
    "    return w.clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check with a toy example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "v = torch.randn(1024, 1024)\n",
    "# v = torch.randn(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 1024])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 256])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = reduce_dim(v, 1, 256)\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 256])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = reduce_dim(w, 0, 256)\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with InkubaLM's original weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the keys of the newly initialized pruned model to make sure we're only loading weights from the original model that are also in the pruned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_keys_prn = set(model_prn.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f8dac119890>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We go through the model layer-by-layer. If a layer was pruned entirely, we skip it. Otherwise, we reduce the dimension of the layer by `reduce_dim_ratio`. We keep track of shared parameters to that we don't create the parameters for those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_prn = OrderedDict()\n",
    "for k, v in weights.items():\n",
    "    # check if param is pruned\n",
    "    if k not in weights_keys_prn:\n",
    "        continue\n",
    "\n",
    "    # check if param is shared\n",
    "    shared_param = k in lup_shared_params\n",
    "    if shared_param:\n",
    "        shared_name = lup_shared_params[k]\n",
    "        if shared_name in weights_prn:\n",
    "            weights_prn[k] = weights_prn[shared_name]\n",
    "            continue\n",
    "        k = shared_name\n",
    "\n",
    "    if k in (\"model.embed_tokens.weight\", \"lm_head.weight\"):\n",
    "        weights_prn[k] = reduce_dim(v, 1, config_prn.hidden_size)\n",
    "    else:\n",
    "        rank = len(v.shape)\n",
    "        out_sz = v.shape[0] // reduce_dim_ratio\n",
    "        w = reduce_dim(v, 0, out_sz)\n",
    "        if rank == 2:\n",
    "            out_sz = v.shape[1] // reduce_dim_ratio\n",
    "            w = reduce_dim(w, 1, out_sz)\n",
    "        weights_prn[k] = w\n",
    "\n",
    "model_prn.load_state_dict(weights_prn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the output of the pruned model without fine-tuning. Since we pruned quite aggressively, we expect that we'll need some finetuning before the model will generate coherent responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> This is a test kasiativeative kasiative kasi kasi kasi kasi kasi kasi kasi kasi kasi kasijidjidjidjidjid'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer.encode(\"This is a test\", return_tensors=\"pt\").to(device)\n",
    "tokenizer.decode(model_prn.generate(inputs)[0].cpu().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the pruned model's weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(weights_prn, save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
