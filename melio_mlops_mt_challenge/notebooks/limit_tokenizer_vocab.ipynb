{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9ce1b17-d0a6-4e12-9192-055b427dccff",
   "metadata": {},
   "source": [
    "# Limit tokenizer vocabulary\n",
    "\n",
    "This Notebook goes through the process of limiting a pretrained NllbTokenizer's vocabulary step-by-step.\n",
    "\n",
    "**Process Overview:**\n",
    " 1. Loads the pretrained tokenizer from `model_id`.\n",
    " 2. Counts the frequency of each token appearing in the dataset.\n",
    " 3. Retains tokens that meet the frequency threshold defined by `min_token_freq`.\n",
    " 4. Checks for frequently occurring tokens that are not in the tokenizer's vocabulary and adds them.\n",
    " 5. Saves the modified tokenizer's configuration, vocabulary, and sentencepiece model to `save_dir`.\n",
    " 6. Ensures that the final tokenizer can be loaded and used with the `transformers` library.\n",
    "\n",
    "**Steps in Detail:**\n",
    " 1. The original tokenizer is loaded and applied to the dataset to generate token IDs.\n",
    " 2. Token frequencies are computed for both source (`dyu_Latn`) and target (`fra_Latn`) languages.\n",
    " 3. Tokens are filtered based on the minimum frequency and unknown tokens are flagged.\n",
    " 4. The tokenizer's configuration files are modified to reflect the new vocabulary and saved.\n",
    " 5. The sentencepiece model is updated by removing unused tokens and adding new tokens.\n",
    " 6. The new tokenizer is tested to ensure compatibility with `transformers` and is saved.\n",
    "\n",
    "**Notes:**\n",
    " - Special tokens are preserved and non-relevant special tokens are removed.\n",
    " - The tokenizer's vocabulary is updated to include high-frequency tokens that were previously unknown.\n",
    " - The function handles both updating the tokenizer's configuration files and the sentencepiece model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206dbbb4-415a-478b-a4b7-676827fffb7d",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a95287b-89c7-42dc-b058-c59952a728aa",
   "metadata": {},
   "source": [
    "Restart the kernel after you have installed packages with `pip install` in the Notebook cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98f3c737",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gradient 2.0.6 requires attrs<=19, but you have attrs 23.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q -U sentencepiece transformers huggingface_hub datasets sacrebleu lxml sentence-transformers accelerate fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5017403f-49a4-453f-ab3c-a8f75bcd48c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BASE_MODEL_ID = \"facebook/nllb-200-distilled-600M\"\n",
    "MIN_TOKEN_FREQ = 1\n",
    "SAVE_DIR = \"tokenizers/tokenizer_freq1\"\n",
    "HFHUB_LOGIN = False\n",
    "LOAD_LOCAL_DATA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36d77565",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if HFHUB_LOGIN:\n",
    "    from huggingface_hub import notebook_login\n",
    "    notebook_login(new_session=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "802e43db-7d6e-4e64-98ec-d40ebbd53364",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizerFast, NllbTokenizer\n",
    "import sentencepiece as spm\n",
    "from sentencepiece import sentencepiece_model_pb2 as sp_model\n",
    "\n",
    "from data import load_from_json, save_data_locally, ds2df\n",
    "from utils import preproc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44130f50-13e6-4289-96f2-8a01fc4776dd",
   "metadata": {},
   "source": [
    "## Prepare tokenizer training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f1d446c-5543-4402-9e77-80cc2c7b014c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10929, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dyu</th>\n",
       "      <th>fr</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A bi ji min na</td>\n",
       "      <td>Il boit de l’eau.</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A le dalakolontɛ lon bɛ.</td>\n",
       "      <td>Il se plaint toujours.</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mun? Fɛn dɔ.</td>\n",
       "      <td>Quoi ? Quelque chose.</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O bɛ bi bɔra fo Gubeta.</td>\n",
       "      <td>Tous sortent excepté Gubetta.</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A ale lo bi da bugɔ la!</td>\n",
       "      <td>Ah ! c’est lui… il sonne…</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        dyu                             fr  split\n",
       "0            A bi ji min na              Il boit de l’eau.  train\n",
       "1  A le dalakolontɛ lon bɛ.         Il se plaint toujours.  train\n",
       "2              Mun? Fɛn dɔ.          Quoi ? Quelque chose.  train\n",
       "3   O bɛ bi bɔra fo Gubeta.  Tous sortent excepté Gubetta.  train\n",
       "4   A ale lo bi da bugɔ la!      Ah ! c’est lui… il sonne…  train"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if LOAD_LOCAL_DATA:\n",
    "    df = load_from_json(\n",
    "        train_files=\"data/dataset_train.json\",\n",
    "        valid_files=\"data/dataset_validation.json\",\n",
    "        test_files=\"data/dataset_test.json\",\n",
    "        return_format=\"df\"\n",
    "    )\n",
    "else:\n",
    "    ds = load_dataset(\"uvci/Koumankan_mt_dyu_fr\")\n",
    "    save_data_locally(ds, save_dir=\"./data\")\n",
    "    df = ds2df(ds)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f70879-e92a-41d4-8965-5cf2248ae6d2",
   "metadata": {},
   "source": [
    "## Load existing tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "039414e5-ee07-4107-bfb7-b1eae131ecbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer_old = NllbTokenizer.from_pretrained(BASE_MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32539763-d3fe-4632-9c87-86cabfc098f1",
   "metadata": {},
   "source": [
    "Get a count of how frequently each token appears in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cab1140d-da33-4ff9-9f3a-1507e23b41ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens used: 10802\n"
     ]
    }
   ],
   "source": [
    "token_counts = Counter()\n",
    "\n",
    "df_counts = df.copy()\n",
    "tokenizer_old.src_lang = \"dyu_Latn\"\n",
    "df_counts[\"tokens_dyu\"] = df_counts[\"dyu\"].apply(lambda s: tokenizer_old(preproc(s)).input_ids)\n",
    "tokenizer_old.src_lang = \"fra_Latn\"\n",
    "df_counts[\"tokens_fra\"] = df_counts[\"fr\"].apply(lambda s: tokenizer_old(preproc(s)).input_ids)\n",
    "df_counts[\"tokens\"] = df_counts[\"tokens_dyu\"] + df_counts[\"tokens_fra\"]\n",
    "\n",
    "for tokens in df_counts[\"tokens\"]:\n",
    "    token_counts.update(dict(Counter(tokens)))\n",
    "token_counts = dict(token_counts)\n",
    "\n",
    "token_ids = sorted(list(token_counts.keys()))\n",
    "print(\"Number of tokens used:\", len(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b9d595-e69d-4e4e-b5ee-5b3102568e77",
   "metadata": {},
   "source": [
    "Covert to a Pandas `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "995d6c92-c2f7-42ab-88af-0fd541c8be95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>21858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>256044</td>\n",
       "      <td>10929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>256057</td>\n",
       "      <td>10929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>4207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>99</td>\n",
       "      <td>3522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7401</th>\n",
       "      <td>209546</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7402</th>\n",
       "      <td>189625</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7403</th>\n",
       "      <td>239427</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7404</th>\n",
       "      <td>1083</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10801</th>\n",
       "      <td>39630</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10802 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        token  count\n",
       "6           2  21858\n",
       "0      256044  10929\n",
       "7      256057  10929\n",
       "1           9   4207\n",
       "119        99   3522\n",
       "...       ...    ...\n",
       "7401   209546      1\n",
       "7402   189625      1\n",
       "7403   239427      1\n",
       "7404     1083      1\n",
       "10801   39630      1\n",
       "\n",
       "[10802 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counts = pd.DataFrame({\"token\": token_counts.keys(), \"count\": token_counts.values()}).sort_values(by=\"count\", ascending=False)\n",
    "token_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913ce8dc-0fe9-43ad-82e0-3bf1a1c06c7b",
   "metadata": {},
   "source": [
    "Number of tokens with at least `MIN_TOKEN_FREQ` occurences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78909ba6-0786-4ba5-a733-05ec64e4dabb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10802"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(token_counts[\"count\"] >= MIN_TOKEN_FREQ).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c584d4d9-3271-49e3-a192-b54c5fa5b091",
   "metadata": {},
   "source": [
    "Let's limit the vocabulary to tokens with at least `MIN_TOKEN_FREQ` occurences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f53fa10-4161-4488-a69d-c3b45329e9db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens used: 10802\n"
     ]
    }
   ],
   "source": [
    "token_ids = sorted(token_counts[token_counts[\"count\"] >= MIN_TOKEN_FREQ][\"token\"].tolist())\n",
    "print(\"Number of tokens used:\", len(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c13f785-e07a-482b-af79-8247f99edf34",
   "metadata": {},
   "source": [
    "We also want to check if there are frequent tokens that are not in the tokenizer's vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa23a3dd-9c27-4334-b405-345287f8a291",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "token_counts = Counter()\n",
    "\n",
    "df_counts = df.copy()\n",
    "tokenizer_old.src_lang = \"dyu_Latn\"\n",
    "df_counts[\"tokens_dyu\"] = df_counts[\"dyu\"].apply(lambda s: tokenizer_old.tokenize(preproc(s)))\n",
    "tokenizer_old.src_lang = \"fra_Latn\"\n",
    "df_counts[\"tokens_fra\"] = df_counts[\"fr\"].apply(lambda s: tokenizer_old.tokenize(preproc(s)))\n",
    "df_counts[\"tokens\"] = df_counts[\"tokens_dyu\"] + df_counts[\"tokens_fra\"]\n",
    "\n",
    "for tokens in df_counts[\"tokens\"]:\n",
    "    token_counts.update(dict(Counter(tokens)))\n",
    "token_counts = dict(token_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f37f1279-1071-4ed3-9d85-5f803719d4c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>count</th>\n",
       "      <th>is_unk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>’</td>\n",
       "      <td>1625</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4262</th>\n",
       "      <td>—</td>\n",
       "      <td>26</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1821</th>\n",
       "      <td>»</td>\n",
       "      <td>15</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4022</th>\n",
       "      <td>«</td>\n",
       "      <td>14</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7072</th>\n",
       "      <td>“</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7073</th>\n",
       "      <td>”</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>–</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8961</th>\n",
       "      <td>—«</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     token  count  is_unk\n",
       "9        ’   1625    True\n",
       "4262     —     26    True\n",
       "1821     »     15    True\n",
       "4022     «     14    True\n",
       "7072     “      3    True\n",
       "7073     ”      3    True\n",
       "1489     –      2    True\n",
       "8961    —«      2    True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _is_unk(t):\n",
    "    return tokenizer_old.convert_tokens_to_ids(t) == tokenizer_old.unk_token_id\n",
    "\n",
    "token_counts = pd.DataFrame({\"token\": token_counts.keys(), \"count\": token_counts.values()}).sort_values(by=\"count\", ascending=False)\n",
    "token_counts[\"is_unk\"] = token_counts[\"token\"].apply(_is_unk)\n",
    "token_counts[token_counts[\"is_unk\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5196e53d-ead8-419c-ac6b-d1c9cbb36bc8",
   "metadata": {},
   "source": [
    "The top token occurs very frequently. We'll add that to the tokenizer vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "709d3c05-b556-42bd-bf2e-6b872ee4dc0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'’'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokens = set(token_counts[token_counts[\"is_unk\"]].iloc[0, 0])\n",
    "new_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ec58d0-2d00-432d-895f-b2a9ff7a407b",
   "metadata": {},
   "source": [
    "## Update the tokenizer's configuration files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c27888fb-cb1d-47fe-9179-10583cd1a711",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/tmp/tokenizer_config.json',\n",
       " '/tmp/special_tokens_map.json',\n",
       " '/tmp/sentencepiece.bpe.model',\n",
       " '/tmp/added_tokens.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_old.save_pretrained(\"/tmp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1b2df5-9149-4cf3-9c76-0c73e34842c9",
   "metadata": {},
   "source": [
    "Special tokens - keep only relevant language tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44ab09c0-997f-4ad5-8ec3-db2968e97cc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'additional_special_tokens': ['dyu_Latn', 'fra_Latn'],\n",
       " 'bos_token': {'content': '<s>',\n",
       "  'lstrip': False,\n",
       "  'normalized': False,\n",
       "  'rstrip': False,\n",
       "  'single_word': False},\n",
       " 'cls_token': {'content': '<s>',\n",
       "  'lstrip': False,\n",
       "  'normalized': False,\n",
       "  'rstrip': False,\n",
       "  'single_word': False},\n",
       " 'eos_token': {'content': '</s>',\n",
       "  'lstrip': False,\n",
       "  'normalized': False,\n",
       "  'rstrip': False,\n",
       "  'single_word': False},\n",
       " 'mask_token': {'content': '<mask>',\n",
       "  'lstrip': True,\n",
       "  'normalized': True,\n",
       "  'rstrip': False,\n",
       "  'single_word': False},\n",
       " 'pad_token': {'content': '<pad>',\n",
       "  'lstrip': False,\n",
       "  'normalized': False,\n",
       "  'rstrip': False,\n",
       "  'single_word': False},\n",
       " 'sep_token': {'content': '</s>',\n",
       "  'lstrip': False,\n",
       "  'normalized': False,\n",
       "  'rstrip': False,\n",
       "  'single_word': False},\n",
       " 'unk_token': {'content': '<unk>',\n",
       "  'lstrip': False,\n",
       "  'normalized': False,\n",
       "  'rstrip': False,\n",
       "  'single_word': False}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"/tmp/special_tokens_map.json\", \"r\") as f:\n",
    "    special_tokens_map = json.load(f)\n",
    "add_special_tokens = [\"dyu_Latn\", \"fra_Latn\"]\n",
    "add_special_tokens_remove = set([\n",
    "    t for t in special_tokens_map[\"additional_special_tokens\"] if t not in add_special_tokens\n",
    "])\n",
    "special_tokens_map[\"additional_special_tokens\"] = add_special_tokens\n",
    "special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8dcc152-1ada-4db3-bbfd-277cc70991cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<mask>': 256203, 'dyu_Latn': 256044, 'fra_Latn': 256057}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"/tmp/added_tokens.json\", \"r\") as f:\n",
    "    added_tokens = json.load(f)\n",
    "added_tokens = {k: v for k, v in added_tokens.items() if k not in add_special_tokens_remove}\n",
    "added_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ccc7438-99b3-4560-b2a9-68115bc780e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'added_tokens_decoder': {'0': {'content': '<s>',\n",
       "   'lstrip': False,\n",
       "   'normalized': False,\n",
       "   'rstrip': False,\n",
       "   'single_word': False,\n",
       "   'special': True},\n",
       "  '1': {'content': '<pad>',\n",
       "   'lstrip': False,\n",
       "   'normalized': False,\n",
       "   'rstrip': False,\n",
       "   'single_word': False,\n",
       "   'special': True},\n",
       "  '2': {'content': '</s>',\n",
       "   'lstrip': False,\n",
       "   'normalized': False,\n",
       "   'rstrip': False,\n",
       "   'single_word': False,\n",
       "   'special': True},\n",
       "  '3': {'content': '<unk>',\n",
       "   'lstrip': False,\n",
       "   'normalized': False,\n",
       "   'rstrip': False,\n",
       "   'single_word': False,\n",
       "   'special': True},\n",
       "  '256044': {'content': 'dyu_Latn',\n",
       "   'lstrip': False,\n",
       "   'normalized': False,\n",
       "   'rstrip': False,\n",
       "   'single_word': False,\n",
       "   'special': True},\n",
       "  '256057': {'content': 'fra_Latn',\n",
       "   'lstrip': False,\n",
       "   'normalized': False,\n",
       "   'rstrip': False,\n",
       "   'single_word': False,\n",
       "   'special': True},\n",
       "  '256203': {'content': '<mask>',\n",
       "   'lstrip': True,\n",
       "   'normalized': True,\n",
       "   'rstrip': False,\n",
       "   'single_word': False,\n",
       "   'special': True}},\n",
       " 'additional_special_tokens': ['dyu_Latn', 'fra_Latn'],\n",
       " 'bos_token': '<s>',\n",
       " 'clean_up_tokenization_spaces': True,\n",
       " 'cls_token': '<s>',\n",
       " 'eos_token': '</s>',\n",
       " 'legacy_behaviour': False,\n",
       " 'mask_token': '<mask>',\n",
       " 'model_max_length': 1024,\n",
       " 'pad_token': '<pad>',\n",
       " 'sep_token': '</s>',\n",
       " 'sp_model_kwargs': {},\n",
       " 'src_lang': 'fra_Latn',\n",
       " 'tgt_lang': None,\n",
       " 'tokenizer_class': 'NllbTokenizer',\n",
       " 'unk_token': '<unk>'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"/tmp/tokenizer_config.json\", \"r\") as f:\n",
    "    tokenizer_config = json.load(f)\n",
    "\n",
    "added_tokens_decoder = {}\n",
    "for k, v in tokenizer_config[\"added_tokens_decoder\"].items():\n",
    "    if v[\"content\"] not in add_special_tokens_remove:\n",
    "        added_tokens_decoder[k] = v\n",
    "tokenizer_config[\"added_tokens_decoder\"] = added_tokens_decoder\n",
    "\n",
    "tokenizer_config[\"additional_special_tokens\"] = add_special_tokens\n",
    "tokenizer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b123558-25b2-4500-91cd-f86073619072",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(SAVE_DIR):\n",
    "    os.makedirs(SAVE_DIR)\n",
    "new_tokenizer_dir = Path(SAVE_DIR)\n",
    "\n",
    "with open(new_tokenizer_dir/\"tokenizer_config.json\", \"w\") as f:\n",
    "    json.dump(tokenizer_config, f,  indent=4)\n",
    "\n",
    "with open(new_tokenizer_dir/\"special_tokens_map.json\", \"w\") as f:\n",
    "    json.dump(special_tokens_map, f,  indent=4)\n",
    "\n",
    "with open(new_tokenizer_dir/\"added_tokens.json\", \"w\") as f:\n",
    "    json.dump(added_tokens, f,  indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ff44f4-50e5-4f4f-9bbf-64cd2e8a8449",
   "metadata": {},
   "source": [
    "## Build new vocabulary\n",
    "\n",
    "The new vocabulary consists of:\n",
    "\n",
    " - Tokens present in the tokenizer's vocabulary and in our dataset\n",
    " - Tokens with high frequency in our dataset that are not in the tokenizer's vocab\n",
    " - The tokenizer's added and special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ef3eb4f-9487-45c1-9ae4-5a6e5d60d782",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10806"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_vocab = (\n",
    "    {tokenizer_old.convert_ids_to_tokens(i) for i in token_ids}\n",
    "    .union(\n",
    "        {v[\"content\"] for v in tokenizer_config[\"added_tokens_decoder\"].values()}\n",
    "    )\n",
    "    .union(\n",
    "        new_tokens\n",
    "    )\n",
    ")\n",
    "len(new_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69672a97-eccb-4ba1-b577-1989b3c788c5",
   "metadata": {},
   "source": [
    "## Update the `sentencepiece` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15c83c27-7272-4a74-804e-9ab08024fa6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4852054"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = sp_model.ModelProto()\n",
    "m.ParseFromString(open(\"/tmp/sentencepiece.bpe.model\", \"rb\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4dd5685-90df-4441-a66a-e85f7219f273",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(m.pieces)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd91a573-f7f7-4c20-9c3d-66d6a68b02f8",
   "metadata": {},
   "source": [
    "Iterate over `m.pieces` and keep only keep the tokens that are in the new vocab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4350b801-199d-4994-9eeb-2d4e8fb7e99c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seen = set()\n",
    "while True:\n",
    "    if m.pieces[0].piece in seen:\n",
    "        break\n",
    "    x = m.pieces.pop(0)\n",
    "    seen.add(x.piece)\n",
    "    if x.piece in new_vocab:\n",
    "        m.pieces.append(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c2addf-bd72-42fc-a3b0-b34ab95ad799",
   "metadata": {},
   "source": [
    "Add new tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a11c6968-29eb-469a-ac06-45c45873c73b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "add_tokens = new_vocab - seen\n",
    "\n",
    "for token in add_tokens:\n",
    "    new_token = sp_model.ModelProto().SentencePiece()\n",
    "    new_token.piece = token\n",
    "    new_token.score = 0\n",
    "    m.pieces.append(new_token)\n",
    "\n",
    "assert len(m.pieces) == len(new_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7280e546-f71a-409b-8b6d-e6752a670735",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piece: \"<unk>\"\n",
       "score: 0\n",
       "type: UNKNOWN"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.pieces[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54715c9a-1bbf-460d-8053-512f69b14692",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10806"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(m.pieces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d7a1b9b-4cb8-48b7-821a-022e9eae229f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(new_tokenizer_dir/'sentencepiece.bpe.model', 'wb') as f:\n",
    "    f.write(m.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a258434-13ef-48a2-85c3-8c48eb1d4e6d",
   "metadata": {},
   "source": [
    "Test loading the updated model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed9d7d5e-05f5-4005-8444-5933470b8a19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(str(new_tokenizer_dir/'sentencepiece.bpe.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ac73cef-6ae1-43e7-bba6-66f54a0712e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁th', 'is', '▁is', '▁a', '▁test']\n",
      "[162, 20, 170, 8, 2248]\n"
     ]
    }
   ],
   "source": [
    "print(sp.encode_as_pieces('this is a test'))\n",
    "print(sp.encode_as_ids('this is a test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962e2de5-b23a-48c5-a635-7810177b9ce6",
   "metadata": {},
   "source": [
    "Test loading the model with `transformers`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66f1f286-cf27-4391-a8f3-39ca0d1bda55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = NllbTokenizer.from_pretrained(new_tokenizer_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db3f11a0-ef28-4a93-b18e-11c928584a43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10807"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "58183057-93bd-4a15-994a-c578043bdea0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10806"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d55035-314c-4550-afed-21628b5bb3a2",
   "metadata": {},
   "source": [
    "## Update indices of `added_tokens`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecce248-c9d2-4d6a-8f86-6c0930589cdb",
   "metadata": {},
   "source": [
    "We need to update the indices in the config files for special tokens with indexes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ffbd3d2-76a5-4f0a-bf3f-a6c301f80595",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated index: 10802: dyu_Latn\n",
      "Updated index: 10804: fra_Latn\n",
      "Updated index: 10806: <mask>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dyu_Latn': 10802, 'fra_Latn': 10804, '<mask>': 10806}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "added_tokens_new = {}\n",
    "for i in range(tokenizer.vocab_size):\n",
    "    t = tokenizer.convert_ids_to_tokens(i)\n",
    "    if t in added_tokens:\n",
    "        print(f\"Updated index: {i}: {t}\")\n",
    "        added_tokens_new[t] = i\n",
    "added_tokens_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1cc1916-cbde-471d-a42a-69a1137eb475",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated index: 10802: dyu_Latn\n",
      "Updated index: 10804: fra_Latn\n",
      "Updated index: 10806: <mask>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'added_tokens_decoder': {'0': {'content': '<s>',\n",
       "   'lstrip': False,\n",
       "   'normalized': False,\n",
       "   'rstrip': False,\n",
       "   'single_word': False,\n",
       "   'special': True},\n",
       "  '1': {'content': '<pad>',\n",
       "   'lstrip': False,\n",
       "   'normalized': False,\n",
       "   'rstrip': False,\n",
       "   'single_word': False,\n",
       "   'special': True},\n",
       "  '2': {'content': '</s>',\n",
       "   'lstrip': False,\n",
       "   'normalized': False,\n",
       "   'rstrip': False,\n",
       "   'single_word': False,\n",
       "   'special': True},\n",
       "  '3': {'content': '<unk>',\n",
       "   'lstrip': False,\n",
       "   'normalized': False,\n",
       "   'rstrip': False,\n",
       "   'single_word': False,\n",
       "   'special': True},\n",
       "  '10802': {'content': 'dyu_Latn',\n",
       "   'lstrip': False,\n",
       "   'normalized': False,\n",
       "   'rstrip': False,\n",
       "   'single_word': False,\n",
       "   'special': True},\n",
       "  '10804': {'content': 'fra_Latn',\n",
       "   'lstrip': False,\n",
       "   'normalized': False,\n",
       "   'rstrip': False,\n",
       "   'single_word': False,\n",
       "   'special': True},\n",
       "  '10806': {'content': '<mask>',\n",
       "   'lstrip': True,\n",
       "   'normalized': True,\n",
       "   'rstrip': False,\n",
       "   'single_word': False,\n",
       "   'special': True}},\n",
       " 'additional_special_tokens': ['dyu_Latn', 'fra_Latn'],\n",
       " 'bos_token': '<s>',\n",
       " 'clean_up_tokenization_spaces': True,\n",
       " 'cls_token': '<s>',\n",
       " 'eos_token': '</s>',\n",
       " 'legacy_behaviour': False,\n",
       " 'mask_token': '<mask>',\n",
       " 'model_max_length': 1024,\n",
       " 'pad_token': '<pad>',\n",
       " 'sep_token': '</s>',\n",
       " 'sp_model_kwargs': {},\n",
       " 'src_lang': 'fra_Latn',\n",
       " 'tgt_lang': None,\n",
       " 'tokenizer_class': 'NllbTokenizer',\n",
       " 'unk_token': '<unk>'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "added_tokens_decoder_new = {}\n",
    "for k, v in added_tokens_decoder.items():\n",
    "    t = v[\"content\"]\n",
    "    i = int(k)\n",
    "    assert t == tokenizer.convert_ids_to_tokens(i)\n",
    "    if i >= tokenizer.vocab_size:\n",
    "        i = added_tokens_new[t]\n",
    "        print(f\"Updated index: {i}: {t}\")\n",
    "    added_tokens_decoder_new[str(i)] = v\n",
    "tokenizer_config[\"added_tokens_decoder\"] = added_tokens_decoder_new\n",
    "tokenizer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "77d80839-1d74-4204-8dcf-fc8355f7e721",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(new_tokenizer_dir/\"tokenizer_config.json\", \"w\") as f:\n",
    "    json.dump(tokenizer_config, f,  indent=4)\n",
    "\n",
    "with open(new_tokenizer_dir/\"added_tokens.json\", \"w\") as f:\n",
    "    json.dump(added_tokens_new, f,  indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f8faae-da77-412a-9431-374c377f3325",
   "metadata": {},
   "source": [
    "Test loading the model with `transformers`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b0a09a21-59ba-4eb4-8725-2abe12b52023",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = NllbTokenizer.from_pretrained(new_tokenizer_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7314f101-9c54-4470-a690-138cef60f73f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10807"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "100cd393-62ac-4e6d-a9e9-2a93c887f703",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁il', '▁boit', '▁de', '▁l', '’', 'eau']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-30 19:18:44.839954: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-30 19:18:44.840015: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-30 19:18:44.841124: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-30 19:18:44.848449: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-30 19:18:45.788311: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fra_Latn il boit de l’eau</s>\n"
     ]
    }
   ],
   "source": [
    "t = \"il boit de l’eau\"\n",
    "print(tokenizer.tokenize(t))\n",
    "print(tokenizer.decode(tokenizer.encode(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "97f1a811-ea1f-46b2-9f15-512d9c16714f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10802"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(\"dyu_Latn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b0ba443a-0380-4368-9e9a-f1576a11ce82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10804"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(\"fra_Latn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "668c1aa2-58eb-4730-bdb2-efed524c0fe0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10806"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(\"<mask>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8e1511-9f4a-47d0-9422-0b03df8898a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
