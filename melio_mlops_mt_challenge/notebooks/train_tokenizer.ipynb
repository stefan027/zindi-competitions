{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a11055a-1083-4c0d-bd79-1881119a4aba",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train new tokenizer\n",
    "\n",
    "This Notebook goes through the process of training a new NllbTokenizer on the Dyula to French dataset.\n",
    "\n",
    "**Process Overview:**\n",
    " 1. Create training data for the tokenizer from the dataset and save it to a text file.\n",
    " 2. Load and save the existing tokenizer's configuration files from the specified model.\n",
    " 3. Train a new tokenizer using the `SentencePieceBPETokenizer` class on the saved text data.\n",
    " 4. Convert the trained tokenizer to a `PreTrainedTokenizerFast` object.\n",
    " 5. Update the tokenizer configuration files with the new vocabulary and special tokens.\n",
    " 6. Save the updated tokenizer configuration, special tokens, and additional tokens.\n",
    " 7. Update the tokenizer's `sentencepiece` model with the new vocabulary and save it.\n",
    " 8. Test the new tokenizer to ensure it loads correctly and is compatible with `transformers`.\n",
    "\n",
    "Notes:\n",
    " - The `create_tokenizer_train_data` function is used to prepare the dataset for tokenizer training.\n",
    " - The existing tokenizer's configuration is used to ensure compatibility with the new tokenizer.\n",
    " - Special tokens and added tokens are preserved and updated in the new tokenizer.\n",
    " - The `SentencePieceBPETokenizer` class is used for training the tokenizer. \n",
    " - The `SentencePiece` model is updated and serialized to ensure that the new vocabulary is included."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b4dd90-f9fc-4157-af10-5f38bf0b75cd",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21de58d-d653-4fb4-88d6-063a1b6e223b",
   "metadata": {},
   "source": [
    "Restart the kernel after you have installed packages with `pip install` in the Notebook cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60095256-74a4-468b-993a-0ce4371a2307",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gradient 2.0.6 requires attrs<=19, but you have attrs 23.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q -U sentencepiece transformers huggingface_hub datasets sacrebleu lxml sentence-transformers accelerate fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c847328f-f2b8-432e-9c31-dda83f101a8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BASE_MODEL_ID = \"facebook/nllb-200-distilled-600M\"\n",
    "VOCAB_SIZE = 2_000\n",
    "SAVE_DIR = \"tokenizers/tokenizer_2k\"\n",
    "HFHUB_LOGIN = False\n",
    "LOAD_LOCAL_DATA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2e84b87-7ce2-4a52-af20-cd351221cb28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if HFHUB_LOGIN:\n",
    "    from huggingface_hub import notebook_login\n",
    "    notebook_login(new_session=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f3f42ee-959b-4957-90b9-f164d9659cb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "from transformers import PreTrainedTokenizerFast, NllbTokenizer\n",
    "import sentencepiece as spm\n",
    "from sentencepiece import sentencepiece_model_pb2 as sp_model\n",
    "\n",
    "from data import load_from_json\n",
    "from utils import preproc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7dd3ce-9188-4824-b32f-3487122e6ffc",
   "metadata": {},
   "source": [
    "## Prepare tokenizer training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f482c33a-0a2a-4139-92c9-8124610e80af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10929, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dyu</th>\n",
       "      <th>fr</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A bi ji min na</td>\n",
       "      <td>Il boit de l’eau.</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A le dalakolontɛ lon bɛ.</td>\n",
       "      <td>Il se plaint toujours.</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mun? Fɛn dɔ.</td>\n",
       "      <td>Quoi ? Quelque chose.</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O bɛ bi bɔra fo Gubeta.</td>\n",
       "      <td>Tous sortent excepté Gubetta.</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A ale lo bi da bugɔ la!</td>\n",
       "      <td>Ah ! c’est lui… il sonne…</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        dyu                             fr  split\n",
       "0            A bi ji min na              Il boit de l’eau.  train\n",
       "1  A le dalakolontɛ lon bɛ.         Il se plaint toujours.  train\n",
       "2              Mun? Fɛn dɔ.          Quoi ? Quelque chose.  train\n",
       "3   O bɛ bi bɔra fo Gubeta.  Tous sortent excepté Gubetta.  train\n",
       "4   A ale lo bi da bugɔ la!      Ah ! c’est lui… il sonne…  train"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_from_json(\n",
    "    train_files=\"data/dataset_train.json\",\n",
    "    valid_files=\"data/dataset_validation.json\",\n",
    "    test_files=\"data/dataset_test.json\",\n",
    "    return_format=\"df\"\n",
    ")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c7960ac-05a6-48b4-b225-0ea113b6b5e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d3ec64bce04fcf9c939ec3d3a10b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not os.path.exists(\"data\"):\n",
    "    os.mkdir(\"data\")\n",
    "\n",
    "with open(\"data/tokenizer_train_data.txt\", \"w\") as f:\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        f.write(f\"{preproc(row['dyu'])}\\n\")\n",
    "        if row['fr'] == \"0\": continue\n",
    "        f.write(f\"{preproc(row['fr'])}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88a542d-e819-44ce-a722-8c964d0a69c2",
   "metadata": {},
   "source": [
    "## Load the existing tokenizer\n",
    "\n",
    "We load the existing tokenizer for the model and save it locally. We'll only use this to get the structure and format of the config files right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdf0e2db-dbe8-4331-a261-56aed68886ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/tmp/tokenizer_config.json',\n",
       " '/tmp/special_tokens_map.json',\n",
       " '/tmp/sentencepiece.bpe.model',\n",
       " '/tmp/added_tokens.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_old = NllbTokenizer.from_pretrained(BASE_MODEL_ID)\n",
    "tokenizer_old.save_pretrained(\"/tmp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a5eb88-2a40-4c0b-9fd7-f0d29dcd6bf0",
   "metadata": {},
   "source": [
    "Read the `tokenizer_config.json` file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c5b6a71-bf96-45c5-bc11-001cf25782ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['added_tokens_decoder', 'additional_special_tokens', 'bos_token', 'clean_up_tokenization_spaces', 'cls_token', 'eos_token', 'legacy_behaviour', 'mask_token', 'model_max_length', 'pad_token', 'sep_token', 'sp_model_kwargs', 'src_lang', 'tgt_lang', 'tokenizer_class', 'unk_token'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"/tmp/tokenizer_config.json\", \"r\") as f:\n",
    "    tokenizer_config = json.load(f)\n",
    "tokenizer_config.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a001589-2a94-4cf1-a37a-cc6af52cd2ab",
   "metadata": {},
   "source": [
    "Read the `special_tokens_map.json` file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b79e4c19-7025-4ea1-ba6d-f5c1c348114d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['additional_special_tokens', 'bos_token', 'cls_token', 'eos_token', 'mask_token', 'pad_token', 'sep_token', 'unk_token'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"/tmp/special_tokens_map.json\", \"r\") as f:\n",
    "    special_tokens_map = json.load(f)\n",
    "special_tokens_map.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e397bec-ec68-45a6-8305-5d00c70a20be",
   "metadata": {},
   "source": [
    "Read the `added_tokens.json` file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ffe7dc3-e920-4bcf-b883-05cad3af1f63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['<mask>', 'ace_Arab', 'ace_Latn', 'acm_Arab', 'acq_Arab', 'aeb_Arab', 'afr_Latn', 'ajp_Arab', 'aka_Latn', 'als_Latn', 'amh_Ethi', 'apc_Arab', 'arb_Arab', 'ars_Arab', 'ary_Arab', 'arz_Arab', 'asm_Beng', 'ast_Latn', 'awa_Deva', 'ayr_Latn', 'azb_Arab', 'azj_Latn', 'bak_Cyrl', 'bam_Latn', 'ban_Latn', 'bel_Cyrl', 'bem_Latn', 'ben_Beng', 'bho_Deva', 'bjn_Arab', 'bjn_Latn', 'bod_Tibt', 'bos_Latn', 'bug_Latn', 'bul_Cyrl', 'cat_Latn', 'ceb_Latn', 'ces_Latn', 'cjk_Latn', 'ckb_Arab', 'crh_Latn', 'cym_Latn', 'dan_Latn', 'deu_Latn', 'dik_Latn', 'dyu_Latn', 'dzo_Tibt', 'ell_Grek', 'eng_Latn', 'epo_Latn', 'est_Latn', 'eus_Latn', 'ewe_Latn', 'fao_Latn', 'fij_Latn', 'fin_Latn', 'fon_Latn', 'fra_Latn', 'fur_Latn', 'fuv_Latn', 'gaz_Latn', 'gla_Latn', 'gle_Latn', 'glg_Latn', 'grn_Latn', 'guj_Gujr', 'hat_Latn', 'hau_Latn', 'heb_Hebr', 'hin_Deva', 'hne_Deva', 'hrv_Latn', 'hun_Latn', 'hye_Armn', 'ibo_Latn', 'ilo_Latn', 'ind_Latn', 'isl_Latn', 'ita_Latn', 'jav_Latn', 'jpn_Jpan', 'kab_Latn', 'kac_Latn', 'kam_Latn', 'kan_Knda', 'kas_Arab', 'kas_Deva', 'kat_Geor', 'kaz_Cyrl', 'kbp_Latn', 'kea_Latn', 'khk_Cyrl', 'khm_Khmr', 'kik_Latn', 'kin_Latn', 'kir_Cyrl', 'kmb_Latn', 'kmr_Latn', 'knc_Arab', 'knc_Latn', 'kon_Latn', 'kor_Hang', 'lao_Laoo', 'lij_Latn', 'lim_Latn', 'lin_Latn', 'lit_Latn', 'lmo_Latn', 'ltg_Latn', 'ltz_Latn', 'lua_Latn', 'lug_Latn', 'luo_Latn', 'lus_Latn', 'lvs_Latn', 'mag_Deva', 'mai_Deva', 'mal_Mlym', 'mar_Deva', 'min_Latn', 'mkd_Cyrl', 'mlt_Latn', 'mni_Beng', 'mos_Latn', 'mri_Latn', 'mya_Mymr', 'nld_Latn', 'nno_Latn', 'nob_Latn', 'npi_Deva', 'nso_Latn', 'nus_Latn', 'nya_Latn', 'oci_Latn', 'ory_Orya', 'pag_Latn', 'pan_Guru', 'pap_Latn', 'pbt_Arab', 'pes_Arab', 'plt_Latn', 'pol_Latn', 'por_Latn', 'prs_Arab', 'quy_Latn', 'ron_Latn', 'run_Latn', 'rus_Cyrl', 'sag_Latn', 'san_Deva', 'sat_Beng', 'scn_Latn', 'shn_Mymr', 'sin_Sinh', 'slk_Latn', 'slv_Latn', 'smo_Latn', 'sna_Latn', 'snd_Arab', 'som_Latn', 'sot_Latn', 'spa_Latn', 'srd_Latn', 'srp_Cyrl', 'ssw_Latn', 'sun_Latn', 'swe_Latn', 'swh_Latn', 'szl_Latn', 'tam_Taml', 'taq_Latn', 'taq_Tfng', 'tat_Cyrl', 'tel_Telu', 'tgk_Cyrl', 'tgl_Latn', 'tha_Thai', 'tir_Ethi', 'tpi_Latn', 'tsn_Latn', 'tso_Latn', 'tuk_Latn', 'tum_Latn', 'tur_Latn', 'twi_Latn', 'tzm_Tfng', 'uig_Arab', 'ukr_Cyrl', 'umb_Latn', 'urd_Arab', 'uzn_Latn', 'vec_Latn', 'vie_Latn', 'war_Latn', 'wol_Latn', 'xho_Latn', 'ydd_Hebr', 'yor_Latn', 'yue_Hant', 'zho_Hans', 'zho_Hant', 'zsm_Latn', 'zul_Latn'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"/tmp/added_tokens.json\", \"r\") as f:\n",
    "    added_tokens = json.load(f)\n",
    "added_tokens.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372d8367-c8fa-4506-814d-46113e746c59",
   "metadata": {},
   "source": [
    "## Train tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dcb79d-2b2e-4da6-8cca-84ffeb04dc7e",
   "metadata": {},
   "source": [
    "Specify the special tokens that we need for our tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4296decc-6eb5-41b5-85a8-e676257ed5fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "special_tokens = ['<s>', '<pad>', '</s>', '<unk>', 'dyu_Latn', 'fra_Latn', '<mask>']\n",
    "add_special_tokens = ['dyu_Latn', 'fra_Latn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4b75d7e-ae21-4a38-89c1-68c59b052709",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SentencePieceBPETokenizer()\n",
    "tokenizer.train(\n",
    "    \"../tokenizer_train_data.txt\",\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    # min_frequency=5,\n",
    "    show_progress=True,\n",
    "    # limit_alphabet=500,\n",
    "    special_tokens=special_tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ff442a-0121-4569-9e69-77ac71103f71",
   "metadata": {},
   "source": [
    "Convert to a Huggingface `PreTrainedTokenizerFast` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cc6b08d-a7e6-40ed-9f8f-2b43bd0abae4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer, clean_up_tokenization_spaces=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973c7630-1a57-4366-81ab-2c4704548b32",
   "metadata": {},
   "source": [
    "Tokenize an example sentence with the new and old tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84a4ad7e-9d35-4318-b06a-c9b20099bb88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[90, 117, 427, 187, 178]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['a bi ji min na']).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d23479a-e5f7-4904-92c9-051b84f1c878",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[256047, 9, 330, 850, 531, 62, 2]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_old(['a bi ji min na']).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c30c5f37-2f0f-42ef-8f8f-f80041b29472",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-30 12:57:52.887992: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-30 12:57:52.888049: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-30 12:57:52.889126: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-30 12:57:52.895175: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-30 12:57:53.597108: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a bi ji min na']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(tokenizer(['a bi ji min na']).input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a689a79-6968-421c-bd34-efd41715a9e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁a', '▁bi', '▁ji', '▁min', '▁na']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenizer(['a bi ji min na']).input_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd34c36-91be-49a1-82ed-579139bb1e6c",
   "metadata": {},
   "source": [
    "## Update tokenizer config files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1467ac-ee21-4bbc-bff3-75245e310fe9",
   "metadata": {},
   "source": [
    "Update the `tokenizer_config` that was loaded from the `tokenizer_config.json` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c759020f-b605-4546-b17c-7a2a1eaa6b7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'added_tokens_decoder': {'4': {'content': 'dyu_Latn',\n",
       "   'lstrip': False,\n",
       "   'normalized': False,\n",
       "   'rstrip': False,\n",
       "   'single_word': False,\n",
       "   'special': True},\n",
       "  '6': {'content': '<mask>',\n",
       "   'lstrip': False,\n",
       "   'normalized': False,\n",
       "   'rstrip': False,\n",
       "   'single_word': False,\n",
       "   'special': True},\n",
       "  '1': {'content': '<pad>',\n",
       "   'lstrip': False,\n",
       "   'normalized': False,\n",
       "   'rstrip': False,\n",
       "   'single_word': False,\n",
       "   'special': True},\n",
       "  '0': {'content': '<s>',\n",
       "   'lstrip': False,\n",
       "   'normalized': False,\n",
       "   'rstrip': False,\n",
       "   'single_word': False,\n",
       "   'special': True},\n",
       "  '5': {'content': 'fra_Latn',\n",
       "   'lstrip': False,\n",
       "   'normalized': False,\n",
       "   'rstrip': False,\n",
       "   'single_word': False,\n",
       "   'special': True},\n",
       "  '3': {'content': '<unk>',\n",
       "   'lstrip': False,\n",
       "   'normalized': False,\n",
       "   'rstrip': False,\n",
       "   'single_word': False,\n",
       "   'special': True},\n",
       "  '2': {'content': '</s>',\n",
       "   'lstrip': False,\n",
       "   'normalized': False,\n",
       "   'rstrip': False,\n",
       "   'single_word': False,\n",
       "   'special': True}},\n",
       " 'additional_special_tokens': ['dyu_Latn', 'fra_Latn'],\n",
       " 'bos_token': '<s>',\n",
       " 'clean_up_tokenization_spaces': True,\n",
       " 'cls_token': '<s>',\n",
       " 'eos_token': '</s>',\n",
       " 'legacy_behaviour': False,\n",
       " 'mask_token': '<mask>',\n",
       " 'model_max_length': 1024,\n",
       " 'pad_token': '<pad>',\n",
       " 'sep_token': '</s>',\n",
       " 'sp_model_kwargs': {},\n",
       " 'src_lang': 'eng_Latn',\n",
       " 'tgt_lang': None,\n",
       " 'tokenizer_class': 'NllbTokenizer',\n",
       " 'unk_token': '<unk>'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _added_tokens_decoder_to_dict(token):\n",
    "    return {\n",
    "        'content': token,\n",
    "        'lstrip': False,\n",
    "        'normalized': False,\n",
    "        'rstrip': False,\n",
    "        'single_word': False,\n",
    "        'special': True\n",
    "    }\n",
    "tokenizer_config[\"added_tokens_decoder\"] = {\n",
    "    str(i): _added_tokens_decoder_to_dict(t) for t, i in tokenizer.vocab.items() if t in special_tokens\n",
    "}\n",
    "tokenizer_config[\"additional_special_tokens\"] = add_special_tokens\n",
    "tokenizer_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17288e63-3bd3-4f45-b148-489ed096e663",
   "metadata": {},
   "source": [
    "Update the `special_tokens_map` that was loaded from the `special_tokens_map.json` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a31ab803-393e-4e21-9d09-61fb48d812a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'additional_special_tokens': ['dyu_Latn', 'fra_Latn'],\n",
       " 'bos_token': {'content': '<s>',\n",
       "  'lstrip': False,\n",
       "  'normalized': False,\n",
       "  'rstrip': False,\n",
       "  'single_word': False},\n",
       " 'cls_token': {'content': '<s>',\n",
       "  'lstrip': False,\n",
       "  'normalized': False,\n",
       "  'rstrip': False,\n",
       "  'single_word': False},\n",
       " 'eos_token': {'content': '</s>',\n",
       "  'lstrip': False,\n",
       "  'normalized': False,\n",
       "  'rstrip': False,\n",
       "  'single_word': False},\n",
       " 'mask_token': {'content': '<mask>',\n",
       "  'lstrip': True,\n",
       "  'normalized': True,\n",
       "  'rstrip': False,\n",
       "  'single_word': False},\n",
       " 'pad_token': {'content': '<pad>',\n",
       "  'lstrip': False,\n",
       "  'normalized': False,\n",
       "  'rstrip': False,\n",
       "  'single_word': False},\n",
       " 'sep_token': {'content': '</s>',\n",
       "  'lstrip': False,\n",
       "  'normalized': False,\n",
       "  'rstrip': False,\n",
       "  'single_word': False},\n",
       " 'unk_token': {'content': '<unk>',\n",
       "  'lstrip': False,\n",
       "  'normalized': False,\n",
       "  'rstrip': False,\n",
       "  'single_word': False}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens_map[\"additional_special_tokens\"] = add_special_tokens\n",
    "special_tokens_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddd51a4-3a0d-4f03-afdb-c06a81553b4f",
   "metadata": {},
   "source": [
    "Update the `added_tokens` that was loaded from the `added_tokens.json` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c58391fb-be59-4082-9a9e-089e6a33bb9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<mask>': 6, 'dyu_Latn': 4, 'fra_Latn': 5}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "added_tokens = {t: tokenizer.convert_tokens_to_ids(t) for t in [\"<mask>\", \"dyu_Latn\", \"fra_Latn\"]}\n",
    "added_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f1d12b-4255-45f2-8574-127bd620482a",
   "metadata": {},
   "source": [
    "## Save the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7decfb4-f4d6-418f-a953-4541f999407f",
   "metadata": {},
   "source": [
    "Create a folder to save the new tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56cac427-6f7a-43f5-a94a-aa2bfeac4ed1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(SAVE_DIR):\n",
    "    os.makedirs(SAVE_DIR)\n",
    "new_tokenizer_dir = Path(SAVE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acf0b92-f361-475e-a0b0-5995ba0dcd4e",
   "metadata": {},
   "source": [
    "Save the new `tokenizer_config.json` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0683c2a-132d-45da-a953-0ac12ad60679",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(new_tokenizer_dir/\"tokenizer_config.json\", \"w\") as f:\n",
    "    json.dump(tokenizer_config, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003772b4-b2c8-46fb-a602-ad7812b9aac2",
   "metadata": {},
   "source": [
    "Save the `special_tokens_map.json` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6cb6b24-fa5a-464a-aa2e-a408ded0b19f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(new_tokenizer_dir/\"special_tokens_map.json\", \"w\") as f:\n",
    "    json.dump(special_tokens_map, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f380c8a-50ea-4aad-990a-41fd91e889f5",
   "metadata": {},
   "source": [
    "Save the `added_tokens.json` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8aef7bc-a741-4301-be2d-70a6e9150e02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(new_tokenizer_dir/\"added_tokens.json\", \"w\") as f:\n",
    "    json.dump(added_tokens, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35fda79-7f5c-41d7-ad25-d30319215822",
   "metadata": {},
   "source": [
    "Update the pre-trained NllbTokenizer's `sentencepiece` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "489c2a6c-0cb2-4688-8805-b908571b52a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4852054"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = sp_model.ModelProto()\n",
    "m.ParseFromString(open(\"/tmp/sentencepiece.bpe.model\", \"rb\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12418318-1c16-48d7-a46e-b9f737c31fd2",
   "metadata": {},
   "source": [
    "Loop over `m.pieces` and keep only keep the tokens that are in the new vocab. This takes a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1f4e5fe-3a1e-4fe3-81b1-6229b872b06d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(m.pieces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77a42c3b-6205-4717-83c5-694b2a65183f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seen = set()\n",
    "while True:\n",
    "    if m.pieces[0].piece in seen:\n",
    "        break\n",
    "    x = m.pieces.pop(0)\n",
    "    seen.add(x.piece)\n",
    "    if x.piece in tokenizer.vocab:\n",
    "        m.pieces.append(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f269cbe-5eac-41db-ad88-4102d2ad4912",
   "metadata": {},
   "source": [
    "Add tokens that were not in the old tokenizer's vocab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b83cb022-76b2-47fb-9e88-393a6d629eb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "add_tokens = set(tokenizer.vocab.keys()) - seen\n",
    "\n",
    "for token in add_tokens:\n",
    "    new_token = sp_model.ModelProto().SentencePiece()\n",
    "    new_token.piece = token\n",
    "    new_token.score = 0\n",
    "    m.pieces.append(new_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "782fb07b-7d55-4d1e-a65a-3a48e16d646f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert len(m.pieces) == len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ddd81241-f4b7-4364-8549-6c73025ca581",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "piece: \"<unk>\"\n",
       "score: 0\n",
       "type: UNKNOWN"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.pieces[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b4611d57-825a-4ffb-b9e0-beb1cc792b7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(new_tokenizer_dir/'sentencepiece.bpe.model', 'wb') as f:\n",
    "    f.write(m.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c42241-1804-499b-8bb1-c44b211e13db",
   "metadata": {},
   "source": [
    "## Test loading the new tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b693931-da4d-4247-8ddf-5c84d57c5e65",
   "metadata": {
    "tags": []
   },
   "source": [
    "Load the new `sentencepiece` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "26ad8e35-c1ab-4a1c-b166-62988ca14c26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(str(new_tokenizer_dir/'sentencepiece.bpe.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "493c16ed-ab7e-4744-804d-9682d1051a80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁th', 'is', '▁', 'is', '▁a', '▁t', 'est']\n",
      "[134, 20, 1426, 20, 8, 6, 195]\n"
     ]
    }
   ],
   "source": [
    "print(sp.encode_as_pieces('this is a test'))\n",
    "print(sp.encode_as_ids('this is a test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979c7139-85ef-45ce-98c9-952a4909148c",
   "metadata": {},
   "source": [
    "Test loading the new tokenizer with `transformers`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a875143-8cbb-4eef-aff8-804a90d43f6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = NllbTokenizer.from_pretrained(new_tokenizer_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "46f7c857-506a-4ae7-a10b-a93bf01d64f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2001"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "65d7615e-4b96-4008-b695-94c5f06c35cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁il', '▁bo', 'it', '▁de', '▁l’', 'e', 'au']\n",
      "fra_Latn il boit de l’eau</s>\n"
     ]
    }
   ],
   "source": [
    "t = \"il boit de l’eau\"\n",
    "tokenizer.src_lang = \"fra_Latn\"\n",
    "print(tokenizer.tokenize(t))\n",
    "print(tokenizer.decode(tokenizer.encode(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8a5c0328-3b51-4197-8986-22cb5f58d951",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(\"dyu_Latn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1ea23106-cac7-4d5c-b58a-9f96571939fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(\"fra_Latn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "679f1000-f9d7-4f4f-99b2-751cf04ca1d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(\"<mask>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a27b0bb-5444-4e2f-b57c-601833b3b730",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
