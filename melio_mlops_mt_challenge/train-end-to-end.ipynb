{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9ce1b17-d0a6-4e12-9192-055b427dccff",
   "metadata": {},
   "source": [
    "# Train end-to-end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192a4a7a-2db1-4dc7-b70c-45c0ca3f9904",
   "metadata": {},
   "source": [
    "This notebook trains the competition models end-to-end. Notebooks for each individual step is also available, and those notebooks describe the steps in much greater detail. Please refer to the `README` for information about the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75e6eeb-efa4-4181-9729-5355f93d01b9",
   "metadata": {},
   "source": [
    "> **Note about signing in to the Hugging Face Hub:** This notebook requires the user to be signed in to the Hugging Face Hub and to be authenticated to download the `uvci/Koumankan_mt_dyu_fr` dataset. If you are not already signed in, run the five four cells one-by-one and use the `notebook_login` functionality to sign in. Once you are signed in, then you can run all cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30c133e-2e24-4892-b380-1690d34cc6cf",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752cfd49-885e-4127-a412-5b15ca7ac298",
   "metadata": {},
   "source": [
    "Make sure that you are connected to a GPU instance. If you opened the notebook in Colab, connect to a T4 GPU instance. In Kaggle, select the P100 GPU accelerator.\n",
    "\n",
    "If you opened this Notebook in a platform such as Colab or Kaggle, make sure that the following source files from the repo is in your working directory:\n",
    " - `data.py`\n",
    " - `evaluation.py`\n",
    " - `modeling.py`\n",
    " - `tokenizers_mod.py`\n",
    " - `translation.py`\n",
    " - `utils.py`\n",
    "\n",
    "In Colab, the easiest way to get the files into your working directory is to simply upload them from your local machine.\n",
    "\n",
    "In Kaggle, to upload files, you have to create a `dataset`. From the notebook editor, click on `Upload`, select `New Dataset`, and upload the files. Once your `dataset` has been created, you must copy the files to your working directory. One way to do that is to add a new cell to the notebook and to run `!cp /kaggle/input/<your-dataset-name>/*py .`\n",
    "\n",
    "Alternatively, the files can be downloaded from a AWS S3 bucket to your local directory. To do that, simply uncomment and execute the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fc6107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -O code.zip --no-check-certificate --no-proxy 'https://pfaof7krtww4e.s3.af-south-1.amazonaws.com/code.zip' && unzip code.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f372d8",
   "metadata": {},
   "source": [
    "If you have not installed the dependencies in the `requirements.txt` file in a terminal, you can uncomment the following cell and install the dependencies directly from the notebook. That can be convenient in platforms such as Colab or Kaggle. Make sure that the `requirements.txt` file is in your working directory (see steps above) before running the install here.\n",
    "\n",
    "> **Restart the kernel after you have installed packages with `pip install` in the notebook cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094fbca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "802e43db-7d6e-4e64-98ec-d40ebbd53364",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from functools import partial\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "from huggingface_hub import notebook_login\n",
    "import sentencepiece as spm\n",
    "from sentencepiece import sentencepiece_model_pb2 as sp_model\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, NllbTokenizer\n",
    "from fastai.callback.training import GradientAccumulation\n",
    "\n",
    "from tokenizers_mod import (\n",
    "    limit_tokenizer_vocab,\n",
    "    train_new_tokenizer\n",
    ")\n",
    "from utils import preproc, set_seed, cleanup\n",
    "from data import (\n",
    "    load_from_json,\n",
    "    save_data_locally,\n",
    "    ds2df,\n",
    "    TranslationDataset,\n",
    "    create_dataloaders\n",
    ")\n",
    "from modeling import (\n",
    "    load_model,\n",
    "    instantiate_small_model,\n",
    "    prune,\n",
    "    create_learner,\n",
    "    create_distillation_learner\n",
    ")\n",
    "from translation import translate, back_translate\n",
    "from evaluation import calculate_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f641ae7f-f49c-420f-b708-f2c38f5e79ed",
   "metadata": {},
   "source": [
    "## Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80ecd4af-d7eb-49cf-a1ac-9e2aa5fc2659",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DOWNLOAD_DATA = True\n",
    "TRAIN_TOKENIZERS = True\n",
    "TRAIN_DYU_FRA_LARGE = True\n",
    "TRAIN_FRA_DYU_LARGE = True\n",
    "CREATE_BACK_TRANSLATIONS = True\n",
    "TRAIN_DYU_FRA_SMALL = True\n",
    "TRAIN_DYU_FRA_DISTILLED = False\n",
    "\n",
    "RANDOM_SEED = 7  # Set `RANDOM_SEED = None` to run without a seed\n",
    "\n",
    "if RANDOM_SEED is not None:\n",
    "    set_seed(RANDOM_SEED, reproducible=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92721dbb-2850-4686-ac77-f797394ec701",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d66eb3-ebd1-48e1-8eae-b95cb6c455a6",
   "metadata": {},
   "source": [
    "Do not run any cells beyond the login cell untill you're signed into the Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36d77565",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User is already logged in.\n"
     ]
    }
   ],
   "source": [
    "if DOWNLOAD_DATA:\n",
    "    notebook_login(new_session=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f1d446c-5543-4402-9e77-80cc2c7b014c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bca16f57bbf430fbc608f9b0681c152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c73c091e095342c8b101029418ac803a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/530k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "176cc503628d43d7bc2a5af4a435386d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/102k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b21e3e787db043ef8c44c8f0dd67716b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/55.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8197d42aace847a885e2c11339265a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/8065 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc551590ec91484f82666c710034a7cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1471 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c344bb3b3a7453d88e8c478960c02e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1393 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if DOWNLOAD_DATA:\n",
    "    ds = load_dataset(\"uvci/Koumankan_mt_dyu_fr\")\n",
    "    save_data_locally(ds, save_dir=\"./data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aa2bb0-23f8-4850-b17f-6890e7b2babf",
   "metadata": {},
   "source": [
    "## Train tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e070b58-f4b0-4e3e-ba66-31bd7bcd9084",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tokenizer 1\n",
    "\n",
    "Pretrained `NllbTokenizer` with vocabulary limited to tokens appearing in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f2eb89f-27e2-4d2f-9c9b-40de925d58bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4929e1c015ef448ba2d9d54f15f9447a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f62e310726d4a2c890d075aef0c7205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "937a372106df4f7ebc06b85d63f3cd5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdb50a4007564707a9685d67924d4fb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/564 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239d1079520647339be34d406f387f5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/4.85M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb96a3235aae48de927f3114548ade7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/3.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28857c3b64514a9baafa1b7f2756faf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens used: 10802\n",
      "New vocabulary size: 10806\n",
      "Updated index: 10802: <mask>\n",
      "Updated index: 10804: dyu_Latn\n",
      "Updated index: 10805: fra_Latn\n",
      "Updated index: 10804: dyu_Latn\n",
      "Updated index: 10805: fra_Latn\n",
      "Updated index: 10802: <mask>\n",
      "CPU times: user 1min 27s, sys: 292 ms, total: 1min 27s\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if TRAIN_TOKENIZERS and (TRAIN_DYU_FRA_LARGE or TRAIN_FRA_DYU_LARGE):\n",
    "    df = load_from_json(\n",
    "        train_files=\"data/dataset_train.json\",\n",
    "        valid_files=\"data/dataset_validation.json\",\n",
    "        test_files=\"data/dataset_test.json\",\n",
    "        return_format=\"df\"\n",
    "    )\n",
    "    limit_tokenizer_vocab(\n",
    "        model_id=\"facebook/nllb-200-distilled-600M\",\n",
    "        df=df,\n",
    "        save_dir=\"tokenizers/tokenizer_freq1\",\n",
    "        min_token_freq=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67133c6-efb5-4490-8740-965f0426a721",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tokenizer 2\n",
    "\n",
    "Pretrained `NllbTokenizer` with vocabulary limited to tokens appearing in the training data with a minimum frequency of 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ee8bb43-7dab-462f-b971-0103e5c7434b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens used: 3705\n",
      "New vocabulary size: 3709\n",
      "Updated index: 3705: <mask>\n",
      "Updated index: 3707: dyu_Latn\n",
      "Updated index: 3708: fra_Latn\n",
      "Updated index: 3707: dyu_Latn\n",
      "Updated index: 3708: fra_Latn\n",
      "Updated index: 3705: <mask>\n",
      "CPU times: user 1min 23s, sys: 159 ms, total: 1min 23s\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if TRAIN_TOKENIZERS and TRAIN_DYU_FRA_DISTILLED:\n",
    "    df = load_from_json(\n",
    "        train_files=\"data/dataset_train.json\",\n",
    "        valid_files=\"data/dataset_validation.json\",\n",
    "        test_files=\"data/dataset_test.json\",\n",
    "        return_format=\"df\"\n",
    "    )\n",
    "    limit_tokenizer_vocab(\n",
    "        model_id=\"facebook/nllb-200-distilled-600M\",\n",
    "        df=df,\n",
    "        save_dir=\"tokenizers/tokenizer_freq5\",\n",
    "        min_token_freq=5\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58debfd6-3c9d-40b3-9ef6-7d790810c86e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tokenizer 3\n",
    "\n",
    "Train a `NllbTokenizer` with a vocabulary of 2,000 tokens from scratch on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f3b130d-050b-40e8-8afe-ea0adf469e91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "CPU times: user 3min, sys: 344 ms, total: 3min\n",
      "Wall time: 3min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if TRAIN_TOKENIZERS and TRAIN_DYU_FRA_SMALL:\n",
    "    df = load_from_json(\n",
    "        train_files=\"data/dataset_train.json\",\n",
    "        valid_files=\"data/dataset_validation.json\",\n",
    "        test_files=\"data/dataset_test.json\",\n",
    "        return_format=\"df\"\n",
    "    )\n",
    "    train_new_tokenizer(\n",
    "        model_id=\"facebook/nllb-200-distilled-600M\",\n",
    "        df=df,\n",
    "        save_dir=\"tokenizers/tokenizer_2k\",\n",
    "        vocab_size=2000\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aa9cc7-ea8d-44f5-911f-160a46b8cc83",
   "metadata": {},
   "source": [
    "## Fine-tune NLLB-200 600M for Dyula to French translation\n",
    "\n",
    "Purpose of the model is to train a high-quality model to create back-translations and to use as a teacher model in knowledge distillation training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4226ed6-3040-4e6b-9ed5-d6ddfb87b89b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation from dyu_Latn -> fra_Latn\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d69cb8071db34c3aa9661499868e0f85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/846 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0f4829e7b4743beac81669cbac23743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0752be8168c1420281027d8524ece45e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens w/o embeddings: 1\n",
      "model.shared.weight torch.Size([256206, 1024])\n",
      "model.encoder.embed_tokens.weight torch.Size([256206, 1024])\n",
      "model.decoder.embed_tokens.weight torch.Size([256206, 1024])\n",
      "lm_head.weight torch.Size([256206, 1024])\n",
      "Memory footprint: 1.36GB\n",
      "Memory footprint: 1.36GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a13c88c644a94cd480b4133757cb0155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1471 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation BLEU before fine-tuning: 4.440538477725872\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>BLEU</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.906576</td>\n",
       "      <td>2.704631</td>\n",
       "      <td>7.013531</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.391741</td>\n",
       "      <td>2.412951</td>\n",
       "      <td>8.104990</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.980386</td>\n",
       "      <td>2.288856</td>\n",
       "      <td>10.148932</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.704136</td>\n",
       "      <td>2.270944</td>\n",
       "      <td>10.677092</td>\n",
       "      <td>01:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.484883</td>\n",
       "      <td>2.294230</td>\n",
       "      <td>11.592265</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.373568</td>\n",
       "      <td>2.329874</td>\n",
       "      <td>11.383374</td>\n",
       "      <td>01:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.223823</td>\n",
       "      <td>2.363985</td>\n",
       "      <td>11.852718</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.161909</td>\n",
       "      <td>2.379085</td>\n",
       "      <td>11.604426</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.132784</td>\n",
       "      <td>2.401020</td>\n",
       "      <td>11.479462</td>\n",
       "      <td>01:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.147457</td>\n",
       "      <td>2.402054</td>\n",
       "      <td>11.493653</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65d33f141d864e95bb5516514562067d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1471 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation BLEU before fine-tuning: 11.452626482241318\n",
      "[2024-09-17 15:37:03,219] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 14s, sys: 1min 23s, total: 12min 38s\n",
      "Wall time: 12min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if TRAIN_DYU_FRA_LARGE:\n",
    "    SRC_LANG = \"dyu\"\n",
    "    TGT_LANG = \"fr\"\n",
    "\n",
    "    MODEL_ID = \"facebook/nllb-200-distilled-600M\"\n",
    "    TOKENIZER_ID = \"tokenizers/tokenizer_freq1\"\n",
    "    MODEL_SAVE_PATH = 'saved_models/dyu-fra-600M'\n",
    "\n",
    "    src_lang_code = \"dyu_Latn\" if SRC_LANG == \"dyu\" else \"fra_Latn\"\n",
    "    tgt_lang_code = \"dyu_Latn\" if TGT_LANG == \"dyu\" else \"fra_Latn\"\n",
    "    print(f\"Translation from {src_lang_code} -> {tgt_lang_code}\")\n",
    "\n",
    "    # Load data\n",
    "    df = load_from_json(\n",
    "        train_files=\"data/dataset_train.json\",\n",
    "        valid_files=\"data/dataset_validation.json\",\n",
    "        test_files=\"data/dataset_test.json\",\n",
    "        return_format=\"df\"\n",
    "    )\n",
    "    df_train = df[df[\"split\"] == \"train\"].copy()\n",
    "    df_valid = df[df[\"split\"]== \"validation\"].copy()\n",
    "    df_test = df[df[\"split\"]== \"test\"].copy()\n",
    "    assert len(df_train) + len(df_valid) + len(df_test) == len(df)\n",
    "\n",
    "    # Load model\n",
    "    model, tokenizer = load_model(\n",
    "        MODEL_ID, tokenizer_id=TOKENIZER_ID, load_tokenizer=True, remap_embeddings=True,\n",
    "        init_embeds_for_new_tokens=True, src_language=src_lang_code, tgt_language=tgt_lang_code,\n",
    "    )\n",
    "    print(f\"Memory footprint: {model.get_memory_footprint() / 1024**3 :.2f}GB\")\n",
    "\n",
    "    # Validation BLEU before fine-tuning\n",
    "    translate_func = partial(\n",
    "        translate, model=model, tokenizer=tokenizer, src_lang=src_lang_code, tgt_lang=tgt_lang_code\n",
    "    )\n",
    "    print(\n",
    "        \"Validation BLEU before fine-tuning:\",\n",
    "        calculate_bleu(\n",
    "            model, df_valid[[SRC_LANG, TGT_LANG]], translate_func, src_lang=src_lang_code, tgt_lang=tgt_lang_code,\n",
    "            preproc_func=preproc\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Create dataloaders and fastai learner\n",
    "    bs, validation_bs = 16, 128\n",
    "    dls = create_dataloaders(\n",
    "        df_train[[SRC_LANG, TGT_LANG]], df_valid[[SRC_LANG, TGT_LANG]], tokenizer, bs=bs,\n",
    "        src_lang=src_lang_code, tgt_lang=tgt_lang_code, preproc_func=preproc,\n",
    "        max_length=128, validation_bs=validation_bs\n",
    "    )\n",
    "    learn = create_learner(\n",
    "        dls, model, tokenizer, src_lang=src_lang_code, tgt_lang=tgt_lang_code\n",
    "    )\n",
    "    accum_steps = max(1, 32//bs)\n",
    "    learn.fit_one_cycle(10, 5e-5, cbs=GradientAccumulation(accum_steps))\n",
    "\n",
    "    # Validation BLEU after fine-tuning\n",
    "    print(\n",
    "        \"Validation BLEU before fine-tuning:\",\n",
    "        calculate_bleu(\n",
    "            model, df_valid, translate_func, src_lang=src_lang_code, tgt_lang=tgt_lang_code,\n",
    "            preproc_func=preproc\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.save_pretrained(MODEL_SAVE_PATH)\n",
    "    tokenizer.save_pretrained(MODEL_SAVE_PATH)\n",
    "    cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a45c53-9eb4-4082-ae9b-476c1fe4e814",
   "metadata": {},
   "source": [
    "## Fine-tune NLLB-200 600M for French to Dyula translation\n",
    "\n",
    "Purpose of the model is to train a high-quality model to create back-translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86b387c1-6524-4327-b0c5-a9372342f0b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation from fra_Latn -> dyu_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens w/o embeddings: 1\n",
      "model.shared.weight torch.Size([256206, 1024])\n",
      "model.encoder.embed_tokens.weight torch.Size([256206, 1024])\n",
      "model.decoder.embed_tokens.weight torch.Size([256206, 1024])\n",
      "lm_head.weight torch.Size([256206, 1024])\n",
      "Memory footprint: 1.36GB\n",
      "Memory footprint: 1.36GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c7e4c4017294e3a9da8516b20d38ab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1471 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation BLEU before fine-tuning: 2.641426643203745\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>BLEU</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.960861</td>\n",
       "      <td>3.052158</td>\n",
       "      <td>6.702611</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.428807</td>\n",
       "      <td>2.920789</td>\n",
       "      <td>7.301053</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.222335</td>\n",
       "      <td>2.909772</td>\n",
       "      <td>7.935876</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a20692ff7a044dddac3d44a7111130e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1471 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation BLEU before fine-tuning: 7.885343659481106\n",
      "CPU times: user 3min 34s, sys: 23.3 s, total: 3min 58s\n",
      "Wall time: 3min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if TRAIN_FRA_DYU_LARGE:\n",
    "    SRC_LANG = \"fr\"\n",
    "    TGT_LANG = \"dyu\"\n",
    "\n",
    "    MODEL_ID = \"facebook/nllb-200-distilled-600M\"\n",
    "    TOKENIZER_ID = \"tokenizers/tokenizer_freq1\"\n",
    "    MODEL_SAVE_PATH = 'saved_models/fra-dyu-600M'\n",
    "\n",
    "    src_lang_code = \"dyu_Latn\" if SRC_LANG == \"dyu\" else \"fra_Latn\"\n",
    "    tgt_lang_code = \"dyu_Latn\" if TGT_LANG == \"dyu\" else \"fra_Latn\"\n",
    "    print(f\"Translation from {src_lang_code} -> {tgt_lang_code}\")\n",
    "\n",
    "    # Load data\n",
    "    df = load_from_json(\n",
    "        train_files=\"data/dataset_train.json\",\n",
    "        valid_files=\"data/dataset_validation.json\",\n",
    "        test_files=\"data/dataset_test.json\",\n",
    "        return_format=\"df\"\n",
    "    )\n",
    "    df_train = df[df[\"split\"] == \"train\"].copy()\n",
    "    df_valid = df[df[\"split\"]== \"validation\"].copy()\n",
    "    df_test = df[df[\"split\"]== \"test\"].copy()\n",
    "    assert len(df_train) + len(df_valid) + len(df_test) == len(df)\n",
    "\n",
    "    # Load model\n",
    "    model, tokenizer = load_model(\n",
    "        MODEL_ID, tokenizer_id=TOKENIZER_ID, load_tokenizer=True, remap_embeddings=True,\n",
    "        init_embeds_for_new_tokens=True, src_language=src_lang_code, tgt_language=tgt_lang_code,\n",
    "    )\n",
    "    print(f\"Memory footprint: {model.get_memory_footprint() / 1024**3 :.2f}GB\")\n",
    "\n",
    "    # Validation BLEU before fine-tuning\n",
    "    translate_func = partial(\n",
    "        translate, model=model, tokenizer=tokenizer, src_lang=src_lang_code, tgt_lang=tgt_lang_code\n",
    "    )\n",
    "    print(\n",
    "        \"Validation BLEU before fine-tuning:\",\n",
    "        calculate_bleu(\n",
    "            model, df_valid[[SRC_LANG, TGT_LANG]], translate_func, src_lang=src_lang_code, tgt_lang=tgt_lang_code,\n",
    "            preproc_func=preproc\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Create dataloaders and fastai learner\n",
    "    bs, validation_bs = 16, 128\n",
    "    dls = create_dataloaders(\n",
    "        df_train[[SRC_LANG, TGT_LANG]], df_valid[[SRC_LANG, TGT_LANG]], tokenizer, bs=bs,\n",
    "        src_lang=src_lang_code, tgt_lang=tgt_lang_code, preproc_func=preproc,\n",
    "        max_length=128, validation_bs=validation_bs\n",
    "    )\n",
    "    learn = create_learner(\n",
    "        dls, model, tokenizer, src_lang=src_lang_code, tgt_lang=tgt_lang_code\n",
    "    )\n",
    "    accum_steps = max(1, 32//bs)\n",
    "    learn.fit_one_cycle(3, 1e-4, cbs=GradientAccumulation(accum_steps))\n",
    "\n",
    "    # Validation BLEU after fine-tuning\n",
    "    print(\n",
    "        \"Validation BLEU before fine-tuning:\",\n",
    "        calculate_bleu(\n",
    "            model, df_valid, translate_func, src_lang=src_lang_code, tgt_lang=tgt_lang_code,\n",
    "            preproc_func=preproc\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.save_pretrained(MODEL_SAVE_PATH)\n",
    "    tokenizer.save_pretrained(MODEL_SAVE_PATH)\n",
    "    cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba56d391-160c-4db0-8246-2d78ae650cf4",
   "metadata": {},
   "source": [
    "## Create back-translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbfdcc63-6ae8-429b-8211-a825476931d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 0.68GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcbfd250d94642bab52565227b6133c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/564 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa54d610f244acc8a4b636b672412a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/4.85M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a98484de3b5d4d079401b10e4edb7ced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/3.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "971ad040a17a4ce9946a1aff109a734b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55a50d51084d40efbcc6fb4421b80d10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/808 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a67b8b2a1e924a788d862fad2f0d22c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/5.48G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d7ff2fcbc834573bd94a71e077bb0ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 2.56GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2510db0b1fd343088cc6f1ca9d408d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8065 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 15:44:12.862652: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-17 15:44:12.862759: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-17 15:44:12.948344: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-17 15:44:13.109240: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-17 15:44:14.707253: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea45553228f240d489bc377f98e73a6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8065 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d65cfe18a2a4a0b9e4756e345767bc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8065 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9292cce11ed94d0793ddd0088995dabb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8065 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "059cfeecf84645e590fe40e37ddc4033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8065 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb2cda9513b54de181fd6bc79b17ddee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8065 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b8d532b95944c91a2386e06e6f2aa63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8065 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4aa2205ce3743dc9c1d50d5ce180416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8065 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e41dd19781a344d19703ff1f93145dad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8065 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeeddff9369348bb92dc8f7b5a97e57d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8065 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c26dbee2deb4361a4a1d8ab39e638db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8065 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3e8549077af4c4eb5e17cfe66d72d8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8065 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fd7ee8cbe2f44f58565875bb3610a11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8065 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c90c7bc4c22f43c6b6c271cab1df68c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8065 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cce9740ab85140cda7f13b2bcf3492af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8065 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc64f707420047af86494fe39306f23d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1471 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99f7ed7de61b4c6fbf84e8b188555573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1471 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73a12a5b636e42ac9eb2eec29c573b02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1471 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84db2a49e467434985852bbb5f29479a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1471 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abfa7f4bf13f43f4a93c97385c8e1727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1471 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "924a285159134e6ea097cf95d1ecb700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1471 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e68452116144a119f4b9c9a3fafe6e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1471 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba24d70e806f498b976079249ddfe239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1471 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "821d9a8b69a64d2bbecd2a8318237a6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1471 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d57eecbe024e818790ed6ea423f965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1471 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 0.68GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db398882f6d2477082e5d7ef3bf8ab02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1393 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa85a5b56df45ba9d2936c9f1ae482c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1393 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "824a092157354e93a1885159d6b8db55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1393 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a21a21285954815be394f243996050d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1393 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "240917aa159346b8b554e89f48226497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1393 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26min 59s, sys: 43.5 s, total: 27min 42s\n",
      "Wall time: 30min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if CREATE_BACK_TRANSLATIONS:\n",
    "\n",
    "    def _save(savedir=\"./data\", df=None, json_data=None, split=\"\"):\n",
    "        if df is None and json_data is None:\n",
    "            print(\"Nothing to save\")\n",
    "            return None\n",
    "\n",
    "        if not os.path.exists(\"./data\"):\n",
    "            os.mkdir(\"data\")\n",
    "\n",
    "        if df is not None:\n",
    "            _ = df_bt.to_csv(f\"data/dataset_bt_{DATA_SPLIT}.txt\", sep=\"|\", index=False)\n",
    "\n",
    "        if json_data is not None:\n",
    "            with open(f\"data/dataset_bt_{DATA_SPLIT}.json\", \"w\") as f:\n",
    "                json.dump(back_translations_json, f, indent=4)\n",
    "\n",
    "    # Load data\n",
    "    ds = load_from_json(\n",
    "        train_files=\"data/dataset_train.json\",\n",
    "        valid_files=\"data/dataset_validation.json\",\n",
    "        test_files=\"data/dataset_test.json\",\n",
    "        return_format=\"ds\"\n",
    "    )\n",
    "    \n",
    "    # ------------------------------------------------\n",
    "    # Create back-translations from the training data\n",
    "    # ------------------------------------------------\n",
    "    MODEL_ID = './saved_models/fra-dyu-600M'\n",
    "    DATA_SPLIT = \"train\"\n",
    "    SRC_LANG = \"fra_Latn\"\n",
    "    TGT_LANG = \"dyu_Latn\"\n",
    "    SRC2SRC_SAMPLING = True\n",
    "    SRC2SRC_MODEL_ID = \"facebook/nllb-200-distilled-1.3B\"\n",
    "\n",
    "    tokenizer = NllbTokenizer.from_pretrained(MODEL_ID, src_lang=SRC_LANG, tgt_lang=TGT_LANG)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_ID, device_map=\"cuda\", torch_dtype=torch.bfloat16)\n",
    "    print(f\"Memory footprint: {model.get_memory_footprint() / 1024**3 :.2f}GB\")\n",
    "\n",
    "    if SRC2SRC_SAMPLING and SRC2SRC_MODEL_ID is not None:\n",
    "        src2src_tokenizer = NllbTokenizer.from_pretrained(SRC2SRC_MODEL_ID, src_lang=SRC_LANG, tgt_lang=TGT_LANG)\n",
    "        src2src_model = AutoModelForSeq2SeqLM.from_pretrained(SRC2SRC_MODEL_ID, device_map=\"cuda\", torch_dtype=torch.bfloat16)\n",
    "        print(f\"Memory footprint: {src2src_model.get_memory_footprint() / 1024**3 :.2f}GB\")\n",
    "\n",
    "    back_translations = []\n",
    "    src_translations = []\n",
    "    for _ in range(15):\n",
    "        dyu, fra = back_translate(\n",
    "            ds, model, tokenizer, split=DATA_SPLIT, batch_size=64, src_lang=SRC_LANG,\n",
    "            tgt_lang=TGT_LANG, sample_src=SRC2SRC_SAMPLING, src2src_model=src2src_model,\n",
    "            src2src_tokenizer=src2src_tokenizer\n",
    "        )\n",
    "        back_translations += dyu\n",
    "        src_translations += fra\n",
    "    assert len(back_translations) == len(src_translations)\n",
    "\n",
    "    df_bt = pd.DataFrame({\"dyu\": back_translations, \"fr\": src_translations}).drop_duplicates()\n",
    "    back_translations_json = {\n",
    "        \"split\": DATA_SPLIT,\n",
    "        \"data\": [{\"ID\": 0, \"translation\": {\"dyu\": row[\"dyu\"], \"fr\": row[\"fr\"]}} for _, row in df_bt.iterrows()]\n",
    "    }\n",
    "    _save(savedir=\"./data\", df=df_bt, json_data=back_translations_json, split=DATA_SPLIT)\n",
    "    cleanup()\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # Create back-translations from the validation data\n",
    "    # ------------------------------------------------\n",
    "    DATA_SPLIT = \"validation\"\n",
    "\n",
    "    back_translations = []\n",
    "    src_translations = []\n",
    "    for _ in range(10):\n",
    "        dyu, fra = back_translate(\n",
    "            ds, model, tokenizer, split=DATA_SPLIT, batch_size=64, src_lang=SRC_LANG,\n",
    "            tgt_lang=TGT_LANG, sample_src=SRC2SRC_SAMPLING, src2src_model=src2src_model,\n",
    "            src2src_tokenizer=src2src_tokenizer\n",
    "        )\n",
    "        back_translations += dyu\n",
    "        src_translations += fra\n",
    "    assert len(back_translations) == len(src_translations)\n",
    "\n",
    "    df_bt = pd.DataFrame({\"dyu\": back_translations, \"fr\": src_translations}).drop_duplicates()\n",
    "    back_translations_json = {\n",
    "        \"split\": DATA_SPLIT,\n",
    "        \"data\": [{\"ID\": 0, \"translation\": {\"dyu\": row[\"dyu\"], \"fr\": row[\"fr\"]}} for _, row in df_bt.iterrows()]\n",
    "    }\n",
    "    _save(savedir=\"./data\", df=df_bt, json_data=back_translations_json, split=DATA_SPLIT)\n",
    "    cleanup()\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # Create back-translations from the test data\n",
    "    # ------------------------------------------------\n",
    "    MODEL_ID = './saved_models/dyu-fra-600M'\n",
    "    DATA_SPLIT = \"test\"\n",
    "    SRC_LANG = \"dyu_Latn\"\n",
    "    TGT_LANG = \"fra_Latn\"\n",
    "    SRC2SRC_SAMPLING = False\n",
    "    SRC2SRC_MODEL_ID = None\n",
    "\n",
    "    tokenizer = NllbTokenizer.from_pretrained(MODEL_ID, src_lang=SRC_LANG, tgt_lang=TGT_LANG)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_ID, device_map=\"cuda\", torch_dtype=torch.bfloat16)\n",
    "    print(f\"Memory footprint: {model.get_memory_footprint() / 1024**3 :.2f}GB\")\n",
    "\n",
    "    back_translations = []\n",
    "    src_translations = []\n",
    "    for _ in range(5):\n",
    "        dyu, fra = back_translate(\n",
    "            ds, model, tokenizer, split=DATA_SPLIT, batch_size=64, src_lang=SRC_LANG,\n",
    "            tgt_lang=TGT_LANG, sample_src=SRC2SRC_SAMPLING, src2src_model=src2src_model,\n",
    "            src2src_tokenizer=src2src_tokenizer\n",
    "        )\n",
    "        back_translations += dyu\n",
    "        src_translations += fra\n",
    "    assert len(back_translations) == len(src_translations)\n",
    "\n",
    "    df_bt = pd.DataFrame({\"dyu\": src_translations, \"fr\": back_translations}).drop_duplicates()\n",
    "    back_translations_json = {\n",
    "        \"split\": DATA_SPLIT,\n",
    "        \"data\": [{\"ID\": 0, \"translation\": {\"dyu\": row[\"dyu\"], \"fr\": row[\"fr\"]}} for _, row in df_bt.iterrows()]\n",
    "    }\n",
    "    _save(savedir=\"./data\", df=df_bt, json_data=back_translations_json, split=DATA_SPLIT)\n",
    "    cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7b51ec-70e2-4d6f-9a26-610e77d2a7cc",
   "metadata": {},
   "source": [
    "## Train small Dyula to French translation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b80d1db4-0565-41db-abc4-3e110931f8ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "218e394943894c8b91f43df7ad52dcaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bce2199ed1340d3bce94508bb5e04fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1ae0843f0ad41dfb618ab51fc233e32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 0.02GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>BLEU</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.178445</td>\n",
       "      <td>3.616487</td>\n",
       "      <td>2.359799</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.299672</td>\n",
       "      <td>3.304130</td>\n",
       "      <td>3.938671</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.958335</td>\n",
       "      <td>3.212257</td>\n",
       "      <td>4.733772</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.831303</td>\n",
       "      <td>3.357568</td>\n",
       "      <td>4.603349</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.513231</td>\n",
       "      <td>3.317283</td>\n",
       "      <td>4.383167</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.318357</td>\n",
       "      <td>3.272649</td>\n",
       "      <td>5.153862</td>\n",
       "      <td>00:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.263743</td>\n",
       "      <td>3.261696</td>\n",
       "      <td>5.452816</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.459558</td>\n",
       "      <td>3.439089</td>\n",
       "      <td>4.734892</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.313965</td>\n",
       "      <td>3.439996</td>\n",
       "      <td>4.862956</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.203292</td>\n",
       "      <td>3.454323</td>\n",
       "      <td>4.932612</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.072692</td>\n",
       "      <td>3.448771</td>\n",
       "      <td>5.125740</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.975122</td>\n",
       "      <td>3.439028</td>\n",
       "      <td>5.881967</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.902148</td>\n",
       "      <td>3.449903</td>\n",
       "      <td>5.782223</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.858590</td>\n",
       "      <td>3.449383</td>\n",
       "      <td>6.026340</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.850442</td>\n",
       "      <td>3.445292</td>\n",
       "      <td>6.004855</td>\n",
       "      <td>00:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.134640</td>\n",
       "      <td>3.569473</td>\n",
       "      <td>5.012852</td>\n",
       "      <td>00:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.104498</td>\n",
       "      <td>3.563462</td>\n",
       "      <td>5.226630</td>\n",
       "      <td>00:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.051743</td>\n",
       "      <td>3.563832</td>\n",
       "      <td>5.635757</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.981483</td>\n",
       "      <td>3.594373</td>\n",
       "      <td>5.638495</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.957159</td>\n",
       "      <td>3.604936</td>\n",
       "      <td>5.276251</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.879342</td>\n",
       "      <td>3.618875</td>\n",
       "      <td>6.053190</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.836787</td>\n",
       "      <td>3.628025</td>\n",
       "      <td>6.048949</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.779344</td>\n",
       "      <td>3.649701</td>\n",
       "      <td>5.814335</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.736423</td>\n",
       "      <td>3.672266</td>\n",
       "      <td>5.857423</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.690869</td>\n",
       "      <td>3.695009</td>\n",
       "      <td>6.156391</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.656865</td>\n",
       "      <td>3.709607</td>\n",
       "      <td>6.480234</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.626243</td>\n",
       "      <td>3.736968</td>\n",
       "      <td>6.446262</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.592974</td>\n",
       "      <td>3.745232</td>\n",
       "      <td>6.753882</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.583121</td>\n",
       "      <td>3.739961</td>\n",
       "      <td>6.416026</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.576299</td>\n",
       "      <td>3.748081</td>\n",
       "      <td>6.484227</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.577101</td>\n",
       "      <td>3.747856</td>\n",
       "      <td>6.467550</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aaa3a887a6f4a549425128b4a8f85aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1471 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation BLEU after initial training 6.5007973468296845\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>BLEU</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.680522</td>\n",
       "      <td>3.751438</td>\n",
       "      <td>7.336107</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.649479</td>\n",
       "      <td>3.752925</td>\n",
       "      <td>7.561769</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.622414</td>\n",
       "      <td>3.748319</td>\n",
       "      <td>7.756585</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.595922</td>\n",
       "      <td>3.755126</td>\n",
       "      <td>7.731476</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.574510</td>\n",
       "      <td>3.759532</td>\n",
       "      <td>7.677297</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.548271</td>\n",
       "      <td>3.768267</td>\n",
       "      <td>7.947700</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.528486</td>\n",
       "      <td>3.770392</td>\n",
       "      <td>7.475109</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.510657</td>\n",
       "      <td>3.779884</td>\n",
       "      <td>7.660944</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.491862</td>\n",
       "      <td>3.784925</td>\n",
       "      <td>7.821357</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.476568</td>\n",
       "      <td>3.786598</td>\n",
       "      <td>7.834554</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14eeb983929d4a659ea2a675577c611b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1471 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation BLEU after further fine-tuning 7.71826143445664\n",
      "Converting to bf16...\n",
      "Memory footprint: 0.01GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2ad61196aeb49feae2a9724905f6938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1471 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation BLEU for bf16 model 7.670348861677615\n",
      "Model saved to saved_models/nllb-dyu-fr-10MB\n",
      "CPU times: user 20min 50s, sys: 14.5 s, total: 21min 5s\n",
      "Wall time: 21min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if TRAIN_DYU_FRA_SMALL:\n",
    "    BASE_MODEL_ID = \"facebook/nllb-200-distilled-600M\"\n",
    "    TOKENIZER_ID = \"tokenizers/tokenizer_2k\"\n",
    "\n",
    "    # Load data\n",
    "    df = load_from_json(\n",
    "        train_files=[\n",
    "            \"data/dataset_train.json\",\n",
    "            \"data/dataset_bt_train.json\",\n",
    "            \"data/dataset_bt_test.json\",\n",
    "            # \"data/dataset_validation.json\", \"data/dataset_bt_validation.json\"\n",
    "        ],\n",
    "        valid_files=\"data/dataset_validation.json\",\n",
    "        test_files=\"data/dataset_test.json\",\n",
    "        return_format=\"df\"\n",
    "    )\n",
    "    df_train = df[df[\"split\"] == \"train\"].copy()\n",
    "    df_valid = df[df[\"split\"]== \"validation\"].copy()\n",
    "    df_test = df[df[\"split\"]== \"test\"].copy()\n",
    "    assert len(df_train) + len(df_valid) + len(df_test) == len(df)\n",
    "\n",
    "    # Load model\n",
    "    tokenizer = NllbTokenizer.from_pretrained(TOKENIZER_ID)\n",
    "    model = instantiate_small_model(\n",
    "        BASE_MODEL_ID, dim_factor=4, layer_factor=4, vocab_size=tokenizer.vocab_size,\n",
    "        encoder_ffn_dim=None, encoder_layers=None, decoder_ffn_dim=None, decoder_layers=None,\n",
    "        num_hidden_layers=None, d_model=None, max_position_embeddings=None\n",
    "    )\n",
    "\n",
    "    # Create dataloaders\n",
    "    dls = create_dataloaders(\n",
    "        df_train[[\"dyu\", \"fr\"]], df_valid[[\"dyu\", \"fr\"]], tokenizer, bs=128,\n",
    "        src_lang=\"dyu_Latn\", tgt_lang=\"fra_Latn\", preproc_func=preproc,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "    # Train on back-translated data\n",
    "    learn = create_learner(dls, model, tokenizer, src_lang=\"dyu_Latn\", tgt_lang=\"fra_Latn\")\n",
    "    learn.fit_sgdr(5, 1, cycle_mult=2, lr_max=1e-3)\n",
    "    translate_func = partial(\n",
    "        translate, model=model, tokenizer=tokenizer, src_lang=\"dyu_Latn\", tgt_lang=\"fra_Latn\"\n",
    "    )\n",
    "    print(\n",
    "        \"Validation BLEU after initial training\",\n",
    "        calculate_bleu(\n",
    "            model, df_valid, translate_func, src_lang=\"dyu_Latn\", tgt_lang=\"fra_Latn\",\n",
    "            preproc_func=preproc\n",
    "        )\n",
    "    )\n",
    "    model.save_pretrained('./tmp')\n",
    "    tokenizer.save_pretrained('./tmp')\n",
    "\n",
    "    # Fine-tune on the original training data\n",
    "    df = load_from_json(\n",
    "        train_files=\"data/dataset_train.json\",\n",
    "        valid_files=\"data/dataset_validation.json\",\n",
    "        test_files=\"data/dataset_test.json\",\n",
    "        return_format=\"df\"\n",
    "    )\n",
    "    df_train = df[df[\"split\"] == \"train\"].copy()\n",
    "    df_valid = df[df[\"split\"]== \"validation\"].copy()\n",
    "    df_test = df[df[\"split\"]== \"test\"].copy()\n",
    "    assert len(df_train) + len(df_valid) + len(df_test) == len(df)\n",
    "\n",
    "    dls = create_dataloaders(\n",
    "        df_train[[\"dyu\", \"fr\"]], df_valid[[\"dyu\", \"fr\"]], tokenizer, bs=256,\n",
    "        src_lang=\"dyu_Latn\", tgt_lang=\"fra_Latn\", preproc_func=preproc,\n",
    "        max_length=128\n",
    "    )\n",
    "    learn = create_learner(dls, model, tokenizer, src_lang=\"dyu_Latn\", tgt_lang=\"fra_Latn\")\n",
    "    learn.fit_flat_cos(10, 1e-4)\n",
    "\n",
    "    translate_func = partial(\n",
    "        translate, model=model, tokenizer=tokenizer, src_lang=\"dyu_Latn\", tgt_lang=\"fra_Latn\"\n",
    "    )\n",
    "    print(\n",
    "        \"Validation BLEU after further fine-tuning\",\n",
    "        calculate_bleu(\n",
    "            model, df_valid, translate_func, src_lang=\"dyu_Latn\", tgt_lang=\"fra_Latn\",\n",
    "            preproc_func=preproc\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(\"Converting to bf16...\")\n",
    "    model.bfloat16()\n",
    "    print(f\"Memory footprint: {model.get_memory_footprint() / 1024**3 :.2f}GB\")\n",
    "\n",
    "    translate_func = partial(\n",
    "        translate, model=model, tokenizer=tokenizer, src_lang=\"dyu_Latn\", tgt_lang=\"fra_Latn\"\n",
    "    )\n",
    "    print(\n",
    "        \"Validation BLEU for bf16 model\",\n",
    "        calculate_bleu(\n",
    "            model, df_valid, translate_func, src_lang=\"dyu_Latn\", tgt_lang=\"fra_Latn\",\n",
    "            preproc_func=preproc\n",
    "        )\n",
    "    )\n",
    "\n",
    "    MODEL_SAVE_PATH = 'saved_models/nllb-dyu-fr-10MB'\n",
    "    print(\"Model saved to\", MODEL_SAVE_PATH)\n",
    "    model.save_pretrained(MODEL_SAVE_PATH)\n",
    "    tokenizer.save_pretrained(MODEL_SAVE_PATH)\n",
    "    cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b358eb77-6bfd-4d2f-8b80-64dfabc851b4",
   "metadata": {},
   "source": [
    "## Train a small Dyula to French translation model through knowledge distillation\n",
    "\n",
    "This is a three-step process:\n",
    " 1. Fine-tune NLLB-200-600M, but with a modified tokenizer with a smaller vocabulary\n",
    " 2. Use the model from step 1 as teacher. The student is a pruned version of the model where half of the encoder and decoder layers are dropped, and half of the neurons in linear layers are dropped.\n",
    " 3. Repeat step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42db0780-a999-4f48-98b2-33258c15ddd6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 6.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if TRAIN_DYU_FRA_DISTILLED:\n",
    "    # ------------------------------------------------\n",
    "    # STEP 1 - Fine-tune NLLB-200-600M with modified tokenizer\n",
    "    # ------------------------------------------------\n",
    "    MODEL_ID = \"facebook/nllb-200-distilled-600M\"\n",
    "    TOKENIZER_ID = \"tokenizers/tokenizer_freq5\"\n",
    "\n",
    "    df = load_from_json(\n",
    "        train_files=\"data/dataset_train.json\",\n",
    "        valid_files=\"data/dataset_validation.json\",\n",
    "        test_files=\"data/dataset_test.json\",\n",
    "        return_format=\"df\"\n",
    "    )\n",
    "    df_train = df[df[\"split\"] == \"train\"].copy()\n",
    "    df_valid = df[df[\"split\"]== \"validation\"].copy()\n",
    "    df_test = df[df[\"split\"]== \"test\"].copy()\n",
    "    assert len(df_train) + len(df_valid) + len(df_test) == len(df)\n",
    "\n",
    "    # Load model\n",
    "    model, tokenizer = load_model(\n",
    "        MODEL_ID, load_tokenizer=True, tokenizer_id=TOKENIZER_ID, remap_embeddings=True, init_embeds_for_new_tokens=True,\n",
    "        old_tokenizer_id=MODEL_ID, src_language=\"dyu_Latn\", tgt_language=\"fra_Latn\"\n",
    "    )\n",
    "\n",
    "    # Create dataloaders and train\n",
    "    dls = create_dataloaders(\n",
    "        df_train[[\"dyu\", \"fr\"]], df_valid[[\"dyu\", \"fr\"]], tokenizer, bs=32,\n",
    "        src_lang=\"dyu_Latn\", tgt_lang=\"fra_Latn\", preproc_func=preproc,\n",
    "        max_length=128\n",
    "    )\n",
    "    learn = create_learner(dls, model, tokenizer, src_lang=\"dyu_Latn\", tgt_lang=\"fra_Latn\")#, wd=1e-3)\n",
    "    learn.fit_flat_cos(10, lr=1e-4, div_final=100_000.0, pct_start=0.75)\n",
    "\n",
    "    translate_func = partial(\n",
    "        translate, model=model, tokenizer=tokenizer, src_lang=\"dyu_Latn\", tgt_lang=\"fra_Latn\"\n",
    "    )\n",
    "    print(\n",
    "        \"Validation BLEU after step 1 fine-tune\",\n",
    "        calculate_bleu(\n",
    "            model, df_valid, translate_func, src_lang=\"dyu_Latn\", tgt_lang=\"fra_Latn\",\n",
    "            preproc_func=preproc\n",
    "        )\n",
    "    )\n",
    "    MODEL_SAVE_PATH = 'saved_models/dyu-fra-vocab-freq5'\n",
    "    model.save_pretrained(MODEL_SAVE_PATH)\n",
    "    tokenizer.save_pretrained(MODEL_SAVE_PATH)\n",
    "    del model\n",
    "    cleanup()\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # STEP 2 - Down-scale and train again\n",
    "    # ------------------------------------------------\n",
    "    df = load_from_json(\n",
    "        train_files=[\n",
    "            \"data/dataset_train.json\",\n",
    "            \"data/dataset_bt_train.json\",\n",
    "            \"data/dataset_bt_test.json\",\n",
    "            # \"data/dataset_validation.json\", \"data/dataset_bt_validation.json\"\n",
    "        ],\n",
    "        valid_files=\"data/dataset_validation.json\",\n",
    "        test_files=\"data/dataset_test.json\",\n",
    "        return_format=\"df\"\n",
    "    )\n",
    "    df_train = df[df[\"split\"] == \"train\"].copy()\n",
    "    df_valid = df[df[\"split\"]== \"validation\"].copy()\n",
    "    df_test = df[df[\"split\"]== \"test\"].copy()\n",
    "    assert len(df_train) + len(df_valid) + len(df_test) == len(df)\n",
    "\n",
    "    # Load the teacher model:\n",
    "    BASE_MODEL_ID = \"saved_models/dyu-fra-vocab-freq5\"\n",
    "    tokenizer = NllbTokenizer.from_pretrained(BASE_MODEL_ID)\n",
    "    teacher_model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL_ID, device_map=\"cuda\")\n",
    "    print(f\"Memory footprint: {teacher_model.get_memory_footprint() / 1024**3 :.2f}GB\")\n",
    "\n",
    "    # Create the student model as a pruned version of the teacher:\n",
    "    student_model = prune(\n",
    "        teacher_model, size_factor=2, layer_size_factor=2, dim_strategy=\"alternate\",\n",
    "        layer_strategy=\"alternate\"\n",
    "    )\n",
    "\n",
    "    # Create dataloaders and train\n",
    "    dls = create_dataloaders(\n",
    "        df_train[[\"dyu\", \"fr\"]], df_valid[[\"dyu\", \"fr\"]], tokenizer, bs=256,\n",
    "        src_lang=\"dyu_Latn\", tgt_lang=\"fra_Latn\", preproc_func=preproc,\n",
    "        max_length=128\n",
    "    )\n",
    "    learn = create_distillation_learner(\n",
    "        dls, student_model, teacher_model, tokenizer, src_lang=\"dyu_Latn\", tgt_lang=\"fra_Latn\"\n",
    "    )\n",
    "    learn.fit_sgdr(5, 1, cycle_mult=2, lr_max=1e-3)\n",
    "\n",
    "    translate_func = partial(\n",
    "        translate, model=student_model, tokenizer=tokenizer, src_lang=\"dyu_Latn\", tgt_lang=\"fra_Latn\"\n",
    "    )\n",
    "    print(\n",
    "        \"Validation BLEU for step 2 student model\",\n",
    "        calculate_bleu(\n",
    "            student_model, df_valid, translate_func, src_lang=\"dyu_Latn\", tgt_lang=\"fra_Latn\",\n",
    "            preproc_func=preproc\n",
    "        )\n",
    "    )\n",
    "    student_model.save_pretrained('./tmp')\n",
    "    tokenizer.save_pretrained('./tmp')\n",
    "    del teacher_model, student_model\n",
    "    cleanup()\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # STEP 3 - Down-scale for the last time\n",
    "    # ------------------------------------------------\n",
    "    # Load the teacher and student models:\n",
    "    BASE_MODEL_ID = \"./tmp\"\n",
    "    tokenizer = NllbTokenizer.from_pretrained(BASE_MODEL_ID)\n",
    "    teacher_model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL_ID, device_map=\"cuda\")\n",
    "    print(f\"Memory footprint: {teacher_model.get_memory_footprint() / 1024**3 :.2f}GB\")\n",
    "    student_model = prune(\n",
    "        teacher_model, size_factor=2, layer_size_factor=2, dim_strategy=\"alternate\", layer_strategy=\"alternate\"\n",
    "    )\n",
    "\n",
    "    # Create `Learner` and train:\n",
    "    learn = create_distillation_learner(\n",
    "        dls, student_model, teacher_model, tokenizer, src_lang=\"dyu_Latn\", tgt_lang=\"fra_Latn\"\n",
    "    )\n",
    "    learn.fit_sgdr(5, 1, cycle_mult=2, lr_max=1e-3)\n",
    "\n",
    "    translate_func = partial(\n",
    "        translate, model=student_model, tokenizer=tokenizer, src_lang=\"dyu_Latn\", tgt_lang=\"fra_Latn\"\n",
    "    )\n",
    "    print(\n",
    "        \"Validation BLEU for step 3 student model\",\n",
    "        calculate_bleu(\n",
    "            student_model, df_valid, translate_func, src_lang=\"dyu_Latn\", tgt_lang=\"fra_Latn\",\n",
    "            preproc_func=preproc\n",
    "        )\n",
    "    )\n",
    "    MODEL_SAVE_PATH = 'saved_models/nllb-dyu-fr-distilled'\n",
    "    student_model.save_pretrained(MODEL_SAVE_PATH)\n",
    "    tokenizer.save_pretrained(MODEL_SAVE_PATH)\n",
    "\n",
    "    # Finally, fine-tune on the original training data only:\n",
    "    df = load_from_json(\n",
    "        train_files=\"data/dataset_train.json\",\n",
    "        valid_files=\"data/dataset_validation.json\",\n",
    "        test_files=\"data/dataset_test.json\",\n",
    "        return_format=\"df\"\n",
    "    )\n",
    "    df_train = df[df[\"split\"] == \"train\"].copy()\n",
    "    df_valid = df[df[\"split\"]== \"validation\"].copy()\n",
    "    df_test = df[df[\"split\"]== \"test\"].copy()\n",
    "    assert len(df_train) + len(df_valid) + len(df_test) == len(df)\n",
    "\n",
    "    dls = create_dataloaders(\n",
    "        df_train[[\"dyu\", \"fr\"]], df_valid[[\"dyu\", \"fr\"]], tokenizer, bs=128,\n",
    "        src_lang=\"dyu_Latn\", tgt_lang=\"fra_Latn\", preproc_func=preproc,\n",
    "        max_length=128\n",
    "    )\n",
    "    learn = create_learner(dls, student_model, tokenizer, src_lang=\"dyu_Latn\", tgt_lang=\"fra_Latn\")\n",
    "    learn.fit_flat_cos(10, 1e-5)\n",
    "\n",
    "    translate_func = partial(\n",
    "        translate, model=student_model, tokenizer=tokenizer, src_lang=\"dyu_Latn\", tgt_lang=\"fra_Latn\"\n",
    "    )\n",
    "    print(\n",
    "        \"Validation BLEU after further fine-tuning\",\n",
    "        calculate_bleu(\n",
    "            student_model, df_valid, translate_func, src_lang=\"dyu_Latn\", tgt_lang=\"fra_Latn\",\n",
    "            preproc_func=preproc\n",
    "        )\n",
    "    )\n",
    "    MODEL_SAVE_PATH = 'saved_models/nllb-dyu-fr-distilled-final'\n",
    "    student_model.save_pretrained(MODEL_SAVE_PATH)\n",
    "    tokenizer.save_pretrained(MODEL_SAVE_PATH)\n",
    "\n",
    "    # Convert to bf16\n",
    "    student_model.bfloat16()\n",
    "    print(f\"Memory footprint: {student_model.get_memory_footprint() / 1024**3 :.2f}GB\")\n",
    "    translate_func = partial(\n",
    "        translate, model=student_model, tokenizer=tokenizer, src_lang=\"dyu_Latn\", tgt_lang=\"fra_Latn\"\n",
    "    )\n",
    "    print(\n",
    "        \"Validation BLEU after converting to bf16\",\n",
    "        calculate_bleu(\n",
    "            student_model, df_valid, translate_func, src_lang=\"dyu_Latn\", tgt_lang=\"fra_Latn\",\n",
    "            preproc_func=preproc\n",
    "        )\n",
    "    )\n",
    "    MODEL_SAVE_PATH = 'saved_models/nllb-dyu-fr-distilled-final-bf16'\n",
    "    student_model.save_pretrained(MODEL_SAVE_PATH)\n",
    "    tokenizer.save_pretrained(MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7774916b-602e-4166-957c-21dee88fc3de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a542fb7-578e-46d2-bb0c-7d789731e566",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e81d92-a8cb-46c5-8793-0733980e5632",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
